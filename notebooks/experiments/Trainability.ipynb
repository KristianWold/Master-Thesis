{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.25, 0.02) - gaussian(x, 0.75, 0.02)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_qnn, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder = Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    \n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKvElEQVR4nO3d32vd9R3H8dfLpF1L4g+0TmpTVgciFMF2xF5YNlhxo/5Ad6mgV0JvJrRsInrpP+BksJugsonOoqggzh8raHEFfzStrbNWRykdDS10zokm6GrS9y5y2iUmbb7nm/PN58vb5wOCiedwfFH77DfnpOf7dUQIQB4XlR4AoLeIGkiGqIFkiBpIhqiBZPqbeNC+wYHov+LyJh66FvefKT1hjphy6Qmz+NuW7ZkqvWCuM43UUs/kfz7X1MTEvP/TGpnZf8XlWv3Q9iYeupZlq74uPWGO0+PLS0+YZcXxdu3pHy+9YK5vVrXnx79jv//deW/j228gGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmUpR295q+1PbR2w/1PQoAPUtGLXtPkl/kHSLpPWS7ra9vulhAOqpcqTeJOlIRByNiNOSdkq6s9lZAOqqEvUaScdnfD3W+Xez2N5me9T26NR4C9/hDnxPVIl6vlOmzDkFRESMRMRwRAz3DQ4ufhmAWqpEPSZp7YyvhySdaGYOgMWqEvVeSdfavsb2ckl3SXq52VkA6lrwxIMRMWn7fklvSOqT9GREHGp8GYBaKp1NNCJelfRqw1sA9AB/owxIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkKr2ho1vuP6Nlq75u4qFr+fSnT5WeMMeOk8OlJ8xy8PmNpSfMsvz1vaUnzHHigZtKTzjnoskL3LZ0MwAsBaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIJkFo7b9pO1Ttj9aikEAFqfKkfqPkrY2vANAjywYdUS8LenzJdgCoAd69pza9jbbo7ZHp76c6NXDAuhSz6KOiJGIGI6I4b5LBnr1sAC6xKvfQDJEDSRT5Udaz0p6R9J1tsds39f8LAB1LXje74i4eymGAOgNvv0GkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmQXf0FFHTFmnx5c38dC17Dg5XHrCHH878ePSE2aJaxr5rVDbpVtvLD1hjsnB0gv+L/rOfxtHaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSqXKBvLW237J92PYh29uXYhiAeqq8iXZS0m8jYr/tiyXts70rIj5ueBuAGhY8UkfEyYjY3/n8K0mHJa1pehiAerp6Tm17naSNkt6b57Zttkdtj06NT/RoHoBuVY7a9qCkFyTtiIgvv3t7RIxExHBEDPcNDvRyI4AuVIra9jJNB/1MRLzY7CQAi1Hl1W9LekLS4Yh4tPlJABajypF6s6R7JW2xfaDzcWvDuwDUtOCPtCJijyQvwRYAPcDfKAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiCZKuco65q/tVYcX97EQ9dy8PmNpSfMEdc08ktfm2/9d+kJs9xw9dHSE+Y4dmBD6QnnxLI4720cqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpspVL1fYft/2QduHbD+yFMMA1FPlTb3/lbQlIsY716neY/u1iHi34W0Aaqhy1cuQNN75clnn4/zv0AZQVKXn1Lb7bB+QdErSroh4b577bLM9ant0amKixzMBVFUp6oiYiogNkoYkbbJ9/Tz3GYmI4YgY7hsY6PFMAFV19ep3RHwhabekrU2MAbB4VV79vtL2ZZ3PV0q6WdInDe8CUFOVV79XS/qT7T5N/yHwXES80uwsAHVVefX7Q0ntO8cugHnxN8qAZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpsq7tLrmKal/fOH7LZXlr+8tPWGOS7feWHrCLDdcfbT0hFkeWz1aesIcrx1ZX3rCOe47/xnFOFIDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEzlqDsXnv/ANhfHA1qsmyP1dkmHmxoCoDcqRW17SNJtkh5vdg6Axap6pH5M0oOSzpzvDra32R61PTr19UQvtgGoYcGobd8u6VRE7LvQ/SJiJCKGI2K4b+VAzwYC6E6VI/VmSXfYPiZpp6Qttp9udBWA2haMOiIejoihiFgn6S5Jb0bEPY0vA1ALP6cGkunqFMERsVvS7kaWAOgJjtRAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMl29S6uqM/3SN6uiiYeu5cQDN5WeMMfkYOkFsx07sKH0hFleO7K+9IQ5vv1sZekJ58Tk+Y/HHKmBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSKbSWy8716b+StKUpMmIGG5yFID6unk/9c8j4rPGlgDoCb79BpKpGnVI+qvtfba3zXcH29tsj9oePTMx0buFALpS9dvvzRFxwvYPJe2y/UlEvD3zDhExImlEkn4wtLY95zICvmcqHakj4kTnn6ckvSRpU5OjANS3YNS2B2xffPZzSb+U9FHTwwDUU+Xb76skvWT77P3/HBGvN7oKQG0LRh0RRyXdsARbAPQAP9ICkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGUf0/nwGtv8l6Z89eKhVktp0XjT2XFjb9kjt29SrPT+KiCvnu6GRqHvF9mibzlzKngtr2x6pfZuWYg/ffgPJEDWQTNujHik94DvYc2Ft2yO1b1Pje1r9nBpA99p+pAbQJaIGkmll1La32v7U9hHbD7Vgz5O2T9luxamRba+1/Zbtw7YP2d5eeM8K2+/bPtjZ80jJPWfZ7rP9ge1XSm+Rpi80afvvtg/YHm3sv9O259S2+yT9Q9IvJI1J2ivp7oj4uOCmn0kal/RURFxfaseMPaslrY6I/Z1zsu+T9KtSv0aePn/0QESM214maY+k7RHxbok9M3b9RtKwpEsi4vaSWzp7jkkabvpCk208Um+SdCQijkbEaUk7Jd1ZclDnEkOfl9wwU0ScjIj9nc+/knRY0pqCeyIixjtfLut8FD1a2B6SdJukx0vuKKGNUa+RdHzG12Mq+Bu27Wyvk7RR0nuFd/TZPiDplKRdEVF0j6THJD0o6UzhHTMteKHJXmhj1J7n37XrOUJL2B6U9IKkHRHxZcktETEVERskDUnaZLvY0xTbt0s6FRH7Sm04j80R8RNJt0j6dedpXc+1MeoxSWtnfD0k6UShLa3Vee76gqRnIuLF0nvOiogvJO2WtLXgjM2S7ug8h90paYvtpwvukbR0F5psY9R7JV1r+xrbyyXdJenlwptapfPC1BOSDkfEoy3Yc6Xtyzqfr5R0s6RPSu2JiIcjYigi1mn698+bEXFPqT3S0l5osnVRR8SkpPslvaHpF4Cei4hDJTfZflbSO5Kusz1m+76SezR9JLpX00egA52PWwvuWS3pLdsfavoP5V0R0YofI7XIVZL22D4o6X1Jf2nqQpOt+5EWgMVp3ZEawOIQNZAMUQPJEDWQDFEDyRA1kAxRA8n8DwFWjEFmRpvZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=200, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=200, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [3, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../src/layers.py:176: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [3, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbb56670c33431588a8eeb86b3d08d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f487e4cc64adf80b18f604fa0852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.012148431196048063\n",
      "epoch: 1, loss: 0.011980385371999271\n",
      "epoch: 2, loss: 0.01179547348901502\n",
      "epoch: 3, loss: 0.011803022517372697\n",
      "epoch: 4, loss: 0.012039628532458262\n",
      "epoch: 5, loss: 0.011928511686742336\n",
      "epoch: 6, loss: 0.011766001574135777\n",
      "epoch: 7, loss: 0.011627308652327785\n",
      "epoch: 8, loss: 0.011673681182331806\n",
      "epoch: 9, loss: 0.011709354036838758\n",
      "epoch: 10, loss: 0.012002848617721613\n",
      "epoch: 11, loss: 0.011750310299730561\n",
      "epoch: 12, loss: 0.011712583025981457\n",
      "epoch: 13, loss: 0.011647812518401872\n",
      "epoch: 14, loss: 0.011715479992813816\n",
      "epoch: 15, loss: 0.011651864716776402\n",
      "epoch: 16, loss: 0.011669518449077897\n",
      "epoch: 17, loss: 0.011868135152118568\n",
      "epoch: 18, loss: 0.011818297423394306\n",
      "epoch: 19, loss: 0.01158188390739788\n",
      "epoch: 20, loss: 0.011635109811138576\n",
      "epoch: 21, loss: 0.011891986178606971\n",
      "epoch: 22, loss: 0.012042224803234176\n",
      "epoch: 23, loss: 0.011636656517251908\n",
      "epoch: 24, loss: 0.011894149943480592\n",
      "epoch: 25, loss: 0.011492520239619528\n",
      "epoch: 26, loss: 0.01155023254606184\n",
      "epoch: 27, loss: 0.011695794959200435\n",
      "epoch: 28, loss: 0.011523478862884637\n",
      "epoch: 29, loss: 0.011814457489974225\n",
      "epoch: 30, loss: 0.01168931407366845\n",
      "epoch: 31, loss: 0.011658247781285025\n",
      "epoch: 32, loss: 0.011926431835727922\n",
      "epoch: 33, loss: 0.011812820866222817\n",
      "epoch: 34, loss: 0.011717147118008718\n",
      "epoch: 35, loss: 0.011928155658254447\n",
      "epoch: 36, loss: 0.0117736519521499\n",
      "epoch: 37, loss: 0.011731010748165458\n",
      "epoch: 38, loss: 0.011655230011606088\n",
      "epoch: 39, loss: 0.011720239471252837\n",
      "epoch: 40, loss: 0.011859545602729048\n",
      "epoch: 41, loss: 0.01175992199002874\n",
      "epoch: 42, loss: 0.01196859609770764\n",
      "epoch: 43, loss: 0.011785300226841124\n",
      "epoch: 44, loss: 0.011755123052639649\n",
      "epoch: 45, loss: 0.011652547708631576\n",
      "epoch: 46, loss: 0.01178929250898367\n",
      "epoch: 47, loss: 0.01168468534224479\n",
      "epoch: 48, loss: 0.011644053761587767\n",
      "epoch: 49, loss: 0.011387379143925873\n",
      "epoch: 50, loss: 0.011497054689366841\n",
      "epoch: 51, loss: 0.011728375400524191\n",
      "epoch: 52, loss: 0.011759872195681849\n",
      "epoch: 53, loss: 0.011581330418786194\n",
      "epoch: 54, loss: 0.011635299288540478\n",
      "epoch: 55, loss: 0.011498769987723858\n",
      "epoch: 56, loss: 0.011834905396830664\n",
      "epoch: 57, loss: 0.011825499726994804\n",
      "epoch: 58, loss: 0.011707183027389781\n",
      "epoch: 59, loss: 0.011637256255292607\n",
      "epoch: 60, loss: 0.011651705029418718\n",
      "epoch: 61, loss: 0.011774870958690136\n",
      "epoch: 62, loss: 0.011723120973905078\n",
      "epoch: 63, loss: 0.0115629170744672\n",
      "epoch: 64, loss: 0.011365235936374903\n",
      "epoch: 65, loss: 0.011761836866699852\n",
      "epoch: 66, loss: 0.011688142912094154\n",
      "epoch: 67, loss: 0.01159034513985037\n",
      "epoch: 68, loss: 0.011434759001125257\n",
      "epoch: 69, loss: 0.01160940650950725\n",
      "epoch: 70, loss: 0.011652277853984459\n",
      "epoch: 71, loss: 0.01186101658886264\n",
      "epoch: 72, loss: 0.011475790145606821\n",
      "epoch: 73, loss: 0.011703758222111586\n",
      "epoch: 74, loss: 0.01161988519826315\n",
      "epoch: 75, loss: 0.011501546841081551\n",
      "epoch: 76, loss: 0.011953154776686372\n",
      "epoch: 77, loss: 0.011554014842114482\n",
      "epoch: 78, loss: 0.011649239209593092\n",
      "epoch: 79, loss: 0.011664699094516348\n",
      "epoch: 80, loss: 0.011746697275507215\n",
      "epoch: 81, loss: 0.011565088940938058\n",
      "epoch: 82, loss: 0.011871181729833409\n",
      "epoch: 83, loss: 0.011666030647356114\n",
      "epoch: 84, loss: 0.011599588327829232\n",
      "epoch: 85, loss: 0.011594468555627414\n",
      "epoch: 86, loss: 0.011462565103789728\n",
      "epoch: 87, loss: 0.01136637883506804\n",
      "epoch: 88, loss: 0.011666274301518904\n",
      "epoch: 89, loss: 0.011761244673635783\n",
      "epoch: 90, loss: 0.011751392009694646\n",
      "epoch: 91, loss: 0.011941567155828484\n",
      "epoch: 92, loss: 0.011567062631887818\n",
      "epoch: 93, loss: 0.011512395899876297\n",
      "epoch: 94, loss: 0.011779875213616661\n",
      "epoch: 95, loss: 0.011446392740293769\n",
      "epoch: 96, loss: 0.011643244928957864\n",
      "epoch: 97, loss: 0.011805176440860822\n",
      "epoch: 98, loss: 0.011542185477402172\n",
      "epoch: 99, loss: 0.011329484129889187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fbe42e0ef84c6682ed546e5ad8bb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.011395458044639048\n",
      "epoch: 1, loss: 0.011496587908272828\n",
      "epoch: 2, loss: 0.011623204234763637\n",
      "epoch: 3, loss: 0.01158552805269232\n",
      "epoch: 4, loss: 0.011621900008657114\n",
      "epoch: 5, loss: 0.011579825815756308\n",
      "epoch: 6, loss: 0.01185262536970564\n",
      "epoch: 7, loss: 0.011615885598159641\n",
      "epoch: 8, loss: 0.01185320951515932\n",
      "epoch: 9, loss: 0.01160423213636829\n",
      "epoch: 10, loss: 0.01154905729880759\n",
      "epoch: 11, loss: 0.011639819616046328\n",
      "epoch: 12, loss: 0.011441296690776436\n",
      "epoch: 13, loss: 0.011902209654550976\n",
      "epoch: 14, loss: 0.011477121009212379\n",
      "epoch: 15, loss: 0.011518714071355724\n",
      "epoch: 16, loss: 0.011381966281987456\n",
      "epoch: 17, loss: 0.011650268387657576\n",
      "epoch: 18, loss: 0.01154835573475249\n",
      "epoch: 19, loss: 0.011780363476037734\n",
      "epoch: 20, loss: 0.01162627459332815\n",
      "epoch: 21, loss: 0.01158554859965728\n",
      "epoch: 22, loss: 0.011388225538858321\n",
      "epoch: 23, loss: 0.011406276081277948\n",
      "epoch: 24, loss: 0.01142429999733119\n",
      "epoch: 25, loss: 0.011501150683861565\n",
      "epoch: 26, loss: 0.011603640138416796\n",
      "epoch: 27, loss: 0.011346502606298194\n",
      "epoch: 28, loss: 0.011744174319784875\n",
      "epoch: 29, loss: 0.011379070886511135\n",
      "epoch: 30, loss: 0.011740096422694704\n",
      "epoch: 31, loss: 0.011497527541537758\n",
      "epoch: 32, loss: 0.011683878583756187\n",
      "epoch: 33, loss: 0.011436072102364084\n",
      "epoch: 34, loss: 0.011432950347635648\n",
      "epoch: 35, loss: 0.011407967965369352\n",
      "epoch: 36, loss: 0.011532626557572161\n",
      "epoch: 37, loss: 0.011630503910156912\n",
      "epoch: 38, loss: 0.01149553443393118\n",
      "epoch: 39, loss: 0.011462756966593832\n",
      "epoch: 40, loss: 0.011462337019880746\n",
      "epoch: 41, loss: 0.011540330190530556\n",
      "epoch: 42, loss: 0.01138433206114602\n",
      "epoch: 43, loss: 0.01178198834269359\n",
      "epoch: 44, loss: 0.01160975009758516\n",
      "epoch: 45, loss: 0.011632413256832966\n",
      "epoch: 46, loss: 0.011622025164084959\n",
      "epoch: 47, loss: 0.011781731333167232\n",
      "epoch: 48, loss: 0.011665703226628635\n",
      "epoch: 49, loss: 0.011369556728612111\n",
      "epoch: 50, loss: 0.011767719360513133\n",
      "epoch: 51, loss: 0.01177280490995084\n",
      "epoch: 52, loss: 0.011414429901412669\n",
      "epoch: 53, loss: 0.01148514793389377\n",
      "epoch: 54, loss: 0.011349645451911456\n",
      "epoch: 55, loss: 0.011682167391124446\n",
      "epoch: 56, loss: 0.011763525726954334\n",
      "epoch: 57, loss: 0.011664407307047472\n",
      "epoch: 58, loss: 0.011449572118332398\n",
      "epoch: 59, loss: 0.011621067145528545\n",
      "epoch: 60, loss: 0.011743253665154472\n",
      "epoch: 61, loss: 0.011770946942427517\n",
      "epoch: 62, loss: 0.011342809320922864\n",
      "epoch: 63, loss: 0.01168600389327452\n",
      "epoch: 64, loss: 0.011798786659395868\n",
      "epoch: 65, loss: 0.011599417252108684\n",
      "epoch: 66, loss: 0.01176808182573439\n",
      "epoch: 67, loss: 0.011892323927601577\n",
      "epoch: 68, loss: 0.011462159557069121\n",
      "epoch: 69, loss: 0.011385613351086909\n",
      "epoch: 70, loss: 0.01181328395920365\n",
      "epoch: 71, loss: 0.0117159784336468\n",
      "epoch: 72, loss: 0.011679260543861225\n",
      "epoch: 73, loss: 0.011518669955520245\n",
      "epoch: 74, loss: 0.011465485710743491\n",
      "epoch: 75, loss: 0.011450726350108329\n",
      "epoch: 76, loss: 0.011285552077935143\n",
      "epoch: 77, loss: 0.011422752219391905\n",
      "epoch: 78, loss: 0.011790442818458481\n",
      "epoch: 79, loss: 0.011540468634026156\n",
      "epoch: 80, loss: 0.011447192219990129\n",
      "epoch: 81, loss: 0.011658074163054791\n",
      "epoch: 82, loss: 0.01141855260848993\n",
      "epoch: 83, loss: 0.011520423993674889\n",
      "epoch: 84, loss: 0.011665053164268023\n",
      "epoch: 85, loss: 0.011515432832762861\n",
      "epoch: 86, loss: 0.011526796975925013\n",
      "epoch: 87, loss: 0.011874205593988474\n",
      "epoch: 88, loss: 0.011496595934646551\n",
      "epoch: 89, loss: 0.011727289135122778\n",
      "epoch: 90, loss: 0.011589235216148996\n",
      "epoch: 91, loss: 0.01174639497414568\n",
      "epoch: 92, loss: 0.011650856829147734\n",
      "epoch: 93, loss: 0.011428270899842326\n",
      "epoch: 94, loss: 0.01181856486263402\n",
      "epoch: 95, loss: 0.011832348945572942\n",
      "epoch: 96, loss: 0.011508982290960213\n",
      "epoch: 97, loss: 0.011836377115095699\n",
      "epoch: 98, loss: 0.011440231076924469\n",
      "epoch: 99, loss: 0.01157084171612125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52884b21d85f47899b09334e41431fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.012830170917000623\n",
      "epoch: 1, loss: 0.013286565079975712\n",
      "epoch: 2, loss: 0.01301810982753995\n",
      "epoch: 3, loss: 0.01273164277419426\n",
      "epoch: 4, loss: 0.01267885769286802\n",
      "epoch: 5, loss: 0.012823909658653453\n",
      "epoch: 6, loss: 0.012721512032123778\n",
      "epoch: 7, loss: 0.01264640601450535\n",
      "epoch: 8, loss: 0.012784840918092491\n",
      "epoch: 9, loss: 0.012907591716778754\n",
      "epoch: 10, loss: 0.012706501256430589\n",
      "epoch: 11, loss: 0.012931533239427294\n",
      "epoch: 12, loss: 0.01289799948993595\n",
      "epoch: 13, loss: 0.013033845884075922\n",
      "epoch: 14, loss: 0.01270486511734362\n",
      "epoch: 15, loss: 0.01265102168302442\n",
      "epoch: 16, loss: 0.012693962311738739\n",
      "epoch: 17, loss: 0.013225700332664472\n",
      "epoch: 18, loss: 0.012849707211142631\n",
      "epoch: 19, loss: 0.013038383655306785\n",
      "epoch: 20, loss: 0.012702711546881485\n",
      "epoch: 21, loss: 0.013202811422779726\n",
      "epoch: 22, loss: 0.012641849822219033\n",
      "epoch: 23, loss: 0.012882969087052623\n",
      "epoch: 24, loss: 0.013171958659441305\n",
      "epoch: 25, loss: 0.012968439417150975\n",
      "epoch: 26, loss: 0.013038660425903847\n",
      "epoch: 27, loss: 0.0127385293194051\n",
      "epoch: 28, loss: 0.01274919173528074\n",
      "epoch: 29, loss: 0.013060847410273354\n",
      "epoch: 30, loss: 0.012716499929288819\n",
      "epoch: 31, loss: 0.012911014122674577\n",
      "epoch: 32, loss: 0.013040097626820187\n",
      "epoch: 33, loss: 0.012790718810229825\n",
      "epoch: 34, loss: 0.013035351369409077\n",
      "epoch: 35, loss: 0.012974906530659403\n",
      "epoch: 36, loss: 0.012663413370976902\n",
      "epoch: 37, loss: 0.012833384615763024\n",
      "epoch: 38, loss: 0.012654908193240945\n",
      "epoch: 39, loss: 0.012798639446830622\n",
      "epoch: 40, loss: 0.013045585199293377\n",
      "epoch: 41, loss: 0.012918273226758212\n",
      "epoch: 42, loss: 0.013053000663929218\n",
      "epoch: 43, loss: 0.012900182375242255\n",
      "epoch: 44, loss: 0.01289510745063714\n",
      "epoch: 45, loss: 0.01283830738667988\n",
      "epoch: 46, loss: 0.012844416511834458\n",
      "epoch: 47, loss: 0.01295547561393062\n",
      "epoch: 48, loss: 0.013039842348409334\n",
      "epoch: 49, loss: 0.012911790592329539\n",
      "epoch: 50, loss: 0.013019159359286301\n",
      "epoch: 51, loss: 0.013064683377854762\n",
      "epoch: 52, loss: 0.01284107889295605\n",
      "epoch: 53, loss: 0.012653118117470531\n",
      "epoch: 54, loss: 0.012758180076506684\n",
      "epoch: 55, loss: 0.012757720316426677\n",
      "epoch: 56, loss: 0.012635690608446937\n",
      "epoch: 57, loss: 0.012713228237935255\n",
      "epoch: 58, loss: 0.012637070482744747\n",
      "epoch: 59, loss: 0.012669520652161927\n",
      "epoch: 60, loss: 0.012932478560322496\n",
      "epoch: 61, loss: 0.012789218313248588\n",
      "epoch: 62, loss: 0.01287462477729651\n",
      "epoch: 63, loss: 0.013118161913156815\n",
      "epoch: 64, loss: 0.012946326561499815\n",
      "epoch: 65, loss: 0.013165212934897027\n",
      "epoch: 66, loss: 0.012936667746050822\n",
      "epoch: 67, loss: 0.012904720604469752\n",
      "epoch: 68, loss: 0.012853530692197326\n",
      "epoch: 69, loss: 0.012831162091537375\n",
      "epoch: 70, loss: 0.013064103613014898\n",
      "epoch: 71, loss: 0.012926153220329195\n",
      "epoch: 72, loss: 0.013082617514533183\n",
      "epoch: 73, loss: 0.01298200655593341\n",
      "epoch: 74, loss: 0.012915536698624441\n",
      "epoch: 75, loss: 0.012790055026163856\n",
      "epoch: 76, loss: 0.012872330061842567\n",
      "epoch: 77, loss: 0.013018206789549827\n",
      "epoch: 78, loss: 0.012844669211572565\n",
      "epoch: 79, loss: 0.012971189472278212\n",
      "epoch: 80, loss: 0.012780295614817019\n",
      "epoch: 81, loss: 0.012763605687146002\n",
      "epoch: 82, loss: 0.013011511858070224\n",
      "epoch: 83, loss: 0.013131926000577653\n",
      "epoch: 84, loss: 0.01292167345586765\n",
      "epoch: 85, loss: 0.012812639344421842\n",
      "epoch: 86, loss: 0.012838082768372103\n",
      "epoch: 87, loss: 0.012838695117100073\n",
      "epoch: 88, loss: 0.012908761115236965\n",
      "epoch: 89, loss: 0.012960145526986684\n",
      "epoch: 90, loss: 0.012959865760260914\n",
      "epoch: 91, loss: 0.012967262026886169\n",
      "epoch: 92, loss: 0.013042565095220582\n",
      "epoch: 93, loss: 0.012771998868247121\n",
      "epoch: 94, loss: 0.01257430877679796\n",
      "epoch: 95, loss: 0.012769979430169458\n",
      "epoch: 96, loss: 0.012891742208885197\n",
      "epoch: 97, loss: 0.01277657281433326\n",
      "epoch: 98, loss: 0.01292623090210441\n",
      "epoch: 99, loss: 0.012825054598250304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546c812a29d141e69cf07377a6b35157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.00757305037939131\n",
      "epoch: 1, loss: 0.00810400250648579\n",
      "epoch: 2, loss: 0.008193873022896531\n",
      "epoch: 3, loss: 0.007331170637551201\n",
      "epoch: 4, loss: 0.00698308776212629\n",
      "epoch: 5, loss: 0.007165659768309427\n",
      "epoch: 6, loss: 0.007310049787245225\n",
      "epoch: 7, loss: 0.006760884523997457\n",
      "epoch: 8, loss: 0.00700884917138737\n",
      "epoch: 9, loss: 0.006655376276473736\n",
      "epoch: 10, loss: 0.006505260537951875\n",
      "epoch: 11, loss: 0.006508849878684826\n",
      "epoch: 12, loss: 0.007001350591818453\n",
      "epoch: 13, loss: 0.006568007832344526\n",
      "epoch: 14, loss: 0.0066333885427617\n",
      "epoch: 15, loss: 0.007127913958471139\n",
      "epoch: 16, loss: 0.006856543404083683\n",
      "epoch: 17, loss: 0.006810416019816984\n",
      "epoch: 18, loss: 0.00650199319542083\n",
      "epoch: 19, loss: 0.006731507717234488\n",
      "epoch: 20, loss: 0.006424400554131115\n",
      "epoch: 21, loss: 0.006554225432557837\n",
      "epoch: 22, loss: 0.0065547057219452155\n",
      "epoch: 23, loss: 0.00658667260102222\n",
      "epoch: 24, loss: 0.0064546770864141526\n",
      "epoch: 25, loss: 0.006649045133827209\n",
      "epoch: 26, loss: 0.006549927615791342\n",
      "epoch: 27, loss: 0.0064638593218047415\n",
      "epoch: 28, loss: 0.006412728450156258\n",
      "epoch: 29, loss: 0.006515289852910271\n",
      "epoch: 30, loss: 0.006387799963819248\n",
      "epoch: 31, loss: 0.0064448403714731\n",
      "epoch: 32, loss: 0.006530842702719236\n",
      "epoch: 33, loss: 0.0066510254120262235\n",
      "epoch: 34, loss: 0.006422156160301087\n",
      "epoch: 35, loss: 0.006575994432545907\n",
      "epoch: 36, loss: 0.006203150590234171\n",
      "epoch: 37, loss: 0.006158284120405398\n",
      "epoch: 38, loss: 0.006563138267874929\n",
      "epoch: 39, loss: 0.00651783054833063\n",
      "epoch: 40, loss: 0.006510862138914388\n",
      "epoch: 41, loss: 0.0065874058545460175\n",
      "epoch: 42, loss: 0.006302655795570573\n",
      "epoch: 43, loss: 0.006609772101179743\n",
      "epoch: 44, loss: 0.006514715005857182\n",
      "epoch: 45, loss: 0.006464159784110423\n",
      "epoch: 46, loss: 0.006333501172832046\n",
      "epoch: 47, loss: 0.006424294051412873\n",
      "epoch: 48, loss: 0.00644859430008517\n",
      "epoch: 49, loss: 0.00649862443899902\n",
      "epoch: 50, loss: 0.006448913512974149\n",
      "epoch: 51, loss: 0.006575835829316029\n",
      "epoch: 52, loss: 0.006504036178338118\n",
      "epoch: 53, loss: 0.0064494576920908556\n",
      "epoch: 54, loss: 0.006300922406397464\n",
      "epoch: 55, loss: 0.006387063089292788\n",
      "epoch: 56, loss: 0.00665803706574795\n",
      "epoch: 57, loss: 0.006543481578050475\n",
      "epoch: 58, loss: 0.006164050391298005\n",
      "epoch: 59, loss: 0.00664257941117758\n",
      "epoch: 60, loss: 0.0065779474663667774\n",
      "epoch: 61, loss: 0.006506543883553202\n",
      "epoch: 62, loss: 0.006594168431135968\n",
      "epoch: 63, loss: 0.0065234492864307565\n",
      "epoch: 64, loss: 0.006407741905738408\n",
      "epoch: 65, loss: 0.006157813458628296\n",
      "epoch: 66, loss: 0.006756178964857166\n",
      "epoch: 67, loss: 0.006465802371888239\n",
      "epoch: 68, loss: 0.006486271016485463\n",
      "epoch: 69, loss: 0.006452773496727582\n",
      "epoch: 70, loss: 0.00634766899322405\n",
      "epoch: 71, loss: 0.006187843348122588\n",
      "epoch: 72, loss: 0.0063774961073602195\n",
      "epoch: 73, loss: 0.006649050165376664\n",
      "epoch: 74, loss: 0.006559441231132144\n",
      "epoch: 75, loss: 0.006446565578063707\n",
      "epoch: 76, loss: 0.006275197975486494\n",
      "epoch: 77, loss: 0.006519001003486673\n",
      "epoch: 78, loss: 0.006523203448264898\n",
      "epoch: 79, loss: 0.006423567139572134\n",
      "epoch: 80, loss: 0.006265654171458905\n",
      "epoch: 81, loss: 0.006390773751369083\n",
      "epoch: 82, loss: 0.006505295838551085\n",
      "epoch: 83, loss: 0.006629841192681267\n",
      "epoch: 84, loss: 0.006345547208688212\n",
      "epoch: 85, loss: 0.006515384482858682\n",
      "epoch: 86, loss: 0.0062938174453956545\n",
      "epoch: 87, loss: 0.006457798042250816\n",
      "epoch: 88, loss: 0.006540667035538077\n",
      "epoch: 89, loss: 0.006666396685997978\n",
      "epoch: 90, loss: 0.006495038824939594\n",
      "epoch: 91, loss: 0.006658591396410191\n",
      "epoch: 92, loss: 0.006325432342553453\n",
      "epoch: 93, loss: 0.006124054342087638\n",
      "epoch: 94, loss: 0.006467431455797814\n",
      "epoch: 95, loss: 0.006430763605553829\n",
      "epoch: 96, loss: 0.006447780370672674\n",
      "epoch: 97, loss: 0.0065183082616798015\n",
      "epoch: 98, loss: 0.006290260378475239\n",
      "epoch: 99, loss: 0.006401627656620207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0402e2f3fe3461c840b7bd44968589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008969581782736121\n",
      "epoch: 1, loss: 0.008990403219843431\n",
      "epoch: 2, loss: 0.008950523233613623\n",
      "epoch: 3, loss: 0.008989770207221673\n",
      "epoch: 4, loss: 0.00912623588076741\n",
      "epoch: 5, loss: 0.008862800969600538\n",
      "epoch: 6, loss: 0.008962413682313139\n",
      "epoch: 7, loss: 0.008990016220524592\n",
      "epoch: 8, loss: 0.009013208473416901\n",
      "epoch: 9, loss: 0.009120436046291765\n",
      "epoch: 10, loss: 0.008966136350913322\n",
      "epoch: 11, loss: 0.009035606635332913\n",
      "epoch: 12, loss: 0.009037520082946893\n",
      "epoch: 13, loss: 0.008757138841433799\n",
      "epoch: 14, loss: 0.0088387335276872\n",
      "epoch: 15, loss: 0.008871449880655883\n",
      "epoch: 16, loss: 0.008920015591073438\n",
      "epoch: 17, loss: 0.009014859992911892\n",
      "epoch: 18, loss: 0.009217522304935322\n",
      "epoch: 19, loss: 0.00908088926186342\n",
      "epoch: 20, loss: 0.008895946925540117\n",
      "epoch: 21, loss: 0.008958517857740402\n",
      "epoch: 22, loss: 0.009140115659516973\n",
      "epoch: 23, loss: 0.008904817737129265\n",
      "epoch: 24, loss: 0.00907392866136311\n",
      "epoch: 25, loss: 0.009026199667109498\n",
      "epoch: 26, loss: 0.009063073794895325\n",
      "epoch: 27, loss: 0.008977878065123544\n",
      "epoch: 28, loss: 0.008940046127415203\n",
      "epoch: 29, loss: 0.008913452703250032\n",
      "epoch: 30, loss: 0.008876924214617887\n",
      "epoch: 31, loss: 0.008925014341722024\n",
      "epoch: 32, loss: 0.009099685838312192\n",
      "epoch: 33, loss: 0.009017203800045297\n",
      "epoch: 34, loss: 0.009036948301056377\n",
      "epoch: 35, loss: 0.009117540453078495\n",
      "epoch: 36, loss: 0.00891743529533101\n",
      "epoch: 37, loss: 0.008789365224999858\n",
      "epoch: 38, loss: 0.008854277118260253\n",
      "epoch: 39, loss: 0.00904597277946286\n",
      "epoch: 40, loss: 0.008877365481930006\n",
      "epoch: 41, loss: 0.009041349554333658\n",
      "epoch: 42, loss: 0.009075401697003362\n",
      "epoch: 43, loss: 0.008906307918912263\n",
      "epoch: 44, loss: 0.0089765588405734\n",
      "epoch: 45, loss: 0.00914201928222789\n",
      "epoch: 46, loss: 0.008805730842469554\n",
      "epoch: 47, loss: 0.009244148108071651\n",
      "epoch: 48, loss: 0.008976076844310633\n",
      "epoch: 49, loss: 0.008847262549720148\n",
      "epoch: 50, loss: 0.008917763163801927\n",
      "epoch: 51, loss: 0.008956767329693116\n",
      "epoch: 52, loss: 0.009040070040336004\n",
      "epoch: 53, loss: 0.009076367939798378\n",
      "epoch: 54, loss: 0.009023521678209065\n",
      "epoch: 55, loss: 0.008889310940837301\n",
      "epoch: 56, loss: 0.008792066961117874\n",
      "epoch: 57, loss: 0.009057260543434113\n",
      "epoch: 58, loss: 0.00903668121691738\n",
      "epoch: 59, loss: 0.008963341911969123\n",
      "epoch: 60, loss: 0.008779046374590743\n",
      "epoch: 61, loss: 0.009212519954982946\n",
      "epoch: 62, loss: 0.00901056964176464\n",
      "epoch: 63, loss: 0.009012875790794768\n",
      "epoch: 64, loss: 0.00889557890674872\n",
      "epoch: 65, loss: 0.009122168084382503\n",
      "epoch: 66, loss: 0.008951816439380113\n",
      "epoch: 67, loss: 0.00898701459717762\n",
      "epoch: 68, loss: 0.008963371214453555\n",
      "epoch: 69, loss: 0.008928028750265873\n",
      "epoch: 70, loss: 0.009043901365648302\n",
      "epoch: 71, loss: 0.008967977961252815\n",
      "epoch: 72, loss: 0.008892978617451634\n",
      "epoch: 73, loss: 0.008973669855420146\n",
      "epoch: 74, loss: 0.009018026008868456\n",
      "epoch: 75, loss: 0.008934962922820724\n",
      "epoch: 76, loss: 0.009025594990674098\n",
      "epoch: 77, loss: 0.008998377107163\n",
      "epoch: 78, loss: 0.008982202246646084\n",
      "epoch: 79, loss: 0.009036767600126014\n",
      "epoch: 80, loss: 0.008857515066258217\n",
      "epoch: 81, loss: 0.008961549372741627\n",
      "epoch: 82, loss: 0.00919663657283898\n",
      "epoch: 83, loss: 0.008964697085312634\n",
      "epoch: 84, loss: 0.008987944484862268\n",
      "epoch: 85, loss: 0.008971955157017427\n",
      "epoch: 86, loss: 0.00916278763847819\n",
      "epoch: 87, loss: 0.009059365038525594\n",
      "epoch: 88, loss: 0.009069035941801857\n",
      "epoch: 89, loss: 0.008924975966141393\n",
      "epoch: 90, loss: 0.009133118551063324\n",
      "epoch: 91, loss: 0.00901918738101074\n",
      "epoch: 92, loss: 0.008940198811048624\n",
      "epoch: 93, loss: 0.009070870285644343\n",
      "epoch: 94, loss: 0.009229243081791759\n",
      "epoch: 95, loss: 0.009036917812711577\n",
      "epoch: 96, loss: 0.008861623117749014\n",
      "epoch: 97, loss: 0.008866434796968161\n",
      "epoch: 98, loss: 0.009182640633141621\n",
      "epoch: 99, loss: 0.009024147144189454\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = loader(data_path(\"trainability_qnn_3D_reps_1\"))\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = qnn_list[i]\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1_epochs_200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d136f3302a482a9f8a0e3afba11d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82947f3a578341a39eb4922413d90c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008746134233797146\n",
      "epoch: 1, loss: 0.008937596194172405\n",
      "epoch: 2, loss: 0.009081986034377626\n",
      "epoch: 3, loss: 0.008913356035676508\n",
      "epoch: 4, loss: 0.008855881456499522\n",
      "epoch: 5, loss: 0.0090878548651503\n",
      "epoch: 6, loss: 0.00903318791048731\n",
      "epoch: 7, loss: 0.008972101147966675\n",
      "epoch: 8, loss: 0.008797889716290306\n",
      "epoch: 9, loss: 0.008860376932476042\n",
      "epoch: 10, loss: 0.009090441798964188\n",
      "epoch: 11, loss: 0.008985500336858689\n",
      "epoch: 12, loss: 0.008907518800971678\n",
      "epoch: 13, loss: 0.008848379263234084\n",
      "epoch: 14, loss: 0.009143261966099983\n",
      "epoch: 15, loss: 0.00892009940081591\n",
      "epoch: 16, loss: 0.00908063240543046\n",
      "epoch: 17, loss: 0.00896665215738498\n",
      "epoch: 18, loss: 0.008837618267906536\n",
      "epoch: 19, loss: 0.008855297073471644\n",
      "epoch: 20, loss: 0.008858821944218805\n",
      "epoch: 21, loss: 0.008822119436677232\n",
      "epoch: 22, loss: 0.008849292406866981\n",
      "epoch: 23, loss: 0.008912835486228494\n",
      "epoch: 24, loss: 0.00879436061032625\n",
      "epoch: 25, loss: 0.008715883856657634\n",
      "epoch: 26, loss: 0.008795829553032406\n",
      "epoch: 27, loss: 0.008915591886263395\n",
      "epoch: 28, loss: 0.008704512415193254\n",
      "epoch: 29, loss: 0.008793739853547882\n",
      "epoch: 30, loss: 0.008832497730038292\n",
      "epoch: 31, loss: 0.008674511082899287\n",
      "epoch: 32, loss: 0.00874758554938424\n",
      "epoch: 33, loss: 0.008702698571611829\n",
      "epoch: 34, loss: 0.008808556968460416\n",
      "epoch: 35, loss: 0.00873259450957573\n",
      "epoch: 36, loss: 0.008861391038533822\n",
      "epoch: 37, loss: 0.008667877895838848\n",
      "epoch: 38, loss: 0.008700995709396989\n",
      "epoch: 39, loss: 0.008840010989441574\n",
      "epoch: 40, loss: 0.008690169837763167\n",
      "epoch: 41, loss: 0.008865024770073844\n",
      "epoch: 42, loss: 0.008699323122790262\n",
      "epoch: 43, loss: 0.008635488045852601\n",
      "epoch: 44, loss: 0.008775405863784324\n",
      "epoch: 45, loss: 0.008716617450885209\n",
      "epoch: 46, loss: 0.008539121810434494\n",
      "epoch: 47, loss: 0.008706789103791879\n",
      "epoch: 48, loss: 0.008652062857639926\n",
      "epoch: 49, loss: 0.008707211126983524\n",
      "epoch: 50, loss: 0.008707986306198323\n",
      "epoch: 51, loss: 0.00841758773050595\n",
      "epoch: 52, loss: 0.008806008476495692\n",
      "epoch: 53, loss: 0.00851430913995128\n",
      "epoch: 54, loss: 0.00839797974476104\n",
      "epoch: 55, loss: 0.008491771454527843\n",
      "epoch: 56, loss: 0.008599271572444759\n",
      "epoch: 57, loss: 0.008453276991822224\n",
      "epoch: 58, loss: 0.008462999921148283\n",
      "epoch: 59, loss: 0.00854497780429296\n",
      "epoch: 60, loss: 0.008324934891019724\n",
      "epoch: 61, loss: 0.008660454905396173\n",
      "epoch: 62, loss: 0.008473104472052044\n",
      "epoch: 63, loss: 0.008453640798008567\n",
      "epoch: 64, loss: 0.008492435963291436\n",
      "epoch: 65, loss: 0.008440832955660124\n",
      "epoch: 66, loss: 0.008537248454681809\n",
      "epoch: 67, loss: 0.008331261834823578\n",
      "epoch: 68, loss: 0.00855088761755749\n",
      "epoch: 69, loss: 0.008447179400058654\n",
      "epoch: 70, loss: 0.00835516830183461\n",
      "epoch: 71, loss: 0.008328597251256117\n",
      "epoch: 72, loss: 0.008452819624580195\n",
      "epoch: 73, loss: 0.008631985114923166\n",
      "epoch: 74, loss: 0.008458331980848091\n",
      "epoch: 75, loss: 0.008325192697743884\n",
      "epoch: 76, loss: 0.00834691592521066\n",
      "epoch: 77, loss: 0.008353556789834371\n",
      "epoch: 78, loss: 0.00839871182109238\n",
      "epoch: 79, loss: 0.008443196823090046\n",
      "epoch: 80, loss: 0.00833863749263554\n",
      "epoch: 81, loss: 0.00850968064646305\n",
      "epoch: 82, loss: 0.00826869448639831\n",
      "epoch: 83, loss: 0.008343753861898591\n",
      "epoch: 84, loss: 0.008447925502364773\n",
      "epoch: 85, loss: 0.008506990870221156\n",
      "epoch: 86, loss: 0.0082735629290741\n",
      "epoch: 87, loss: 0.008307386637221373\n",
      "epoch: 88, loss: 0.00848868340180873\n",
      "epoch: 89, loss: 0.008317263712210357\n",
      "epoch: 90, loss: 0.00801040955722858\n",
      "epoch: 91, loss: 0.008235181214321713\n",
      "epoch: 92, loss: 0.008111509962141617\n",
      "epoch: 93, loss: 0.008160770359445756\n",
      "epoch: 94, loss: 0.00792817084663418\n",
      "epoch: 95, loss: 0.007743119643158864\n",
      "epoch: 96, loss: 0.007770692251205103\n",
      "epoch: 97, loss: 0.007910897050262839\n",
      "epoch: 98, loss: 0.007620907653085989\n",
      "epoch: 99, loss: 0.007577857941494868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b1d2f8c0fc483f93ef0047c0d14459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.009055371535323993\n",
      "epoch: 1, loss: 0.009067331949588534\n",
      "epoch: 2, loss: 0.009041857263811407\n",
      "epoch: 3, loss: 0.009077083198140027\n",
      "epoch: 4, loss: 0.008988829505578772\n",
      "epoch: 5, loss: 0.008939845944554194\n",
      "epoch: 6, loss: 0.008926057478245427\n",
      "epoch: 7, loss: 0.00904165863835962\n",
      "epoch: 8, loss: 0.009083069868847906\n",
      "epoch: 9, loss: 0.008917295097869414\n",
      "epoch: 10, loss: 0.009097310799236083\n",
      "epoch: 11, loss: 0.008977048601755277\n",
      "epoch: 12, loss: 0.00913737384679398\n",
      "epoch: 13, loss: 0.008915762313828079\n",
      "epoch: 14, loss: 0.009095273208648126\n",
      "epoch: 15, loss: 0.008875817555474997\n",
      "epoch: 16, loss: 0.008985186227912475\n",
      "epoch: 17, loss: 0.008961790222533695\n",
      "epoch: 18, loss: 0.008931200312599032\n",
      "epoch: 19, loss: 0.008972477528718952\n",
      "epoch: 20, loss: 0.00878275723009505\n",
      "epoch: 21, loss: 0.009029806067163281\n",
      "epoch: 22, loss: 0.008893867728750617\n",
      "epoch: 23, loss: 0.008862523072425453\n",
      "epoch: 24, loss: 0.008964072447301331\n",
      "epoch: 25, loss: 0.008763632154749146\n",
      "epoch: 26, loss: 0.008921313708046271\n",
      "epoch: 27, loss: 0.008909883224371086\n",
      "epoch: 28, loss: 0.008728014740897459\n",
      "epoch: 29, loss: 0.008801937822718351\n",
      "epoch: 30, loss: 0.008877363394044309\n",
      "epoch: 31, loss: 0.00866757642197645\n",
      "epoch: 32, loss: 0.008780724767592075\n",
      "epoch: 33, loss: 0.008818333481862149\n",
      "epoch: 34, loss: 0.008800642153097267\n",
      "epoch: 35, loss: 0.00873837944951971\n",
      "epoch: 36, loss: 0.008710658194264207\n",
      "epoch: 37, loss: 0.008732079349978276\n",
      "epoch: 38, loss: 0.008680339043762864\n",
      "epoch: 39, loss: 0.008539221770609978\n",
      "epoch: 40, loss: 0.008628704610815488\n",
      "epoch: 41, loss: 0.008804276593304626\n",
      "epoch: 42, loss: 0.00873699323139827\n",
      "epoch: 43, loss: 0.008717377354711701\n",
      "epoch: 44, loss: 0.008716695966506658\n",
      "epoch: 45, loss: 0.00866969682088075\n",
      "epoch: 46, loss: 0.008682319607529925\n",
      "epoch: 47, loss: 0.008676221889980788\n",
      "epoch: 48, loss: 0.00869297358877062\n",
      "epoch: 49, loss: 0.008653529591434962\n",
      "epoch: 50, loss: 0.008827513448019667\n",
      "epoch: 51, loss: 0.008752971976630927\n",
      "epoch: 52, loss: 0.008618710279793166\n",
      "epoch: 53, loss: 0.008684106336780049\n",
      "epoch: 54, loss: 0.00872091726458616\n",
      "epoch: 55, loss: 0.008857323385522456\n",
      "epoch: 56, loss: 0.008720729858889307\n",
      "epoch: 57, loss: 0.008625506727134193\n",
      "epoch: 58, loss: 0.00863152378902312\n",
      "epoch: 59, loss: 0.008696924216344678\n",
      "epoch: 60, loss: 0.008694769757304623\n",
      "epoch: 61, loss: 0.008665128824595462\n",
      "epoch: 62, loss: 0.008565517623980024\n",
      "epoch: 63, loss: 0.008701543754048332\n",
      "epoch: 64, loss: 0.008643178842493925\n",
      "epoch: 65, loss: 0.00877093190338437\n",
      "epoch: 66, loss: 0.008743487629963869\n",
      "epoch: 67, loss: 0.008743389814486338\n",
      "epoch: 68, loss: 0.008805827795007524\n",
      "epoch: 69, loss: 0.008724420149736034\n",
      "epoch: 70, loss: 0.008607706689160315\n",
      "epoch: 71, loss: 0.008744578322560665\n",
      "epoch: 72, loss: 0.00867133465236007\n",
      "epoch: 73, loss: 0.008678243948632373\n",
      "epoch: 74, loss: 0.00868725965707072\n",
      "epoch: 75, loss: 0.00867869374105109\n",
      "epoch: 76, loss: 0.008733797129164146\n",
      "epoch: 77, loss: 0.008710559835989598\n",
      "epoch: 78, loss: 0.008532687982031408\n",
      "epoch: 79, loss: 0.0088642922259701\n",
      "epoch: 80, loss: 0.008674755662014477\n",
      "epoch: 81, loss: 0.008644846940250365\n",
      "epoch: 82, loss: 0.008724311257877239\n",
      "epoch: 83, loss: 0.008812912679913347\n",
      "epoch: 84, loss: 0.008556108689953065\n",
      "epoch: 85, loss: 0.008579312347771546\n",
      "epoch: 86, loss: 0.008886671618065027\n",
      "epoch: 87, loss: 0.008550388135216554\n",
      "epoch: 88, loss: 0.00862330936201036\n",
      "epoch: 89, loss: 0.008713298161488612\n",
      "epoch: 90, loss: 0.0087175377649968\n",
      "epoch: 91, loss: 0.008744567044470162\n",
      "epoch: 92, loss: 0.008738442962131306\n",
      "epoch: 93, loss: 0.008510987888398434\n",
      "epoch: 94, loss: 0.008584678745963104\n",
      "epoch: 95, loss: 0.008638275290030223\n",
      "epoch: 96, loss: 0.008703516427609538\n",
      "epoch: 97, loss: 0.008763861654466652\n",
      "epoch: 98, loss: 0.00876800021934648\n",
      "epoch: 99, loss: 0.008674107972757647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5c39c21a8a43219cbdba073e645c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.00989933639338777\n",
      "epoch: 1, loss: 0.009930869123856505\n",
      "epoch: 2, loss: 0.010000345571632445\n",
      "epoch: 3, loss: 0.009994210965177172\n",
      "epoch: 4, loss: 0.009845751794893665\n",
      "epoch: 5, loss: 0.009599680874598516\n",
      "epoch: 6, loss: 0.009531611455659202\n",
      "epoch: 7, loss: 0.009504027846474743\n",
      "epoch: 8, loss: 0.009456416855735098\n",
      "epoch: 9, loss: 0.009217964603137735\n",
      "epoch: 10, loss: 0.009140590456779155\n",
      "epoch: 11, loss: 0.008983097365733675\n",
      "epoch: 12, loss: 0.009159160693452097\n",
      "epoch: 13, loss: 0.009095798064490747\n",
      "epoch: 14, loss: 0.008788174313210243\n",
      "epoch: 15, loss: 0.008884063830072118\n",
      "epoch: 16, loss: 0.008917762687263865\n",
      "epoch: 17, loss: 0.008757223377408899\n",
      "epoch: 18, loss: 0.008814979057955767\n",
      "epoch: 19, loss: 0.00874464950003251\n",
      "epoch: 20, loss: 0.008844559854578018\n",
      "epoch: 21, loss: 0.008826687763995256\n",
      "epoch: 22, loss: 0.009044861225583786\n",
      "epoch: 23, loss: 0.008644507727056092\n",
      "epoch: 24, loss: 0.008677208928179496\n",
      "epoch: 25, loss: 0.008611019148274321\n",
      "epoch: 26, loss: 0.008676611880196012\n",
      "epoch: 27, loss: 0.008616257816295195\n",
      "epoch: 28, loss: 0.008745036039060997\n",
      "epoch: 29, loss: 0.008453396470102969\n",
      "epoch: 30, loss: 0.008385918253305516\n",
      "epoch: 31, loss: 0.008658626259990991\n",
      "epoch: 32, loss: 0.008456819506570767\n",
      "epoch: 33, loss: 0.008455880504254169\n",
      "epoch: 34, loss: 0.008623059730517371\n",
      "epoch: 35, loss: 0.00859996985623747\n",
      "epoch: 36, loss: 0.008626271161875108\n",
      "epoch: 37, loss: 0.008619536394520707\n",
      "epoch: 38, loss: 0.008586918754148088\n",
      "epoch: 39, loss: 0.008409245625428767\n",
      "epoch: 40, loss: 0.008500089583215733\n",
      "epoch: 41, loss: 0.008380393316652373\n",
      "epoch: 42, loss: 0.008354758413046323\n",
      "epoch: 43, loss: 0.008292181005967002\n",
      "epoch: 44, loss: 0.008351648916544364\n",
      "epoch: 45, loss: 0.008602165593185837\n",
      "epoch: 46, loss: 0.008406745060331004\n",
      "epoch: 47, loss: 0.00829428773080734\n",
      "epoch: 48, loss: 0.008377107714525621\n",
      "epoch: 49, loss: 0.00849453979400958\n",
      "epoch: 50, loss: 0.008245859491120264\n",
      "epoch: 51, loss: 0.008389641755723543\n",
      "epoch: 52, loss: 0.008543544152356261\n",
      "epoch: 53, loss: 0.00844527446311711\n",
      "epoch: 54, loss: 0.008326567380912787\n",
      "epoch: 55, loss: 0.008497387696932176\n",
      "epoch: 56, loss: 0.008262185327223791\n",
      "epoch: 57, loss: 0.008470414258551164\n",
      "epoch: 58, loss: 0.008323394022419543\n",
      "epoch: 59, loss: 0.008548617338381874\n",
      "epoch: 60, loss: 0.00855223951633005\n",
      "epoch: 61, loss: 0.00839572282206103\n",
      "epoch: 62, loss: 0.008416725597726566\n",
      "epoch: 63, loss: 0.00827456441037262\n",
      "epoch: 64, loss: 0.008342875856003728\n",
      "epoch: 65, loss: 0.008379047580147323\n",
      "epoch: 66, loss: 0.008413252903775034\n",
      "epoch: 67, loss: 0.00861753672379595\n",
      "epoch: 68, loss: 0.008413849612939648\n",
      "epoch: 69, loss: 0.008500319991062658\n",
      "epoch: 70, loss: 0.008563855043045838\n",
      "epoch: 71, loss: 0.008324197985826667\n",
      "epoch: 72, loss: 0.00831803476063698\n",
      "epoch: 73, loss: 0.008341138094168678\n",
      "epoch: 74, loss: 0.008323198499613004\n",
      "epoch: 75, loss: 0.008320619732272947\n",
      "epoch: 76, loss: 0.008318806083412787\n",
      "epoch: 77, loss: 0.008396252505882891\n",
      "epoch: 78, loss: 0.008365886111203555\n",
      "epoch: 79, loss: 0.008230359658720446\n",
      "epoch: 80, loss: 0.00836451728948897\n",
      "epoch: 81, loss: 0.008320482407126496\n",
      "epoch: 82, loss: 0.008229466248930542\n",
      "epoch: 83, loss: 0.008346268053000823\n",
      "epoch: 84, loss: 0.008278296883398654\n",
      "epoch: 85, loss: 0.008309380788044097\n",
      "epoch: 86, loss: 0.008438746325981532\n",
      "epoch: 87, loss: 0.00847313460406833\n",
      "epoch: 88, loss: 0.008227779420071859\n",
      "epoch: 89, loss: 0.008334035952744675\n",
      "epoch: 90, loss: 0.00838891334539668\n",
      "epoch: 91, loss: 0.008285041370177932\n",
      "epoch: 92, loss: 0.008402116272642316\n",
      "epoch: 93, loss: 0.008489879821697719\n",
      "epoch: 94, loss: 0.008299923385739975\n",
      "epoch: 95, loss: 0.008349944373722074\n",
      "epoch: 96, loss: 0.008412060275690874\n",
      "epoch: 97, loss: 0.008542892581529554\n",
      "epoch: 98, loss: 0.008286627475595283\n",
      "epoch: 99, loss: 0.008141837093100854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79e60cf2f8f453c8d50abce88ab69e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008102538306410239\n",
      "epoch: 1, loss: 0.008084062653029354\n",
      "epoch: 2, loss: 0.008156739522289512\n",
      "epoch: 3, loss: 0.007813203669178601\n",
      "epoch: 4, loss: 0.008212897488985757\n",
      "epoch: 5, loss: 0.008125856080069487\n",
      "epoch: 6, loss: 0.007953940263476969\n",
      "epoch: 7, loss: 0.008197058121678237\n",
      "epoch: 8, loss: 0.008037712353876235\n",
      "epoch: 9, loss: 0.00810022129593939\n",
      "epoch: 10, loss: 0.00807393756213914\n",
      "epoch: 11, loss: 0.008039992669074045\n",
      "epoch: 12, loss: 0.008038608868575473\n",
      "epoch: 13, loss: 0.008130989870517493\n",
      "epoch: 14, loss: 0.008054774462851567\n",
      "epoch: 15, loss: 0.008133608028869085\n",
      "epoch: 16, loss: 0.008170270317660858\n",
      "epoch: 17, loss: 0.00806886705813575\n",
      "epoch: 18, loss: 0.008170451937802643\n",
      "epoch: 19, loss: 0.00813750338581141\n",
      "epoch: 20, loss: 0.008262341522775904\n",
      "epoch: 21, loss: 0.007882458035645575\n",
      "epoch: 22, loss: 0.00792411307367969\n",
      "epoch: 23, loss: 0.008276269515941681\n",
      "epoch: 24, loss: 0.008115101675799015\n",
      "epoch: 25, loss: 0.008044732942717294\n",
      "epoch: 26, loss: 0.00812157561611271\n",
      "epoch: 27, loss: 0.008149430927724867\n",
      "epoch: 28, loss: 0.008144752533896913\n",
      "epoch: 29, loss: 0.008041478316906615\n",
      "epoch: 30, loss: 0.00790133139264754\n",
      "epoch: 31, loss: 0.008098908401694074\n",
      "epoch: 32, loss: 0.008062014041557448\n",
      "epoch: 33, loss: 0.008004565904292792\n",
      "epoch: 34, loss: 0.007881999414776044\n",
      "epoch: 35, loss: 0.0079036512579841\n",
      "epoch: 36, loss: 0.007686121440028336\n",
      "epoch: 37, loss: 0.007694189471873185\n",
      "epoch: 38, loss: 0.007485458028270771\n",
      "epoch: 39, loss: 0.007551911967631074\n",
      "epoch: 40, loss: 0.007427361434650358\n",
      "epoch: 41, loss: 0.0073419097970962004\n",
      "epoch: 42, loss: 0.007345546794320724\n",
      "epoch: 43, loss: 0.007226920424178368\n",
      "epoch: 44, loss: 0.00715559884857168\n",
      "epoch: 45, loss: 0.006948361922236656\n",
      "epoch: 46, loss: 0.007119276480784899\n",
      "epoch: 47, loss: 0.0070247709416696134\n",
      "epoch: 48, loss: 0.007180213791294009\n",
      "epoch: 49, loss: 0.00678327085990106\n",
      "epoch: 50, loss: 0.006859942693200732\n",
      "epoch: 51, loss: 0.0067868004034472035\n",
      "epoch: 52, loss: 0.006887606062432944\n",
      "epoch: 53, loss: 0.006726464525160267\n",
      "epoch: 54, loss: 0.006634471639700079\n",
      "epoch: 55, loss: 0.00648777318561598\n",
      "epoch: 56, loss: 0.006558900545645128\n",
      "epoch: 57, loss: 0.00638463309335009\n",
      "epoch: 58, loss: 0.00624292138833527\n",
      "epoch: 59, loss: 0.006384429761693356\n",
      "epoch: 60, loss: 0.006142709404460082\n",
      "epoch: 61, loss: 0.006116598313392197\n",
      "epoch: 62, loss: 0.006067079032336214\n",
      "epoch: 63, loss: 0.006001166219504206\n",
      "epoch: 64, loss: 0.006117119315031402\n",
      "epoch: 65, loss: 0.006115136350626531\n",
      "epoch: 66, loss: 0.006082104256399774\n",
      "epoch: 67, loss: 0.005916542791162993\n",
      "epoch: 68, loss: 0.006195392355016534\n",
      "epoch: 69, loss: 0.006209400297856708\n",
      "epoch: 70, loss: 0.00600559859776876\n",
      "epoch: 71, loss: 0.006050475175065127\n",
      "epoch: 72, loss: 0.006120444121882835\n",
      "epoch: 73, loss: 0.00595245914058535\n",
      "epoch: 74, loss: 0.005901333302554053\n",
      "epoch: 75, loss: 0.0058514715135471055\n",
      "epoch: 76, loss: 0.005859987964514767\n",
      "epoch: 77, loss: 0.0061185410793636475\n",
      "epoch: 78, loss: 0.005784366787456684\n",
      "epoch: 79, loss: 0.005783422036197251\n",
      "epoch: 80, loss: 0.005862200919655141\n",
      "epoch: 81, loss: 0.006008018349166213\n",
      "epoch: 82, loss: 0.005990832304550764\n",
      "epoch: 83, loss: 0.005918633924778969\n",
      "epoch: 84, loss: 0.005973870183544793\n",
      "epoch: 85, loss: 0.005918751219471384\n",
      "epoch: 86, loss: 0.005854890042444354\n",
      "epoch: 87, loss: 0.005821430871928034\n",
      "epoch: 88, loss: 0.005933701947515109\n",
      "epoch: 89, loss: 0.0058997260897838445\n",
      "epoch: 90, loss: 0.005895352201202061\n",
      "epoch: 91, loss: 0.005699588940796226\n",
      "epoch: 92, loss: 0.005822048450222123\n",
      "epoch: 93, loss: 0.005927832467614718\n",
      "epoch: 94, loss: 0.005740272723661323\n",
      "epoch: 95, loss: 0.005917958678608753\n",
      "epoch: 96, loss: 0.00573284948004207\n",
      "epoch: 97, loss: 0.005735909437303778\n",
      "epoch: 98, loss: 0.005533475367377834\n",
      "epoch: 99, loss: 0.005542696990766213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de562d078034440194421364b74c2851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.006912370323641016\n",
      "epoch: 1, loss: 0.006783292849428258\n",
      "epoch: 2, loss: 0.006907514266761174\n",
      "epoch: 3, loss: 0.007006003363531338\n",
      "epoch: 4, loss: 0.006954118155741081\n",
      "epoch: 5, loss: 0.006990239109371334\n",
      "epoch: 6, loss: 0.0070091614171556614\n",
      "epoch: 7, loss: 0.006734634441801671\n",
      "epoch: 8, loss: 0.006981041505693204\n",
      "epoch: 9, loss: 0.007019075609251479\n",
      "epoch: 10, loss: 0.006954371361974437\n",
      "epoch: 11, loss: 0.006935354210318921\n",
      "epoch: 12, loss: 0.006790149813036379\n",
      "epoch: 13, loss: 0.006579884227644361\n",
      "epoch: 14, loss: 0.006760929855448221\n",
      "epoch: 15, loss: 0.006716719564092678\n",
      "epoch: 16, loss: 0.006808398180837867\n",
      "epoch: 17, loss: 0.006874711996305145\n",
      "epoch: 18, loss: 0.006851420490279158\n",
      "epoch: 19, loss: 0.006777733609710033\n",
      "epoch: 20, loss: 0.0066962276636044855\n",
      "epoch: 21, loss: 0.006598594488226144\n",
      "epoch: 22, loss: 0.006871275516235155\n",
      "epoch: 23, loss: 0.006765660637479334\n",
      "epoch: 24, loss: 0.006614422753377147\n",
      "epoch: 25, loss: 0.006660332807216115\n",
      "epoch: 26, loss: 0.006635734162524111\n",
      "epoch: 27, loss: 0.006753211180464864\n",
      "epoch: 28, loss: 0.006511372907305745\n",
      "epoch: 29, loss: 0.006757182874498204\n",
      "epoch: 30, loss: 0.006549554403759594\n",
      "epoch: 31, loss: 0.0067390385492469606\n",
      "epoch: 32, loss: 0.0063896454780193465\n",
      "epoch: 33, loss: 0.006562551744712305\n",
      "epoch: 34, loss: 0.006469855403913747\n",
      "epoch: 35, loss: 0.006752758956049\n",
      "epoch: 36, loss: 0.006552185278352264\n",
      "epoch: 37, loss: 0.00659971975713799\n",
      "epoch: 38, loss: 0.006494557000242425\n",
      "epoch: 39, loss: 0.006444877181010662\n",
      "epoch: 40, loss: 0.006496222129292227\n",
      "epoch: 41, loss: 0.006388803281183386\n",
      "epoch: 42, loss: 0.006437307842326795\n",
      "epoch: 43, loss: 0.006552693747436743\n",
      "epoch: 44, loss: 0.00647506195475639\n",
      "epoch: 45, loss: 0.006515776724598953\n",
      "epoch: 46, loss: 0.006512803864489967\n",
      "epoch: 47, loss: 0.006562315579370286\n",
      "epoch: 48, loss: 0.006423458358782472\n",
      "epoch: 49, loss: 0.006287617179873702\n",
      "epoch: 50, loss: 0.006454617475412544\n",
      "epoch: 51, loss: 0.006428039438267142\n",
      "epoch: 52, loss: 0.006500976789131591\n",
      "epoch: 53, loss: 0.006447698142099538\n",
      "epoch: 54, loss: 0.006374119686612441\n",
      "epoch: 55, loss: 0.006485317408459477\n",
      "epoch: 56, loss: 0.0062766633473894775\n",
      "epoch: 57, loss: 0.0064615630724050134\n",
      "epoch: 58, loss: 0.006540781633815219\n",
      "epoch: 59, loss: 0.00650371781034475\n",
      "epoch: 60, loss: 0.006535807128217917\n",
      "epoch: 61, loss: 0.006498493878456578\n",
      "epoch: 62, loss: 0.0062356042388733565\n",
      "epoch: 63, loss: 0.0063424080719591495\n",
      "epoch: 64, loss: 0.006287709339012575\n",
      "epoch: 65, loss: 0.006483074659021219\n",
      "epoch: 66, loss: 0.006412472615570085\n",
      "epoch: 67, loss: 0.00645922722307808\n",
      "epoch: 68, loss: 0.006567340051082276\n",
      "epoch: 69, loss: 0.006655664721081418\n",
      "epoch: 70, loss: 0.006586402635508028\n",
      "epoch: 71, loss: 0.006744247759145773\n",
      "epoch: 72, loss: 0.006320769054875248\n",
      "epoch: 73, loss: 0.006331637449820358\n",
      "epoch: 74, loss: 0.006306252517275302\n",
      "epoch: 75, loss: 0.00642564310071893\n",
      "epoch: 76, loss: 0.006407331648148392\n",
      "epoch: 77, loss: 0.006203970235625245\n",
      "epoch: 78, loss: 0.006334664262545792\n",
      "epoch: 79, loss: 0.0064373237815477085\n",
      "epoch: 80, loss: 0.006343571735256014\n",
      "epoch: 81, loss: 0.00614485719120264\n",
      "epoch: 82, loss: 0.006497451814353346\n",
      "epoch: 83, loss: 0.006304082261552963\n",
      "epoch: 84, loss: 0.006281787741681504\n",
      "epoch: 85, loss: 0.0063526748655321815\n",
      "epoch: 86, loss: 0.006337295578608932\n",
      "epoch: 87, loss: 0.0063467446282774575\n",
      "epoch: 88, loss: 0.006321873570904578\n",
      "epoch: 89, loss: 0.006657266573655242\n",
      "epoch: 90, loss: 0.00620836445703634\n",
      "epoch: 91, loss: 0.006345788831257016\n",
      "epoch: 92, loss: 0.0064129468769671336\n",
      "epoch: 93, loss: 0.0063585722143772744\n",
      "epoch: 94, loss: 0.006288505728885017\n",
      "epoch: 95, loss: 0.006205446882832039\n",
      "epoch: 96, loss: 0.006231785020439491\n",
      "epoch: 97, loss: 0.006437160116627656\n",
      "epoch: 98, loss: 0.0064196754697424495\n",
      "epoch: 99, loss: 0.00642562601152824\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = loader(data_path(\"trainability_qnn_3D_reps_2\"))\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = qnn_list[i]\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2_epochs_200\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b26d4144394cbf806adc277b30998c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07127398614235778\n",
      "epoch: 1, loss: 0.03859135820573581\n",
      "epoch: 2, loss: 0.030764468021473118\n",
      "epoch: 3, loss: 0.027220754720571776\n",
      "epoch: 4, loss: 0.02679027360532808\n",
      "epoch: 5, loss: 0.024991331310244988\n",
      "epoch: 6, loss: 0.02285660423840059\n",
      "epoch: 7, loss: 0.02178305827616\n",
      "epoch: 8, loss: 0.02063444313487282\n",
      "epoch: 9, loss: 0.019071287919812364\n",
      "epoch: 10, loss: 0.018074138267264336\n",
      "epoch: 11, loss: 0.01821947423153643\n",
      "epoch: 12, loss: 0.01811047410409626\n",
      "epoch: 13, loss: 0.018165278904042008\n",
      "epoch: 14, loss: 0.01793703926835272\n",
      "epoch: 15, loss: 0.0176410844272219\n",
      "epoch: 16, loss: 0.01730191936388806\n",
      "epoch: 17, loss: 0.01711680167276803\n",
      "epoch: 18, loss: 0.016605563645549985\n",
      "epoch: 19, loss: 0.016833693151009348\n",
      "epoch: 20, loss: 0.01653656480817363\n",
      "epoch: 21, loss: 0.01634578939455766\n",
      "epoch: 22, loss: 0.015500163648678957\n",
      "epoch: 23, loss: 0.015482478876533165\n",
      "epoch: 24, loss: 0.015040812347437885\n",
      "epoch: 25, loss: 0.014641995641454129\n",
      "epoch: 26, loss: 0.014394075068666752\n",
      "epoch: 27, loss: 0.014355755623623753\n",
      "epoch: 28, loss: 0.014080487154378952\n",
      "epoch: 29, loss: 0.013870946024701177\n",
      "epoch: 30, loss: 0.01321675319343042\n",
      "epoch: 31, loss: 0.013284712542928302\n",
      "epoch: 32, loss: 0.013812975207626602\n",
      "epoch: 33, loss: 0.013082376842405484\n",
      "epoch: 34, loss: 0.012781918118099513\n",
      "epoch: 35, loss: 0.012533005437897078\n",
      "epoch: 36, loss: 0.012299949063708201\n",
      "epoch: 37, loss: 0.01178279889490009\n",
      "epoch: 38, loss: 0.011799504514741278\n",
      "epoch: 39, loss: 0.011753230363936095\n",
      "epoch: 40, loss: 0.01148318669894991\n",
      "epoch: 41, loss: 0.011404617104275554\n",
      "epoch: 42, loss: 0.011085245798621213\n",
      "epoch: 43, loss: 0.0110209717756383\n",
      "epoch: 44, loss: 0.01080142431157999\n",
      "epoch: 45, loss: 0.010551431544950082\n",
      "epoch: 46, loss: 0.01022685860884935\n",
      "epoch: 47, loss: 0.010194734659759187\n",
      "epoch: 48, loss: 0.010112731922624803\n",
      "epoch: 49, loss: 0.010015658149987046\n",
      "epoch: 50, loss: 0.009777368712724685\n",
      "epoch: 51, loss: 0.00962935745691862\n",
      "epoch: 52, loss: 0.009157465632813104\n",
      "epoch: 53, loss: 0.009407645121940067\n",
      "epoch: 54, loss: 0.009152155695954324\n",
      "epoch: 55, loss: 0.008977708544678983\n",
      "epoch: 56, loss: 0.009059462176713058\n",
      "epoch: 57, loss: 0.009295600400679178\n",
      "epoch: 58, loss: 0.009179611347025787\n",
      "epoch: 59, loss: 0.009160755556044434\n",
      "epoch: 60, loss: 0.00908465598430744\n",
      "epoch: 61, loss: 0.00890272555964217\n",
      "epoch: 62, loss: 0.008843974387363686\n",
      "epoch: 63, loss: 0.00898334637280458\n",
      "epoch: 64, loss: 0.008616983716344662\n",
      "epoch: 65, loss: 0.008558046599545559\n",
      "epoch: 66, loss: 0.00837412896966017\n",
      "epoch: 67, loss: 0.008567253976730834\n",
      "epoch: 68, loss: 0.008568889790803282\n",
      "epoch: 69, loss: 0.008427482350520684\n",
      "epoch: 70, loss: 0.008235921729957055\n",
      "epoch: 71, loss: 0.00838749409849234\n",
      "epoch: 72, loss: 0.008376255097200398\n",
      "epoch: 73, loss: 0.00831019341222881\n",
      "epoch: 74, loss: 0.008307300651372957\n",
      "epoch: 75, loss: 0.008517748049962017\n",
      "epoch: 76, loss: 0.008193136972364148\n",
      "epoch: 77, loss: 0.00828059884143291\n",
      "epoch: 78, loss: 0.008055399909276008\n",
      "epoch: 79, loss: 0.00854882524006499\n",
      "epoch: 80, loss: 0.00832414185289855\n",
      "epoch: 81, loss: 0.008195254367175348\n",
      "epoch: 82, loss: 0.00814587554860551\n",
      "epoch: 83, loss: 0.008341817203724523\n",
      "epoch: 84, loss: 0.008176971131014165\n",
      "epoch: 85, loss: 0.008368516643254968\n",
      "epoch: 86, loss: 0.008021733825216886\n",
      "epoch: 87, loss: 0.008148046155408295\n",
      "epoch: 88, loss: 0.008166051515700224\n",
      "epoch: 89, loss: 0.008124633761861807\n",
      "epoch: 90, loss: 0.008204955893212186\n",
      "epoch: 91, loss: 0.008166536355564205\n",
      "epoch: 92, loss: 0.00811548466543255\n",
      "epoch: 93, loss: 0.008154988840484932\n",
      "epoch: 94, loss: 0.008153688870625125\n",
      "epoch: 95, loss: 0.008072130890368814\n",
      "epoch: 96, loss: 0.008032695569624994\n",
      "epoch: 97, loss: 0.007923371814855041\n",
      "epoch: 98, loss: 0.008035921755790479\n",
      "epoch: 99, loss: 0.008009592401396204\n",
      "epoch: 100, loss: 0.008189166615755134\n",
      "epoch: 101, loss: 0.008098028563117674\n",
      "epoch: 102, loss: 0.008064212522692868\n",
      "epoch: 103, loss: 0.008112002031641847\n",
      "epoch: 104, loss: 0.008008016403928793\n",
      "epoch: 105, loss: 0.007746250376729857\n",
      "epoch: 106, loss: 0.008001006015907634\n",
      "epoch: 107, loss: 0.007955005809549099\n",
      "epoch: 108, loss: 0.00790449467082992\n",
      "epoch: 109, loss: 0.007980192314111704\n",
      "epoch: 110, loss: 0.0077471706675193925\n",
      "epoch: 111, loss: 0.007733869061241272\n",
      "epoch: 112, loss: 0.007601284459700286\n",
      "epoch: 113, loss: 0.007670760423477429\n",
      "epoch: 114, loss: 0.007992122282589829\n",
      "epoch: 115, loss: 0.007869446126919592\n",
      "epoch: 116, loss: 0.00758906633356865\n",
      "epoch: 117, loss: 0.007942123426162585\n",
      "epoch: 118, loss: 0.007695582426859256\n",
      "epoch: 119, loss: 0.007904425980198703\n",
      "epoch: 120, loss: 0.008003513698127261\n",
      "epoch: 121, loss: 0.007653786205901985\n",
      "epoch: 122, loss: 0.00780950082756412\n",
      "epoch: 123, loss: 0.007807538024179907\n",
      "epoch: 124, loss: 0.00771937908007464\n",
      "epoch: 125, loss: 0.007693975527279134\n",
      "epoch: 126, loss: 0.00775865524357152\n",
      "epoch: 127, loss: 0.007631917093810377\n",
      "epoch: 128, loss: 0.00790046843456402\n",
      "epoch: 129, loss: 0.007717330772571667\n",
      "epoch: 130, loss: 0.007655912141137303\n",
      "epoch: 131, loss: 0.007710384843666772\n",
      "epoch: 132, loss: 0.007624174771960122\n",
      "epoch: 133, loss: 0.007952488494775053\n",
      "epoch: 134, loss: 0.007558165282669038\n",
      "epoch: 135, loss: 0.007536185670248753\n",
      "epoch: 136, loss: 0.007610985510826194\n",
      "epoch: 137, loss: 0.007334040419637915\n",
      "epoch: 138, loss: 0.007713332719902418\n",
      "epoch: 139, loss: 0.0075426619579636355\n",
      "epoch: 140, loss: 0.007564171344132334\n",
      "epoch: 141, loss: 0.0074798462009543045\n",
      "epoch: 142, loss: 0.007521494547932215\n",
      "epoch: 143, loss: 0.007698823164236195\n",
      "epoch: 144, loss: 0.0075299892239812805\n",
      "epoch: 145, loss: 0.007533305615638129\n",
      "epoch: 146, loss: 0.007382717558201275\n",
      "epoch: 147, loss: 0.007748171336141779\n",
      "epoch: 148, loss: 0.00738779041723676\n",
      "epoch: 149, loss: 0.007377324381110136\n",
      "epoch: 150, loss: 0.007270234765582224\n",
      "epoch: 151, loss: 0.007392174361432479\n",
      "epoch: 152, loss: 0.007425604082461191\n",
      "epoch: 153, loss: 0.007322892642505864\n",
      "epoch: 154, loss: 0.007456631048182482\n",
      "epoch: 155, loss: 0.007555716115209763\n",
      "epoch: 156, loss: 0.007209288210014147\n",
      "epoch: 157, loss: 0.007524396413538635\n",
      "epoch: 158, loss: 0.007368026610508793\n",
      "epoch: 159, loss: 0.007370614164694334\n",
      "epoch: 160, loss: 0.007570386300604503\n",
      "epoch: 161, loss: 0.007430455478110543\n",
      "epoch: 162, loss: 0.007445150611296097\n",
      "epoch: 163, loss: 0.00763944145073835\n",
      "epoch: 164, loss: 0.007381688247913993\n",
      "epoch: 165, loss: 0.007227661202952123\n",
      "epoch: 166, loss: 0.0072969986403969775\n",
      "epoch: 167, loss: 0.007242168605370586\n",
      "epoch: 168, loss: 0.007152229487313212\n",
      "epoch: 169, loss: 0.007220834900263143\n",
      "epoch: 170, loss: 0.007172383543317668\n",
      "epoch: 171, loss: 0.007255570805970278\n",
      "epoch: 172, loss: 0.007187059937168647\n",
      "epoch: 173, loss: 0.007377241215493902\n",
      "epoch: 174, loss: 0.007440540591583368\n",
      "epoch: 175, loss: 0.006860001571580015\n",
      "epoch: 176, loss: 0.007293693783435428\n",
      "epoch: 177, loss: 0.007551524499568445\n",
      "epoch: 178, loss: 0.007218872532006702\n",
      "epoch: 179, loss: 0.007310318647721132\n",
      "epoch: 180, loss: 0.007363578656602203\n",
      "epoch: 181, loss: 0.007242030043816095\n",
      "epoch: 182, loss: 0.007425672128129759\n",
      "epoch: 183, loss: 0.007509823767958967\n",
      "epoch: 184, loss: 0.0069097624817709815\n",
      "epoch: 185, loss: 0.007050152279000182\n",
      "epoch: 186, loss: 0.00738818581989817\n",
      "epoch: 187, loss: 0.007027323910178852\n",
      "epoch: 188, loss: 0.00703426752122594\n",
      "epoch: 189, loss: 0.007410379536225313\n",
      "epoch: 190, loss: 0.00710235404878366\n",
      "epoch: 191, loss: 0.0069753972223195355\n",
      "epoch: 192, loss: 0.006854468907584159\n",
      "epoch: 193, loss: 0.006902403050072018\n",
      "epoch: 194, loss: 0.006755332717503355\n",
      "epoch: 195, loss: 0.007131274175411388\n",
      "epoch: 196, loss: 0.007041146768713472\n",
      "epoch: 197, loss: 0.006890194604912853\n",
      "epoch: 198, loss: 0.006850002834324447\n",
      "epoch: 199, loss: 0.006875258314180448\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(n_qubits = [3, 4, 4],\n",
    "                         dim = [3, 4, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "\n",
    "qnn.train(x_qnn, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjSklEQVR4nO3deXSU933v8fd3ZrQvCC2AQIAAgzEmXrCMcbw1ThwDdUxv07R2b+I0Seu4sduka5zmtDdNb8+5SW5yG7euqZP4Nk6T2InjtNRxgtPEznJtYgTGGIxlBGYRCBCrNrTMzPf+MQ9itKHBSBr50ed1zhzm+T2/0Xzn0fCZn37zm2fM3RERkfCKZLsAEREZWwp6EZGQU9CLiIScgl5EJOQU9CIiIRfLdgFDqays9Nra2myXISLylrFp06aj7l411L4JGfS1tbXU19dnuwwRkbcMM9s73D5N3YiIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScqEK+gd+spOfvd6S7TJERCaUUAX9Q8/t4pc7FfQiIukyCnozW2lmDWbWaGb3D7HfzOyBYP9WM1sWtF9sZlvSLq1m9olRfgx9YhEjkRyrny4i8tY04ikQzCwKPAjcAjQBG81snbu/mtZtFbAwuFwDPARc4+4NwBVpP+cA8P3RfADpolEjkVTSi4iky2REvxxodPfd7t4DPAasGdBnDfCop2wAysysekCfdwK73H3Y8zFcqFjEiCf11YgiIukyCfpZwP607aag7Xz73AF8e7g7MbO7zazezOpbWt7cPHs0YiQU9CIi/WQS9DZE28A0PWcfM8sFbge+O9yduPvD7l7n7nVVVUOeaXNEsUhEI3oRkQEyCfomYHbadg1w8Dz7rAI2u/vhN1NkpjSiFxEZLJOg3wgsNLN5wcj8DmDdgD7rgLuC1TcrgFPu3py2/07OMW0zWjRHLyIy2Iirbtw9bmb3AeuBKPCIu283s3uC/WuBp4HVQCPQCXzozO3NrJDUip2Pjn75/aVG9Fp1IyKSLqNvmHL3p0mFeXrb2rTrDtw7zG07gYoLqDFj0YgRT2hELyKSLlSfjNUcvYjIYKEK+ljESLiCXkQkXaiCXiN6EZHBQhX0sUhEc/QiIgOEKug1ohcRGSxUQR+LGnEtrxQR6SdUQa8RvYjIYKEKen0yVkRksFAFvUb0IiKDhS7oNaIXEekvZEEf0YheRGSAUAV9TFM3IiKDhCroNUcvIjJYqII+tepG6+hFRNKFKug1ohcRGSxUQa919CIig4Uq6KORCAmd1ExEpJ9QBX3qXDcKehGRdKEKes3Ri4gMFq6gN626EREZKFxBHzGSDkmN6kVE+oQq6GMRA9D3xoqIpMko6M1spZk1mFmjmd0/xH4zsweC/VvNbFnavjIze8LMXjOzHWZ27Wg+gHTRaBD0GtGLiPQZMejNLAo8CKwClgB3mtmSAd1WAQuDy93AQ2n7vgz8yN0XA5cDO0ah7iH1jegV9CIifTIZ0S8HGt19t7v3AI8Bawb0WQM86ikbgDIzqzazUuBG4GsA7t7j7idHr/z+opHUw9ESSxGRszIJ+lnA/rTtpqAtkz7zgRbg/5rZS2b2VTMrGupOzOxuM6s3s/qWlpaMH0A6jehFRAbLJOhtiLaBSTpcnxiwDHjI3a8EOoBBc/wA7v6wu9e5e11VVVUGZQ0WDYJeSyxFRM7KJOibgNlp2zXAwQz7NAFN7v6roP0JUsE/JjSiFxEZLJOg3wgsNLN5ZpYL3AGsG9BnHXBXsPpmBXDK3Zvd/RCw38wuDvq9E3h1tIofqG9Er/PdiIj0iY3Uwd3jZnYfsB6IAo+4+3YzuyfYvxZ4GlgNNAKdwIfSfsQfAd8MXiR2D9g3qqIa0YuIDDJi0AO4+9Okwjy9bW3adQfuHea2W4C6N19i5s7O0SvoRUTOCNknY1MPRyN6EZGzQhX0mroRERksVEGvVTciIoOFKujPnOtG6+hFRM4KVdBrRC8iMliogl6rbkREBgtV0GvVjYjIYKEKeo3oRUQGC2XQJ/RmrIhIn1AFfUznuhERGSRUQa8PTImIDBaqoNeXg4uIDBaqoNeIXkRksFAF/ZnllZqjFxE5K1RBf+YUCBrRi4icFaqgj2kdvYjIIKEKeq2jFxEZLFRBrxG9iMhgoQp6rboRERkslEGvEb2IyFmhDHqN6EVEzsoo6M1spZk1mFmjmd0/xH4zsweC/VvNbFnavj1m9oqZbTGz+tEsfiCdplhEZLDYSB3MLAo8CNwCNAEbzWydu7+a1m0VsDC4XAM8FPx7xjvc/eioVT2MYECvqRsRkTSZjOiXA43uvtvde4DHgDUD+qwBHvWUDUCZmVWPcq0jMjNiEdPyShGRNJkE/Sxgf9p2U9CWaR8HnjGzTWZ293B3YmZ3m1m9mdW3tLRkUNbQohHTiF5EJE0mQW9DtA1M0nP1uc7dl5Ga3rnXzG4c6k7c/WF3r3P3uqqqqgzKGlosYiR0rhsRkT6ZBH0TMDttuwY4mGkfdz/z7xHg+6SmgsaMRvQiIv1lEvQbgYVmNs/McoE7gHUD+qwD7gpW36wATrl7s5kVmVkJgJkVAe8Gto1i/YPEohGtuhERSTPiqht3j5vZfcB6IAo84u7bzeyeYP9a4GlgNdAIdAIfCm4+Hfi+mZ25r2+5+49G/VGk0YheRKS/EYMewN2fJhXm6W1r0647cO8Qt9sNXH6BNZ6XqGnVjYhIulB9MhY0ohcRGSh0QR+LmuboRUTShC7ooxEFvYhIutAFfUxBLyLST+iCPhqJaI5eRCRN6IJeI3oRkf5CF/RadSMi0l/ogl5nrxQR6S90QR+NGHGd1ExEpE8og15z9CIiZ4Uy6DVHLyJyVuiCXqtuRET6C13QRyM6TbGISLrQBb1G9CIi/YUu6KNRI67llSIifUIX9BrRi4j0F7qg16obEZH+Qhf0GtGLiPQXuqDX2StFRPoLYdCjEb2ISJrQBX0sEiGe0KobEZEzQhf0OteNiEh/GQW9ma00swYzazSz+4fYb2b2QLB/q5ktG7A/amYvmdlTo1X4cGIRI+EKehGRM0YMejOLAg8Cq4AlwJ1mtmRAt1XAwuByN/DQgP0fB3ZccLUZ0IheRKS/TEb0y4FGd9/t7j3AY8CaAX3WAI96ygagzMyqAcysBvh14KujWPewYlpHLyLSTyZBPwvYn7bdFLRl2ucfgL8EzvkOqZndbWb1Zlbf0tKSQVlDi0YiuENSYS8iAmQW9DZE28AUHbKPmd0GHHH3TSPdibs/7O517l5XVVWVQVlDi0VTpWhULyKSkknQNwGz07ZrgIMZ9rkOuN3M9pCa8rnZzP7tTVebgWgkFfSapxcRSckk6DcCC81snpnlAncA6wb0WQfcFay+WQGccvdmd/+Uu9e4e21wu5+6+/tH8wEMFIucGdFrLb2ICEBspA7uHjez+4D1QBR4xN23m9k9wf61wNPAaqAR6AQ+NHYln1tuLPXa1dWbpCQ/W1WIiEwcIwY9gLs/TSrM09vWpl134N4RfsZzwHPnXeF5Ks3PAaCtq5eqkryxvjsRkQkvdJ+MLS1IvXa1dsWzXImIyMQQvqAPRvStp3uzXImIyMQQuqCfUpAK+lMKehERIIRBXxoEfWuXgl5EBMIY9H1TN5qjFxGBEAZ9fk6EnKhpRC8iEghd0JsZUwpyNEcvIhIIXdBDavpGq25ERFJCGfQlBTlaRy8iEghl0JfmxzSiFxEJhDLopxRo6kZE5IxQBn1pQY5W3YiIBMIZ9Pk5tJ6O4/qScBGRkAZ9QYyeRJLuuM5JLyISyqDX+W5ERM4KZdDrDJYiImeFM+h1YjMRkT7hDPr84MtHdGIzEZFwBr3m6EVEzgpl0GvqRkTkrFAGfUnf1I2CXkQko6A3s5Vm1mBmjWZ2/xD7zcweCPZvNbNlQXu+mb1oZi+b2XYz+9vRfgBDyYtFyc+J6MRmIiJkEPRmFgUeBFYBS4A7zWzJgG6rgIXB5W7goaC9G7jZ3S8HrgBWmtmK0Sn93CqK8jjc2jUedyUiMqFlMqJfDjS6+2537wEeA9YM6LMGeNRTNgBlZlYdbLcHfXKCy7icl6C2spC9xzrH465ERCa0TIJ+FrA/bbspaMuoj5lFzWwLcAT4sbv/aqg7MbO7zazezOpbWloyLH94cyuK2Hus44J/jojIW10mQW9DtA0clQ/bx90T7n4FUAMsN7OlQ92Juz/s7nXuXldVVZVBWedWW1HIic5eTnXqDVkRmdwyCfomYHbadg1w8Hz7uPtJ4Dlg5fkW+WbMrSgCYO9xjepFZHLLJOg3AgvNbJ6Z5QJ3AOsG9FkH3BWsvlkBnHL3ZjOrMrMyADMrAN4FvDZ65Q+vNgj6PZqnF5FJLjZSB3ePm9l9wHogCjzi7tvN7J5g/1rgaWA10Ah0Ah8Kbl4NfD1YuRMBvuPuT43+wxhsTnkhAHuPakQvIpPbiEEP4O5Pkwrz9La1adcduHeI220FrrzAGt+UgtwoM0rzNaIXkUkvlJ+MPWNuRaFW3ojIpBfqoK+tKNKIXkQmvVAH/dzKQo62d7P/uMJeRCavUAf9uy6ZTklejPc+9Dw7mluzXY6ISFaEOugXTS/hex97O0l3PvejcVnVKSIy4YQ66CEV9rddNpMNu4/R1ZvIdjkiIuMu9EEPcOOiSrp6k2zaeyLbpYiIjLtJEfTXzKsgJ2r8/PULP1maiMhbzaQI+qK8GHVzy/mZgl5EJqFJEfQANy6q4rVDbRzRl5GIyCQzaYL+2gUVAGzco3l6EZlcJk3QXzqzlPycCPV7j2e7FBGRcTVpgj4nGuHK2VOp14heRCaZSRP0AFfXTuXV5lY6uuPZLkVEZNxMqqC/qracRNLZsv9ktksRERk3kyrol80pI2KwcY/m6UVk8phUQV+Sn8PSWVP40bZDpL4rRUQk/CZV0AO8/5q5vHaojed3Hct2KSIi42LSBf2aK2dSWZzHV36xO9uliIiMi0kX9HmxKHddO5fnGlrYebgt2+WIiIy5SRf0AO9fMZf8nAhf++Ub2S5FRGTMTcqgLy/K5b3LanjypQO0tHVnuxwRkTGVUdCb2UozazCzRjO7f4j9ZmYPBPu3mtmyoH22mT1rZjvMbLuZfXy0H8Cb9eHr59ETT/KNDXuzXYqIyJgaMejNLAo8CKwClgB3mtmSAd1WAQuDy93AQ0F7HPgzd78EWAHcO8Rts2JBVTE3Lqriyc1NWmopIqGWyYh+OdDo7rvdvQd4DFgzoM8a4FFP2QCUmVm1uze7+2YAd28DdgCzRrH+C7J66QyaTpzmVX1xuIiEWCZBPwvYn7bdxOCwHrGPmdUCVwK/GupOzOxuM6s3s/qWlvH5gpB3LZlOxGD9tkPjcn8iItmQSdDbEG0D5zrO2cfMioHvAZ9w9yGHz+7+sLvXuXtdVVVVBmVduMriPOpqy1m//fC43J+ISDZkEvRNwOy07RrgYKZ9zCyHVMh/092ffPOljo1bL51Bw+E2Go+0Z7sUEZExkUnQbwQWmtk8M8sF7gDWDeizDrgrWH2zAjjl7s1mZsDXgB3u/qVRrXyUvOfyavJiEf7lZ7uyXYqIyJgYMejdPQ7cB6wn9Wbqd9x9u5ndY2b3BN2eBnYDjcBXgI8F7dcBHwBuNrMtwWX1aD+ICzGtJJ/fvWYOT750gF0t7bx26Oz56nviySxXJyJy4WwiLi2sq6vz+vr6cbu/I61d3PD5Z+lJJHGHiEFBTpSOngQ3Larivpsv4ura8nGrR0TkfJnZJnevG2pfbLyLmYimlebzmdsvZWvTSermlrP3eCetp3vJjUV4cnMT71v7Au+5fCZ/fdslTCvJz3a5IiLnRSP6EXT1Jlj7s13883O7KMyN8unVl/Cey2eSnxPNdmkiIn3ONaKflOe6OR/5OVE+8a5FPP3HNzC3vJC/eGIry//+v/jB1uZslyYikhFN3WToomnFPPmx63hh1zG++OMG/vixl/jFzhZ+sfMoy+eV86lVi5lWqmkdEZl4NKI/D9GIcf3CSr7xkWu4vGYKj9fvZ3Z5AT/Y2swNn3+Wjz/2EvuPd2a7TBGRfjSifxOK82J86w9WcKKzh+opBew52sEj/+8NntjUxMGTp/nOR68l9RECEZHs04j+TcrPiVI9pQCA2soiPrtmKX9z2xI27jnBupcHfnBYRCR7FPSj6LfrZnNZzRT+8omt3PSFZ3nw2UadAllEsk5BP4oiEeOBO67kfXU11Ewt4AvrG/jE41vo6k1kuzQRmcQ0Rz/KaiuL+J+/8TbcnX9+bhdfWN/A/uOdrH3/VVqVIyJZoaAfI2bGve+4iHmVRfzJ41u46QvP8d6rZlFbUcT1CytZPKM02yWKyCShoB9jq99WzeIZJfzjTxt5fON+ehOOGfzmlTV89Kb5LJpeku0SRSTkdAqEcZRMOkc7uvnqL97g68/voTue5K5r5/LZNUsBaDzSzj8/18hf3Hpx34oeEZFM6KRmE0QkYkwryeevVl/CH960gP/9TAOPvrCXpTOnUFc7lQ987Vc0n+riSGs3j354OZGI1uKLyIVT0GfJ1KJcPrtmKW8c7eAvv7cVgNL8GH9wwzy+8os3+Jef7+aem+brg1cicsEU9FkUjRj/9LvLePSFPRTnxbh58TTmVRax51gnn/vRa/z89RY+uWoxV8wuy3apIvIWpjn6CSiRdL714j6++EwDJzt7uWFhJfe94yKumV+R7dJEZILSaYrfYqIR4wMr5vLLT97Mp1YtZkdzK7/z8AYe+MnObJcmIm9BmrqZwIrzYnz0pgXcdW0tn/73V/jSj19nd0s7pQU5TCnIYUl1KbdeOkNv2orIOSno3wIKcqN84bcuJxYxfrC1mVg0QltXL0mHJdWl3LJkOlfXlnP9wspslyoiE5Dm6N+iEknnqa0HefDZRnYeaccdbr10Ou9dVsMl1aXMLi/MdokiMo7ONUevoA+Brt4E//r8Hv7Pj1+nO54E4JYl0/nDX1vAsjlTs1ydiIyHCw56M1sJfBmIAl919/81YL8F+1cDncDvufvmYN8jwG3AEXdfmknBCvo3p7Wrlz1HO/jJjiP86/N7OHW6l+suquCza5ayoKo42+WJyBi6oKA3syjwOnAL0ARsBO5091fT+qwG/ohU0F8DfNndrwn23Qi0A48q6MdPR3ecb7+4j3/8aSOnexNcXTuV8qI8Lp5ezOIZpbytZgrTdTZNkdC40FMgLAca3X138MMeA9YAr6b1WUMqyB3YYGZlZlbt7s3u/nMzq72whyDnqygvxu/fMJ/br5jJF9e/TmNLOy/tO8F/Bt9+ZQYrL53BB1bM5ep55eREtdJWJKwyCfpZwP607SZSo/aR+swCmjMtxMzuBu4GmDNnTqY3kxFMK8nnc791Wd92e3echkOt/PS1Izz6/F5+uO0Q5UW5rH3/VVw5p4xdLe0smlaiJZsiIZJJ0A/1P37gfE8mfc7J3R8GHobU1M353FYyV5wX46q55Vw1t5yP/dpF/LLxKJ/70Wvc9civqCzOo+nEaRbPKOGjN83nXZdM51h7D9/dtJ91Lx+ktqKIi6YVU5QbY3Z5AZfVlLF4RonOxyMywWUS9E3A7LTtGmDgt19n0kcmmKK8GLdeOoOr5k7lI19PvSfywWtr+faL+/iTx1/u62cG119USUtbNy/tO8np3gSJZOq1eNH0YpbOmkJHd5yX95/i9itm8ufvvpjcmKaCRCaKTN6MjZF6M/adwAFSb8b+rrtvT+vz68B9nH0z9gF3X562vxZ4Sm/GvjUkk87GPcd5ftcxZpbls3xeBfMqi/r2J5LOgROn+dnOFn74SjN7jnYQiRjzKov4xc6jzCor4OIZJVx/USXzKot4amszxzu6Kc7P4boFFdyyZDoVxXkc7+ih9XQvhblRKovziESM1q5ennq5meop+bxj8bR+dX23fj+HTnVx5zVzqCzOG7Z2M/RXhkw6o7G8cjXwD6SWVz7i7n9vZvcAuPvaYHnlPwErSS2v/JC71we3/Tbwa0AlcBj4H+7+tXPdn4L+rWv99kM8samJN4520HikHYApBTnMKS+kpa2bQ61d5ESNBVXFNBxu48zTLzcWoSQvRlt3nJ7gswDXXVRBPOFML81nZlkBa3+2K9U3GqGudiqX1ZRRkBPl6VeaOd7Zw7XzK3jxjeP0JpJ8ds1SSgtiNB5pp6s3SXFeFDNjz9EOYtEIlcW5VBbn0ZNI0nq6F4DppfnMryqisjiPquCFpzueIJ5wTvcm+OErzXT0JLh4egkVxbkkks6p070sqCqmZmqBXlwkq/SBKcmKhkNtHDjZydsXVJKfE8Xdee1QG09ubmLbgVauXVBBzdQCOrrjNJ04TXt3nIKcKLddPpMXdh3jGy/sYcaUfHYeaaetK86vX1bNH9+8kO/W7+eXjUdpPNJOPOlcMbuMmWX5PL/rGFfOLqOlvZttB1qHrCk/J0Ii6fQmzv28L8mLUVNeyK4j7fQkkiM+1gVVRdx1bS0/3NbMqwdbqSrJY1pJavnqwVOnefuCSt5XV0Npfg5zKwrJCU5j8YX1DTzbcITLZpXxvroalsws5UvPvE5JfoyVS2fw1NZmNuw+zvGObv7slou5dkEFG/ccJ550ygtzWTS9hLKiHE73JGhp62ZBVTG7j7bznY37WTprCrcsmU5ZYS498SRJT/218+Czu2g41MrfrVk65BfWJ4NpueHekHf3QS9q7k7zqS6qp+TrBS9LFPTyltbeHWfjG8e5YWElsbRloMmk09ETpyQ/p1//nniSp7YepLwol7fNmkJhboy27l7iCWdGaT5mcOp0L0fbe8iLRSgNbn/w1Gn2HO3gaEcPDYda2Xusk0uqS6koyiXp8I7FVcwozafxSDsnOnuJGJTk59BwqJVvbNjL64fbmVaSx7uWTOdERw9H2rpJulNRlMfPX2/pe8Eoyo0yt6KIvcc66OxNcNOiKl492MqRtm7yYhGc1PRYIunkRiNcM7+cju44m/edHPFY5USN3oQTjVjf+yiFuVE6exKYpd6Mb+uKkxuNUFaYw62XziCeTNJ04jQr5lcwv7KIT//7NvJiEW5YWIk7lBfnUjO1kJMdPTz3egtbm05y7YJKLqkuITcaYfm8cp7Y1MR/bDnI7ZfP5FOrF1OSn0NxXuotwKPt3RwLjvWc8kIOnjpN/Z4T7DjUyqqlqe9U/v5LB6gszmPF/HISSefpVw6xo7mVq+eVM60kj10t7fznyweZU17IH9288LxP8ZFMOgdOnqa1q5cl1aWYWXA/zZzo7OF3rp5NXiwK9H8he+NoB1ubTpIXi3LrpdP72rvjCXKjkb5td+dYR8+wU4rjQUEvMsYSSeeVA6dYPKOE/JzooP1H2rrYvPcknT1xNu87wf7jp5lTXsh7r6rhitll9MSTfGPDXl7YdZRPrlxMTjTCxj3HuXnxNCqK80gmne9u2k9bV5wbF1VRmBvlcGsXu4500NrVS14swtSiXF5pOkVhbozfe3stjS3t1O85zqHWLsoKcgHYd7yT2y6rprosn08+sZV9xzuJmFFZnEfD4TYgdaK8mWUFvLTvBLmxCMfae/pepBZNL+bq2nJ+2XiUQ6e6iAcvSBGDlUtnsH774b4XmFllBUQisP/46b7jkBuL9E3NQeqU3DNK8zlw8myfM/Jikb5TegDMryyi6eRp4okkV82dypSCHFraupk1tYDaiiJmTMnn0KkuEkmntCCHvcc6iCedkrwY67cf5lBrFwAfWDGXD18/jz/8t028dij1mBdOK6ayOI/dR9tpaetmbkURBTlRXm0++5fhX9+2hI9cP4+nth7kr/99G3MqivjHO65kalEO9z/5Cj/Y2sx7l9XwZ+9exMyywd/5nEw6m/edYGpRLrUVRURHeQmzgl5ERrRh9zG27D/J7729tt+LVTyR5FhHD1MKcga9iHX2xHlh1zFmTMnn0plT2HbgFJv3naC9O872g60kEk5d7VSqpxTQ1tVLw+E2aiuKqKudyqyyAv7uqR28cuAkf7X6EqIRY9uBViIGdbXlXDG7jG0HTtHRE6e8KJeLp5dwuLWbb7+4j5+8dph4wqkszuPgydPsO95JPOnEIkbEjJ5EksriXHKiEVraurlhYSW3LJnB9oOn+Oav9lGQE6UgN8pnbr+Uotwon/9RA/m50b7AbzzSxonOXlYtncF1F1Xy5f/ayTOvHqJmaiH7jndy6cxS9h/vpLUrDqResG69dDrPbD9MPOnUVhSyYn4FFcW5tHfFuXLOVP5jywGebWgBoKwwh5sWVTG3ooi84MXv5sXTuPwCvk1OQS8iodabSHK8o4eKolyiEaOrN0lB7uCpmGTS+dPvbOHV5lYe/kAdtWmryc6lsyfOvd/cjJlxy5LpvO+qGg61dvH9zQdIOty4qJIr50xlz9EO/mvHYTbsPs6LbxyjoydBXixCZ09qqucvbr2YKYU5bNh1jF80HuVoezfpEbx8XjmPfnj5kH8VjkRBLyKSZqg3lEdbMul9nxp95cApphbmMLei/wtLPJEknnR6E0ke37ifnYfb+32S/Xxc6LluRERCZTxWBqWvWrpimCmZWDRCLAr5OVF+/4b5Y1fLmP1kERGZEBT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiITchPxkrJm1AHvf5M0rgaOjWM5oUV3nR3WdH9V1fsJY11x3rxpqx4QM+gthZvXDfQw4m1TX+VFd50d1nZ/JVpembkREQk5BLyIScmEM+oezXcAwVNf5UV3nR3Wdn0lVV+jm6EVEpL8wjuhFRCSNgl5EJORCE/RmttLMGsys0czuz2Ids83sWTPbYWbbzezjQftnzOyAmW0JLquzUNseM3sluP/6oK3czH5sZjuDf6eOc00Xpx2TLWbWamafyNbxMrNHzOyImW1Laxv2GJnZp4LnXIOZ3TrOdX3BzF4zs61m9n0zKwvaa83sdNqxWzvOdQ37u8vy8Xo8raY9ZrYlaB+X43WObBj755e7v+UvQBTYBcwHcoGXgSVZqqUaWBZcLwFeB5YAnwH+PMvHaQ9QOaDt88D9wfX7gc9l+fd4CJibreMF3AgsA7aNdIyC3+vLQB4wL3gORsexrncDseD659Lqqk3vl4XjNeTvLtvHa8D+LwJ/M57H6xzZMObPr7CM6JcDje6+2917gMeANdkoxN2b3X1zcL0N2AHMykYtGVoDfD24/nXgN7JXCu8Edrn7m/1U9AVz958Dxwc0D3eM1gCPuXu3u78BNJJ6Lo5LXe7+jLvHg80NQM1Y3Pf51nUOWT1eZ1jqewR/G/j2WNz3OWoaLhvG/PkVlqCfBexP225iAoSrmdUCVwK/CpruC/7MfmS8p0gCDjxjZpvM7O6gbbq7N0PqiQhMy0JdZ9xB//982T5eZwx3jCbS8+7DwA/TtueZ2Utm9jMzuyEL9Qz1u5sox+sG4LC770xrG9fjNSAbxvz5FZagH+qbfrO6btTMioHvAZ9w91bgIWABcAXQTOpPx/F2nbsvA1YB95rZjVmoYUhmlgvcDnw3aJoIx2skE+J5Z2afBuLAN4OmZmCOu18J/CnwLTMrHceShvvdTYjjBdxJ/wHFuB6vIbJh2K5DtL2p4xWWoG8CZqdt1wAHs1QLZpZD6hf5TXd/EsDdD7t7wt2TwFcYoz9Zz8XdDwb/HgG+H9Rw2Myqg7qrgSPjXVdgFbDZ3Q8HNWb9eKUZ7hhl/XlnZh8EbgP+uwcTu8Gf+seC65tIze0uGq+azvG7mwjHKwb8JvD4mbbxPF5DZQPj8PwKS9BvBBaa2bxgZHgHsC4bhQTzf18Ddrj7l9Laq9O6/Tdg28DbjnFdRWZWcuY6qTfytpE6Th8Mun0Q+I/xrCtNv1FWto/XAMMdo3XAHWaWZ2bzgIXAi+NVlJmtBD4J3O7unWntVWYWDa7PD+raPY51Dfe7y+rxCrwLeM3dm840jNfxGi4bGI/n11i/0zxeF2A1qXexdwGfzmId15P682orsCW4rAa+AbwStK8Dqse5rvmk3sF/Gdh+5hgBFcBPgJ3Bv+VZOGaFwDFgSlpbVo4XqRebZqCX1IjqI+c6RsCng+dcA7BqnOtqJDWHe+Z5tjbo+97gd/wysBl4zzjXNezvLpvHK2j/V+CeAX3H5XidIxvG/PmlUyCIiIRcWKZuRERkGAp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjI/X+xqT09z1NWzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qnn.loss)\n",
    "#plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.train(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
