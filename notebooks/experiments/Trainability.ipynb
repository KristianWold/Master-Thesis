{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.25, 0.02) - gaussian(x, 0.75, 0.02)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_qnn, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1081393c732c421392117bfbf668e7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf196a8cbc04018b12b2dcc56e72633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.10756772041964606\n",
      "epoch: 1, loss: 0.08472352253196841\n",
      "epoch: 2, loss: 0.06513384000207995\n",
      "epoch: 3, loss: 0.053847436095626214\n",
      "epoch: 4, loss: 0.04732092353085084\n",
      "epoch: 5, loss: 0.040712028291198174\n",
      "epoch: 6, loss: 0.032319690332919586\n",
      "epoch: 7, loss: 0.02350238777851792\n",
      "epoch: 8, loss: 0.01650801890263716\n",
      "epoch: 9, loss: 0.012594840989411792\n",
      "epoch: 10, loss: 0.01126608488820943\n",
      "epoch: 11, loss: 0.010773602977232382\n",
      "epoch: 12, loss: 0.010280098685411831\n",
      "epoch: 13, loss: 0.01002997234644193\n",
      "epoch: 14, loss: 0.009746780568587778\n",
      "epoch: 15, loss: 0.009158113675563153\n",
      "epoch: 16, loss: 0.008688767645956472\n",
      "epoch: 17, loss: 0.008608604254939823\n",
      "epoch: 18, loss: 0.008770226995818142\n",
      "epoch: 19, loss: 0.00890776938086217\n",
      "epoch: 20, loss: 0.008899914109474955\n",
      "epoch: 21, loss: 0.008759766080919672\n",
      "epoch: 22, loss: 0.008519265598736794\n",
      "epoch: 23, loss: 0.008194760583872331\n",
      "epoch: 24, loss: 0.00782116409925287\n",
      "epoch: 25, loss: 0.007453696020194765\n",
      "epoch: 26, loss: 0.007120607997067066\n",
      "epoch: 27, loss: 0.006788526809663437\n",
      "epoch: 28, loss: 0.006383171881409287\n",
      "epoch: 29, loss: 0.00584960578681305\n",
      "epoch: 30, loss: 0.005209717654994295\n",
      "epoch: 31, loss: 0.004581097610033449\n",
      "epoch: 32, loss: 0.0041279091339896\n",
      "epoch: 33, loss: 0.003942101443429638\n",
      "epoch: 34, loss: 0.003939532784164095\n",
      "epoch: 35, loss: 0.003917145344560288\n",
      "epoch: 36, loss: 0.0037661620337405793\n",
      "epoch: 37, loss: 0.003580475695473264\n",
      "epoch: 38, loss: 0.003508420386345711\n",
      "epoch: 39, loss: 0.0035673928587733185\n",
      "epoch: 40, loss: 0.0036623218905977206\n",
      "epoch: 41, loss: 0.003719435029368745\n",
      "epoch: 42, loss: 0.003730508541472868\n",
      "epoch: 43, loss: 0.0037074903289272625\n",
      "epoch: 44, loss: 0.0036634540322413186\n",
      "epoch: 45, loss: 0.003629816797833674\n",
      "epoch: 46, loss: 0.0036298849036982815\n",
      "epoch: 47, loss: 0.0036321901586315185\n",
      "epoch: 48, loss: 0.003586945402039004\n",
      "epoch: 49, loss: 0.0035038036134928054\n",
      "epoch: 50, loss: 0.003437366112136424\n",
      "epoch: 51, loss: 0.003411152243098051\n",
      "epoch: 52, loss: 0.003406097790520882\n",
      "epoch: 53, loss: 0.0034051865360444824\n",
      "epoch: 54, loss: 0.003406610298330928\n",
      "epoch: 55, loss: 0.0034067327892803715\n",
      "epoch: 56, loss: 0.003403384881927223\n",
      "epoch: 57, loss: 0.003406241164127729\n",
      "epoch: 58, loss: 0.0034191451327890187\n",
      "epoch: 59, loss: 0.003427063719335698\n",
      "epoch: 60, loss: 0.0034208221011825645\n",
      "epoch: 61, loss: 0.0034134131996692136\n",
      "epoch: 62, loss: 0.0034151941261247715\n",
      "epoch: 63, loss: 0.003417636424366448\n",
      "epoch: 64, loss: 0.0034128294281337256\n",
      "epoch: 65, loss: 0.003403252456545906\n",
      "epoch: 66, loss: 0.0033913047838923845\n",
      "epoch: 67, loss: 0.0033798927651045225\n",
      "epoch: 68, loss: 0.0033761415721844956\n",
      "epoch: 69, loss: 0.0033802473715297873\n",
      "epoch: 70, loss: 0.0033825785395846255\n",
      "epoch: 71, loss: 0.0033794560461617323\n",
      "epoch: 72, loss: 0.003376750470932416\n",
      "epoch: 73, loss: 0.003376926951142044\n",
      "epoch: 74, loss: 0.00337732690636958\n",
      "epoch: 75, loss: 0.0033773418337485106\n",
      "epoch: 76, loss: 0.0033769837232152256\n",
      "epoch: 77, loss: 0.0033751138267607656\n",
      "epoch: 78, loss: 0.003373379885584796\n",
      "epoch: 79, loss: 0.003374065835144166\n",
      "epoch: 80, loss: 0.0033749856229452137\n",
      "epoch: 81, loss: 0.003373603312425045\n",
      "epoch: 82, loss: 0.0033716445101155007\n",
      "epoch: 83, loss: 0.0033710345862335693\n",
      "epoch: 84, loss: 0.003371229365754214\n",
      "epoch: 85, loss: 0.0033717377402867595\n",
      "epoch: 86, loss: 0.003372205579784739\n",
      "epoch: 87, loss: 0.0033716914902118117\n",
      "epoch: 88, loss: 0.0033706247717421047\n",
      "epoch: 89, loss: 0.0033702881054642176\n",
      "epoch: 90, loss: 0.0033702459606340103\n",
      "epoch: 91, loss: 0.003369581037553806\n",
      "epoch: 92, loss: 0.0033688269487146764\n",
      "epoch: 93, loss: 0.0033685272511798747\n",
      "epoch: 94, loss: 0.0033684327909246758\n",
      "epoch: 95, loss: 0.0033684910200737074\n",
      "epoch: 96, loss: 0.0033685929823336403\n",
      "epoch: 97, loss: 0.00336840579513141\n",
      "epoch: 98, loss: 0.003368240078383698\n",
      "epoch: 99, loss: 0.003368509076951866\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder = Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 0)\n",
    "    \n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbce5f3b1d54672a6af42075cb0ef21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214d264a22a6473381d89548a068a27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-53f517c989bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                          \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          shots=0)\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_qnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mqnn_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/neuralnetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, verbose)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# swap here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/neuralnetwork.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y, samplewise)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             weight_gradient, delta = layer.grad(\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self.a[i], delta, samplewise=samplewise)\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_gradient_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/layers.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, inputs, delta, samplewise)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_partial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_partial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_weights_per_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 self.encoder(circuit, data_register,\n\u001b[0m\u001b[1;32m    116\u001b[0m                              self.weight[:idx, i], x)\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/data_encoders.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, circuit, data_register, weight, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_register\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_register\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"z\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_register\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/quantumcircuit.py\u001b[0m in \u001b[0;36mry\u001b[0;34m(self, theta, qubit, label)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0;34m\"\"\"Apply :class:`~qiskit.circuit.library.RYGate`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_gates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRYGate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRYGate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mqubit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_qubit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_qubit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/quantumcircuit.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, instruction, qargs, cargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0minstructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstructionSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mqarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_qargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_cargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0minstructions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/quantumcircuit.py\u001b[0m in \u001b[0;36m_append\u001b[0;34m(self, instruction, qargs, cargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;31m# do some compatibility checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_dups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_qargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_cargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/quantumcircuit.py\u001b[0m in \u001b[0;36m_check_qargs\u001b[0;34m(self, qargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_qargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;34m\"\"\"Raise exception if a qarg is not in this circuit or bad format.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQubit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCircuitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qarg is not a Qubit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f72a6ec61349d98dde3f3573558773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc3b85a464e4f089cdff2bd7f10ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06726630157232004\n",
      "epoch: 1, loss: 0.057192736282336086\n",
      "epoch: 2, loss: 0.050649972945189525\n",
      "epoch: 3, loss: 0.04781296860666636\n",
      "epoch: 4, loss: 0.04730101182953739\n",
      "epoch: 5, loss: 0.04768763649494786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fb0dbacb08a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                          \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          shots=0)\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_qnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mqnn_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/neuralnetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, verbose)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# swap here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/neuralnetwork.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y, samplewise)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             weight_gradient, delta = layer.grad(\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self.a[i], delta, samplewise=samplewise)\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_gradient_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/layers.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, inputs, delta, samplewise)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_weights_per_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_partial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_partial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/src/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshots\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcircuit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcircuit_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/execute.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(experiments, backend, basis_gates, coupling_map, backend_properties, initial_layout, seed_transpiler, optimization_level, pass_manager, qobj_id, qobj_header, shots, memory, max_credits, seed_simulator, default_qubit_los, default_meas_los, schedule_los, meas_level, meas_return, memory_slots, memory_slot_size, rep_time, rep_delay, parameter_binds, schedule_circuit, inst_map, meas_map, scheduling_method, init_qubits, **run_config)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# transpiling the circuits using given transpile options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         experiments = transpile(experiments,\n\u001b[0m\u001b[1;32m    253\u001b[0m                                 \u001b[0mbasis_gates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbasis_gates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                                 \u001b[0mcoupling_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoupling_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/compiler/transpile.py\u001b[0m in \u001b[0;36mtranspile\u001b[0;34m(circuits, backend, basis_gates, coupling_map, backend_properties, initial_layout, layout_method, routing_method, translation_method, scheduling_method, instruction_durations, dt, seed_transpiler, optimization_level, pass_manager, callback, output_name)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Transpile circuits in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mcircuits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transpile_circuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspile_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/tools/parallel.py\u001b[0m in \u001b[0;36mparallel_map\u001b[0;34m(task, values, task_args, task_kwargs, num_processes)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtask_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtask_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mPublisher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"terra.parallel.start\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/compiler/transpile.py\u001b[0m in \u001b[0;36m_transpile_circuit\u001b[0;34m(circuit_config_tuple)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTranspilerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimization_level can range from 0 to 3.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     result = pass_manager.run(circuit, callback=transpile_config['callback'],\n\u001b[0m\u001b[1;32m    327\u001b[0m                               output_name=transpile_config['output_name'])\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/transpiler/passmanager.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, circuits, output_name, callback)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantumCircuit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_single_circuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_single_circuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/transpiler/passmanager.py\u001b[0m in \u001b[0;36m_run_single_circuit\u001b[0;34m(self, circuit, output_name, callback)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO to remove with __init__(callback)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_passmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_passmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/transpiler/runningpassmanager.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, circuit, output_name, callback)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcircuit_to_dag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/converters/circuit_to_dag.py\u001b[0m in \u001b[0;36mcircuit_to_dag\u001b[0;34m(circuit)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mdagcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_operation_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mdagcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/dagcircuit/dagcircuit.py\u001b[0m in \u001b[0;36mapply_operation_back\u001b[0;34m(self, op, qargs, cargs, condition)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_bits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_cbits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mnode_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Add new in-edges from predecessors of the output nodes to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/dagcircuit/dagcircuit.py\u001b[0m in \u001b[0;36m_add_op_node\u001b[0;34m(self, op, qargs, cargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Add a new operation node to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         new_node = DAGNode(type=\"op\", op=op, name=op.name, qargs=qargs,\n\u001b[0m\u001b[1;32m    334\u001b[0m                            cargs=cargs)\n\u001b[1;32m    335\u001b[0m         \u001b[0mnode_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/dagcircuit/dagnode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, op, name, qargs, cargs, condition, wire, nid)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/bit.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"Return the official string representing the bit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s(%s, %s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.9/site-packages/qiskit/circuit/register.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;34m\"\"\"Return the official string representing the register.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s(%d, '%s')\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4],\n",
    "                         dim = [2, 1],\n",
    "                         encoder= RZZEncoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_QNN_2D_reps_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe96f9afc8d4044af43b8329dd93a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f57fbf05fa4bff99ed5e902d1ec501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06053292139966247\n",
      "epoch: 1, loss: 0.04835194275586967\n",
      "epoch: 2, loss: 0.04582226029657446\n",
      "epoch: 3, loss: 0.04334438080172202\n",
      "epoch: 4, loss: 0.039220484715734465\n",
      "epoch: 5, loss: 0.034686397307269\n",
      "epoch: 6, loss: 0.030670976685410346\n",
      "epoch: 7, loss: 0.027034841113276836\n",
      "epoch: 8, loss: 0.02287660919512585\n",
      "epoch: 9, loss: 0.019244814227636024\n",
      "epoch: 10, loss: 0.01744839603420172\n",
      "epoch: 11, loss: 0.017027410673967115\n",
      "epoch: 12, loss: 0.017045210169954483\n",
      "epoch: 13, loss: 0.016621301946025145\n",
      "epoch: 14, loss: 0.01580948114675065\n",
      "epoch: 15, loss: 0.015168945421845596\n",
      "epoch: 16, loss: 0.015731108973812916\n",
      "epoch: 17, loss: 0.016384429391695102\n",
      "epoch: 18, loss: 0.01613981382404393\n",
      "epoch: 19, loss: 0.015172462114230179\n",
      "epoch: 20, loss: 0.014310082910800437\n",
      "epoch: 21, loss: 0.013737405539512351\n",
      "epoch: 22, loss: 0.013303995544703476\n",
      "epoch: 23, loss: 0.013203905160511571\n",
      "epoch: 24, loss: 0.013116676741475451\n",
      "epoch: 25, loss: 0.012687143837100718\n",
      "epoch: 26, loss: 0.012159402662222797\n",
      "epoch: 27, loss: 0.011816973204928955\n",
      "epoch: 28, loss: 0.011538434917505423\n",
      "epoch: 29, loss: 0.01122482773586235\n",
      "epoch: 30, loss: 0.010612738921723399\n",
      "epoch: 31, loss: 0.00983189122329874\n",
      "epoch: 32, loss: 0.009369418628018697\n",
      "epoch: 33, loss: 0.008964947422224749\n",
      "epoch: 34, loss: 0.00842452243813514\n",
      "epoch: 35, loss: 0.008217129816058972\n",
      "epoch: 36, loss: 0.008147980368621502\n",
      "epoch: 37, loss: 0.007923855153554574\n",
      "epoch: 38, loss: 0.007726039969649146\n",
      "epoch: 39, loss: 0.007322848882068356\n",
      "epoch: 40, loss: 0.0069785859517978695\n",
      "epoch: 41, loss: 0.006520382346009471\n",
      "epoch: 42, loss: 0.006239168643321769\n",
      "epoch: 43, loss: 0.0060766112281916175\n",
      "epoch: 44, loss: 0.005974545750026993\n",
      "epoch: 45, loss: 0.006014168454453445\n",
      "epoch: 46, loss: 0.0059424507959029375\n",
      "epoch: 47, loss: 0.005894091217077385\n",
      "epoch: 48, loss: 0.005892970763338183\n",
      "epoch: 49, loss: 0.005823167707309176\n",
      "epoch: 50, loss: 0.005744544579508777\n",
      "epoch: 51, loss: 0.005657980053355494\n",
      "epoch: 52, loss: 0.0055848780520095145\n",
      "epoch: 53, loss: 0.005568702483474133\n",
      "epoch: 54, loss: 0.0055523889646264145\n",
      "epoch: 55, loss: 0.005548901272119349\n",
      "epoch: 56, loss: 0.005516807044032104\n",
      "epoch: 57, loss: 0.005462754197037298\n",
      "epoch: 58, loss: 0.005402771388729577\n",
      "epoch: 59, loss: 0.005353545947128572\n",
      "epoch: 60, loss: 0.005354695802861096\n",
      "epoch: 61, loss: 0.005354977337214922\n",
      "epoch: 62, loss: 0.005368279962589532\n",
      "epoch: 63, loss: 0.005368296262435712\n",
      "epoch: 64, loss: 0.005351747262753812\n",
      "epoch: 65, loss: 0.005336269241203257\n",
      "epoch: 66, loss: 0.005306066263380922\n",
      "epoch: 67, loss: 0.005296789224671402\n",
      "epoch: 68, loss: 0.005295945420507067\n",
      "epoch: 69, loss: 0.0052997094593981485\n",
      "epoch: 70, loss: 0.005301626703724088\n",
      "epoch: 71, loss: 0.005297365143999267\n",
      "epoch: 72, loss: 0.005302369099748952\n",
      "epoch: 73, loss: 0.005301719504431463\n",
      "epoch: 74, loss: 0.005300365076277161\n",
      "epoch: 75, loss: 0.005295294561216928\n",
      "epoch: 76, loss: 0.00528892472544154\n",
      "epoch: 77, loss: 0.005285070082149249\n",
      "epoch: 78, loss: 0.005281155772813159\n",
      "epoch: 79, loss: 0.005281945364072095\n",
      "epoch: 80, loss: 0.00528246119017356\n",
      "epoch: 81, loss: 0.0052833871902826035\n",
      "epoch: 82, loss: 0.005282142182501003\n",
      "epoch: 83, loss: 0.005279830613133877\n",
      "epoch: 84, loss: 0.005278121282621135\n",
      "epoch: 85, loss: 0.005276905761961017\n",
      "epoch: 86, loss: 0.005276963655277714\n",
      "epoch: 87, loss: 0.005275319999063151\n",
      "epoch: 88, loss: 0.005274760525258608\n",
      "epoch: 89, loss: 0.00527381156051706\n",
      "epoch: 90, loss: 0.005273615383478515\n",
      "epoch: 91, loss: 0.005272117991883596\n",
      "epoch: 92, loss: 0.005271458529530386\n",
      "epoch: 93, loss: 0.005271468533221148\n",
      "epoch: 94, loss: 0.0052714516885248265\n",
      "epoch: 95, loss: 0.005271499574052204\n",
      "epoch: 96, loss: 0.0052712184826911946\n",
      "epoch: 97, loss: 0.00527123016994761\n",
      "epoch: 98, loss: 0.005270172444403818\n",
      "epoch: 99, loss: 0.005269464497890622\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4079db403d5d4dbf8a640c8806698488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a06a26202684805a87f40f984c87163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1276206425531826\n",
      "epoch: 1, loss: 0.06594590333219355\n",
      "epoch: 2, loss: 0.04636803856464966\n",
      "epoch: 3, loss: 0.04845327287181595\n",
      "epoch: 4, loss: 0.04785017492574352\n",
      "epoch: 5, loss: 0.03864594650138124\n",
      "epoch: 6, loss: 0.027956827928486073\n",
      "epoch: 7, loss: 0.021360428053627246\n",
      "epoch: 8, loss: 0.018052954114806834\n",
      "epoch: 9, loss: 0.017298460710034394\n",
      "epoch: 10, loss: 0.016000003133367233\n",
      "epoch: 11, loss: 0.015170618978832985\n",
      "epoch: 12, loss: 0.015256656299470825\n",
      "epoch: 13, loss: 0.01458236319523361\n",
      "epoch: 14, loss: 0.013675289004790935\n",
      "epoch: 15, loss: 0.01285492990347118\n",
      "epoch: 16, loss: 0.011561171781000306\n",
      "epoch: 17, loss: 0.010324627422611931\n",
      "epoch: 18, loss: 0.009806168305422705\n",
      "epoch: 19, loss: 0.009618184227116832\n",
      "epoch: 20, loss: 0.00964380580911841\n",
      "epoch: 21, loss: 0.00991411809551177\n",
      "epoch: 22, loss: 0.01011642640260631\n",
      "epoch: 23, loss: 0.00994553257669265\n",
      "epoch: 24, loss: 0.009479675525330643\n",
      "epoch: 25, loss: 0.009091725906244989\n",
      "epoch: 26, loss: 0.008957803910358254\n",
      "epoch: 27, loss: 0.008847125691698687\n",
      "epoch: 28, loss: 0.008722787014641988\n",
      "epoch: 29, loss: 0.0086914888139352\n",
      "epoch: 30, loss: 0.008598594900220153\n",
      "epoch: 31, loss: 0.008474616045298509\n",
      "epoch: 32, loss: 0.008313026527420688\n",
      "epoch: 33, loss: 0.007973623030804115\n",
      "epoch: 34, loss: 0.0077556765882657785\n",
      "epoch: 35, loss: 0.007631391264815517\n",
      "epoch: 36, loss: 0.007541592127299181\n",
      "epoch: 37, loss: 0.007508499111062372\n",
      "epoch: 38, loss: 0.007265898680287362\n",
      "epoch: 39, loss: 0.006977063796797922\n",
      "epoch: 40, loss: 0.006603134056463678\n",
      "epoch: 41, loss: 0.006284759880740192\n",
      "epoch: 42, loss: 0.006012535563018824\n",
      "epoch: 43, loss: 0.005721606110150404\n",
      "epoch: 44, loss: 0.005417247026329808\n",
      "epoch: 45, loss: 0.005077797770795886\n",
      "epoch: 46, loss: 0.004827737535115403\n",
      "epoch: 47, loss: 0.004617538448957583\n",
      "epoch: 48, loss: 0.0044701799996632145\n",
      "epoch: 49, loss: 0.004271411330698107\n",
      "epoch: 50, loss: 0.004065241304320152\n",
      "epoch: 51, loss: 0.003852580565064424\n",
      "epoch: 52, loss: 0.003677241702758921\n",
      "epoch: 53, loss: 0.00352251882028619\n",
      "epoch: 54, loss: 0.0034021573585874664\n",
      "epoch: 55, loss: 0.0033275236447058627\n",
      "epoch: 56, loss: 0.0032767800631621933\n",
      "epoch: 57, loss: 0.003213912625231177\n",
      "epoch: 58, loss: 0.0031037036066426955\n",
      "epoch: 59, loss: 0.0029597123636847665\n",
      "epoch: 60, loss: 0.0028196029104948454\n",
      "epoch: 61, loss: 0.002716752952267727\n",
      "epoch: 62, loss: 0.0026555460950448365\n",
      "epoch: 63, loss: 0.002616843506835084\n",
      "epoch: 64, loss: 0.0026147501710656273\n",
      "epoch: 65, loss: 0.0026292168605247398\n",
      "epoch: 66, loss: 0.0026366168970617236\n",
      "epoch: 67, loss: 0.0026095114967223977\n",
      "epoch: 68, loss: 0.0025714491070960676\n",
      "epoch: 69, loss: 0.0025331875788566736\n",
      "epoch: 70, loss: 0.0024949955367230257\n",
      "epoch: 71, loss: 0.0024616181302759246\n",
      "epoch: 72, loss: 0.0024368043578338453\n",
      "epoch: 73, loss: 0.0024256754953494407\n",
      "epoch: 74, loss: 0.0024124674626152796\n",
      "epoch: 75, loss: 0.002392767640817698\n",
      "epoch: 76, loss: 0.0023627795083523175\n",
      "epoch: 77, loss: 0.0023291020153658184\n",
      "epoch: 78, loss: 0.002292064288255995\n",
      "epoch: 79, loss: 0.0022580184139160252\n",
      "epoch: 80, loss: 0.002231709146980563\n",
      "epoch: 81, loss: 0.0022139755278090365\n",
      "epoch: 82, loss: 0.002201780002978468\n",
      "epoch: 83, loss: 0.002188845409484632\n",
      "epoch: 84, loss: 0.0021741046975126146\n",
      "epoch: 85, loss: 0.002153927697324751\n",
      "epoch: 86, loss: 0.002129597724187782\n",
      "epoch: 87, loss: 0.0021034257578209196\n",
      "epoch: 88, loss: 0.002076486614567919\n",
      "epoch: 89, loss: 0.0020515383916359267\n",
      "epoch: 90, loss: 0.002028104278491552\n",
      "epoch: 91, loss: 0.0020043510478479232\n",
      "epoch: 92, loss: 0.001978645526788529\n",
      "epoch: 93, loss: 0.0019508674298757927\n",
      "epoch: 94, loss: 0.001922375265668659\n",
      "epoch: 95, loss: 0.0018928584449827877\n",
      "epoch: 96, loss: 0.001863887818502026\n",
      "epoch: 97, loss: 0.001837175875627104\n",
      "epoch: 98, loss: 0.0018114522894915551\n",
      "epoch: 99, loss: 0.0017853032440713363\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c18225ed5640e19b3f254a1dada157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6fdfe0cb44d3d9e59df48453cf1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.02616421090110864\n",
      "epoch: 1, loss: 0.023636985069046455\n",
      "epoch: 2, loss: 0.023236637796756696\n",
      "epoch: 3, loss: 0.023091643215221756\n",
      "epoch: 4, loss: 0.022984223091902883\n",
      "epoch: 5, loss: 0.022953813798121\n",
      "epoch: 6, loss: 0.022932075037858372\n",
      "epoch: 7, loss: 0.022842472661382226\n",
      "epoch: 8, loss: 0.022722775675088323\n",
      "epoch: 9, loss: 0.022635718273382583\n",
      "epoch: 10, loss: 0.022586477097556193\n",
      "epoch: 11, loss: 0.02253292341518813\n",
      "epoch: 12, loss: 0.02246195849612422\n",
      "epoch: 13, loss: 0.02240184601456359\n",
      "epoch: 14, loss: 0.02238009339864387\n",
      "epoch: 15, loss: 0.022389465546880944\n",
      "epoch: 16, loss: 0.022400123979620892\n",
      "epoch: 17, loss: 0.022399719934812615\n",
      "epoch: 18, loss: 0.022399106285238165\n",
      "epoch: 19, loss: 0.022408457233717663\n",
      "epoch: 20, loss: 0.022422396012902076\n",
      "epoch: 21, loss: 0.022423179340251134\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4],\n",
    "                         dim = [3, 1],\n",
    "                         encoder= RZZEncoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\", \"rx\"], reps=4),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_QNN_3D_reps_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0fea1a3d2a4da0a23c3d2ab56872ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372ca024c430457abecea3886c88b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.03413086149429913\n",
      "epoch: 1, loss: 0.024813917000044768\n",
      "epoch: 2, loss: 0.023288908390507103\n",
      "epoch: 3, loss: 0.02294723033101617\n",
      "epoch: 4, loss: 0.022817322157137592\n",
      "epoch: 5, loss: 0.022748717298517403\n",
      "epoch: 6, loss: 0.02273586136671576\n",
      "epoch: 7, loss: 0.022720464881048015\n",
      "epoch: 8, loss: 0.022628373354539082\n",
      "epoch: 9, loss: 0.02243778174592224\n",
      "epoch: 10, loss: 0.022148780128218748\n",
      "epoch: 11, loss: 0.02172794257274075\n",
      "epoch: 12, loss: 0.021168491241988854\n",
      "epoch: 13, loss: 0.020684145409854044\n",
      "epoch: 14, loss: 0.020463002047748254\n",
      "epoch: 15, loss: 0.02016151031809331\n",
      "epoch: 16, loss: 0.019651334860756403\n",
      "epoch: 17, loss: 0.019040345900673942\n",
      "epoch: 18, loss: 0.01872367018024863\n",
      "epoch: 19, loss: 0.018474223615551504\n",
      "epoch: 20, loss: 0.018139041045761018\n",
      "epoch: 21, loss: 0.017772593976886433\n",
      "epoch: 22, loss: 0.01726856692062583\n",
      "epoch: 23, loss: 0.01660218802170632\n",
      "epoch: 24, loss: 0.0160784782535287\n",
      "epoch: 25, loss: 0.015680869592557815\n",
      "epoch: 26, loss: 0.015487985779051153\n",
      "epoch: 27, loss: 0.015072760924466851\n",
      "epoch: 28, loss: 0.014914056362673343\n",
      "epoch: 29, loss: 0.014926318771086793\n",
      "epoch: 30, loss: 0.01476970490889804\n",
      "epoch: 31, loss: 0.014534764787812517\n",
      "epoch: 32, loss: 0.014288729961544178\n",
      "epoch: 33, loss: 0.013996939432834211\n",
      "epoch: 34, loss: 0.013676329544633492\n",
      "epoch: 35, loss: 0.013486342512079112\n",
      "epoch: 36, loss: 0.013308414964203015\n",
      "epoch: 37, loss: 0.013133270887205926\n",
      "epoch: 38, loss: 0.012853348688501954\n",
      "epoch: 39, loss: 0.012390177024814786\n",
      "epoch: 40, loss: 0.012088034067198297\n",
      "epoch: 41, loss: 0.011828121226681988\n",
      "epoch: 42, loss: 0.011711098057245043\n",
      "epoch: 43, loss: 0.011683019874395413\n",
      "epoch: 44, loss: 0.01157106120373006\n",
      "epoch: 45, loss: 0.011580130820728528\n",
      "epoch: 46, loss: 0.01141587495800138\n",
      "epoch: 47, loss: 0.01106010102118411\n",
      "epoch: 48, loss: 0.010760160759767004\n",
      "epoch: 49, loss: 0.010527177166858762\n",
      "epoch: 50, loss: 0.010282991564998525\n",
      "epoch: 51, loss: 0.010135446043192695\n",
      "epoch: 52, loss: 0.010078658000104612\n",
      "epoch: 53, loss: 0.010033666467405762\n",
      "epoch: 54, loss: 0.010008509788768806\n",
      "epoch: 55, loss: 0.009922279589735171\n",
      "epoch: 56, loss: 0.009881686859387295\n",
      "epoch: 57, loss: 0.009802161276461592\n",
      "epoch: 58, loss: 0.009776754884069948\n",
      "epoch: 59, loss: 0.009778283914736619\n",
      "epoch: 60, loss: 0.00974881464770425\n",
      "epoch: 61, loss: 0.009722940008961112\n",
      "epoch: 62, loss: 0.009637486620028522\n",
      "epoch: 63, loss: 0.00957016187227532\n",
      "epoch: 64, loss: 0.009512825227759307\n",
      "epoch: 65, loss: 0.009372321199499165\n",
      "epoch: 66, loss: 0.009219337256663554\n",
      "epoch: 67, loss: 0.008965837917785298\n",
      "epoch: 68, loss: 0.008544488622181013\n",
      "epoch: 69, loss: 0.007965854457657276\n",
      "epoch: 70, loss: 0.007681845360025258\n",
      "epoch: 71, loss: 0.00828442434736065\n",
      "epoch: 72, loss: 0.007267681465539997\n",
      "epoch: 73, loss: 0.006723309370381757\n",
      "epoch: 74, loss: 0.006812016774741698\n",
      "epoch: 75, loss: 0.006691226401005674\n",
      "epoch: 76, loss: 0.006568118319550716\n",
      "epoch: 77, loss: 0.006315401372803183\n",
      "epoch: 78, loss: 0.0063763368294423965\n",
      "epoch: 79, loss: 0.006125149180948496\n",
      "epoch: 80, loss: 0.006155791444247627\n",
      "epoch: 81, loss: 0.006081302944008408\n",
      "epoch: 82, loss: 0.0061814479128444325\n",
      "epoch: 83, loss: 0.005977432667423118\n",
      "epoch: 84, loss: 0.0060094591153727695\n",
      "epoch: 85, loss: 0.005718770188691858\n",
      "epoch: 86, loss: 0.00595730059082416\n",
      "epoch: 87, loss: 0.005704273674573531\n",
      "epoch: 88, loss: 0.00577318517303134\n",
      "epoch: 89, loss: 0.005742119780774381\n",
      "epoch: 90, loss: 0.00572814824102319\n",
      "epoch: 91, loss: 0.005670549476141224\n",
      "epoch: 92, loss: 0.005639771488849289\n",
      "epoch: 93, loss: 0.00565430869303476\n",
      "epoch: 94, loss: 0.005617437644085669\n",
      "epoch: 95, loss: 0.005608365224204139\n",
      "epoch: 96, loss: 0.0055206253862393525\n",
      "epoch: 97, loss: 0.005510277318922703\n",
      "epoch: 98, loss: 0.005513910067078189\n",
      "epoch: 99, loss: 0.005475012608544046\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [4, 5, 5],\n",
    "                         dim = [3, 5, 5, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 0)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5846e97ae244df298a90a05b60ece75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0fb9aed00744758893382e6dc8d09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.02547056293522781\n",
      "epoch: 1, loss: 0.021748480600561775\n",
      "epoch: 2, loss: 0.02051392624587805\n",
      "epoch: 3, loss: 0.017986821021357603\n",
      "epoch: 4, loss: 0.01657558984155573\n",
      "epoch: 5, loss: 0.015298452254258714\n",
      "epoch: 6, loss: 0.015061575016638596\n",
      "epoch: 7, loss: 0.012654326540540863\n",
      "epoch: 8, loss: 0.01250155408462557\n",
      "epoch: 9, loss: 0.011770037725041234\n",
      "epoch: 10, loss: 0.011556304504841932\n",
      "epoch: 11, loss: 0.010922071283222744\n",
      "epoch: 12, loss: 0.00995043271166552\n",
      "epoch: 13, loss: 0.009228349764361363\n",
      "epoch: 14, loss: 0.00918213592265556\n",
      "epoch: 15, loss: 0.008675155245653744\n",
      "epoch: 16, loss: 0.00847740024928649\n",
      "epoch: 17, loss: 0.008374971259114525\n",
      "epoch: 18, loss: 0.008128651113929292\n",
      "epoch: 19, loss: 0.007881869039162685\n",
      "epoch: 20, loss: 0.007627213374917509\n",
      "epoch: 21, loss: 0.007535481877516642\n",
      "epoch: 22, loss: 0.008203639066455635\n",
      "epoch: 23, loss: 0.00833129975423787\n",
      "epoch: 24, loss: 0.0070757087357955165\n",
      "epoch: 25, loss: 0.00807517066715739\n",
      "epoch: 26, loss: 0.006966360725772462\n",
      "epoch: 27, loss: 0.006594336218345198\n",
      "epoch: 28, loss: 0.00663049164201393\n",
      "epoch: 29, loss: 0.005801701699130002\n",
      "epoch: 30, loss: 0.006343004616656118\n",
      "epoch: 31, loss: 0.005377369860712896\n",
      "epoch: 32, loss: 0.005578720908544665\n",
      "epoch: 33, loss: 0.005044781721013341\n",
      "epoch: 34, loss: 0.00484360474861278\n",
      "epoch: 35, loss: 0.004890241187626655\n",
      "epoch: 36, loss: 0.004360656537927599\n",
      "epoch: 37, loss: 0.00464897453453872\n",
      "epoch: 38, loss: 0.004110568408102828\n",
      "epoch: 39, loss: 0.004224616261404188\n",
      "epoch: 40, loss: 0.003977812570603545\n",
      "epoch: 41, loss: 0.003768321943651466\n",
      "epoch: 42, loss: 0.0036865262734285067\n",
      "epoch: 43, loss: 0.0034755001673913402\n",
      "epoch: 44, loss: 0.003272495629477585\n",
      "epoch: 45, loss: 0.0032565507094532253\n",
      "epoch: 46, loss: 0.0029883546573046898\n",
      "epoch: 47, loss: 0.002896234986371148\n",
      "epoch: 48, loss: 0.0027914040540697204\n",
      "epoch: 49, loss: 0.0025537127408233736\n",
      "epoch: 50, loss: 0.0025429515259008796\n",
      "epoch: 51, loss: 0.0022861375826450803\n",
      "epoch: 52, loss: 0.0022601385115412877\n",
      "epoch: 53, loss: 0.0021196803772582217\n",
      "epoch: 54, loss: 0.0020049863751318837\n",
      "epoch: 55, loss: 0.0019469449442998585\n",
      "epoch: 56, loss: 0.001792146775508863\n",
      "epoch: 57, loss: 0.001767600731306205\n",
      "epoch: 58, loss: 0.001624274872184951\n",
      "epoch: 59, loss: 0.001568288931032766\n",
      "epoch: 60, loss: 0.0015078007843292189\n",
      "epoch: 61, loss: 0.0014035694649345693\n",
      "epoch: 62, loss: 0.0013832807859150945\n",
      "epoch: 63, loss: 0.0012962070116813182\n",
      "epoch: 64, loss: 0.001216440981951326\n",
      "epoch: 65, loss: 0.0011959814598497514\n",
      "epoch: 66, loss: 0.0011387350795641345\n",
      "epoch: 67, loss: 0.0010684595753593204\n",
      "epoch: 68, loss: 0.0010517364527414808\n",
      "epoch: 69, loss: 0.0010281211684421221\n",
      "epoch: 70, loss: 0.0009805611517491378\n",
      "epoch: 71, loss: 0.0009555288400661545\n",
      "epoch: 72, loss: 0.0009452834653512086\n",
      "epoch: 73, loss: 0.0009178876488658554\n",
      "epoch: 74, loss: 0.0008890709563291314\n",
      "epoch: 75, loss: 0.0008640786491662852\n",
      "epoch: 76, loss: 0.0008331560555227698\n",
      "epoch: 77, loss: 0.0007884159013652403\n",
      "epoch: 78, loss: 0.0007471222877135532\n",
      "epoch: 79, loss: 0.0007169803029127439\n",
      "epoch: 80, loss: 0.0006864942854930881\n",
      "epoch: 81, loss: 0.0006568701374971192\n",
      "epoch: 82, loss: 0.0006421233219396021\n",
      "epoch: 83, loss: 0.0006540437398840244\n",
      "epoch: 84, loss: 0.0006991851539680781\n",
      "epoch: 85, loss: 0.0008220238851947875\n",
      "epoch: 86, loss: 0.000952685914086006\n",
      "epoch: 87, loss: 0.001035302217951818\n",
      "epoch: 88, loss: 0.0006972094507986843\n",
      "epoch: 89, loss: 0.0005033182501286523\n",
      "epoch: 90, loss: 0.0006692299699247853\n",
      "epoch: 91, loss: 0.0007534325428126432\n",
      "epoch: 92, loss: 0.0005732817117547021\n",
      "epoch: 93, loss: 0.0004439035176236155\n",
      "epoch: 94, loss: 0.0005761232287998523\n",
      "epoch: 95, loss: 0.0006411166654687086\n",
      "epoch: 96, loss: 0.0004752527333147369\n",
      "epoch: 97, loss: 0.0004088321649647658\n",
      "epoch: 98, loss: 0.0005106304230096528\n",
      "epoch: 99, loss: 0.0005354035464785194\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [5, 5, 5],\n",
    "                         dim = [3, 5, 5, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 0)\n",
    "    qnn.train(x_qnn, y, epochs = 100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [3, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=100)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D_epochs_100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../src/layers.py:176: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [3, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D_epochs_10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbb56670c33431588a8eeb86b3d08d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f487e4cc64adf80b18f604fa0852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.012148431196048063\n",
      "epoch: 1, loss: 0.011980385371999271\n",
      "epoch: 2, loss: 0.01179547348901502\n",
      "epoch: 3, loss: 0.011803022517372697\n",
      "epoch: 4, loss: 0.012039628532458262\n",
      "epoch: 5, loss: 0.011928511686742336\n",
      "epoch: 6, loss: 0.011766001574135777\n",
      "epoch: 7, loss: 0.011627308652327785\n",
      "epoch: 8, loss: 0.011673681182331806\n",
      "epoch: 9, loss: 0.011709354036838758\n",
      "epoch: 10, loss: 0.012002848617721613\n",
      "epoch: 11, loss: 0.011750310299730561\n",
      "epoch: 12, loss: 0.011712583025981457\n",
      "epoch: 13, loss: 0.011647812518401872\n",
      "epoch: 14, loss: 0.011715479992813816\n",
      "epoch: 15, loss: 0.011651864716776402\n",
      "epoch: 16, loss: 0.011669518449077897\n",
      "epoch: 17, loss: 0.011868135152118568\n",
      "epoch: 18, loss: 0.011818297423394306\n",
      "epoch: 19, loss: 0.01158188390739788\n",
      "epoch: 20, loss: 0.011635109811138576\n",
      "epoch: 21, loss: 0.011891986178606971\n",
      "epoch: 22, loss: 0.012042224803234176\n",
      "epoch: 23, loss: 0.011636656517251908\n",
      "epoch: 24, loss: 0.011894149943480592\n",
      "epoch: 25, loss: 0.011492520239619528\n",
      "epoch: 26, loss: 0.01155023254606184\n",
      "epoch: 27, loss: 0.011695794959200435\n",
      "epoch: 28, loss: 0.011523478862884637\n",
      "epoch: 29, loss: 0.011814457489974225\n",
      "epoch: 30, loss: 0.01168931407366845\n",
      "epoch: 31, loss: 0.011658247781285025\n",
      "epoch: 32, loss: 0.011926431835727922\n",
      "epoch: 33, loss: 0.011812820866222817\n",
      "epoch: 34, loss: 0.011717147118008718\n",
      "epoch: 35, loss: 0.011928155658254447\n",
      "epoch: 36, loss: 0.0117736519521499\n",
      "epoch: 37, loss: 0.011731010748165458\n",
      "epoch: 38, loss: 0.011655230011606088\n",
      "epoch: 39, loss: 0.011720239471252837\n",
      "epoch: 40, loss: 0.011859545602729048\n",
      "epoch: 41, loss: 0.01175992199002874\n",
      "epoch: 42, loss: 0.01196859609770764\n",
      "epoch: 43, loss: 0.011785300226841124\n",
      "epoch: 44, loss: 0.011755123052639649\n",
      "epoch: 45, loss: 0.011652547708631576\n",
      "epoch: 46, loss: 0.01178929250898367\n",
      "epoch: 47, loss: 0.01168468534224479\n",
      "epoch: 48, loss: 0.011644053761587767\n",
      "epoch: 49, loss: 0.011387379143925873\n",
      "epoch: 50, loss: 0.011497054689366841\n",
      "epoch: 51, loss: 0.011728375400524191\n",
      "epoch: 52, loss: 0.011759872195681849\n",
      "epoch: 53, loss: 0.011581330418786194\n",
      "epoch: 54, loss: 0.011635299288540478\n",
      "epoch: 55, loss: 0.011498769987723858\n",
      "epoch: 56, loss: 0.011834905396830664\n",
      "epoch: 57, loss: 0.011825499726994804\n",
      "epoch: 58, loss: 0.011707183027389781\n",
      "epoch: 59, loss: 0.011637256255292607\n",
      "epoch: 60, loss: 0.011651705029418718\n",
      "epoch: 61, loss: 0.011774870958690136\n",
      "epoch: 62, loss: 0.011723120973905078\n",
      "epoch: 63, loss: 0.0115629170744672\n",
      "epoch: 64, loss: 0.011365235936374903\n",
      "epoch: 65, loss: 0.011761836866699852\n",
      "epoch: 66, loss: 0.011688142912094154\n",
      "epoch: 67, loss: 0.01159034513985037\n",
      "epoch: 68, loss: 0.011434759001125257\n",
      "epoch: 69, loss: 0.01160940650950725\n",
      "epoch: 70, loss: 0.011652277853984459\n",
      "epoch: 71, loss: 0.01186101658886264\n",
      "epoch: 72, loss: 0.011475790145606821\n",
      "epoch: 73, loss: 0.011703758222111586\n",
      "epoch: 74, loss: 0.01161988519826315\n",
      "epoch: 75, loss: 0.011501546841081551\n",
      "epoch: 76, loss: 0.011953154776686372\n",
      "epoch: 77, loss: 0.011554014842114482\n",
      "epoch: 78, loss: 0.011649239209593092\n",
      "epoch: 79, loss: 0.011664699094516348\n",
      "epoch: 80, loss: 0.011746697275507215\n",
      "epoch: 81, loss: 0.011565088940938058\n",
      "epoch: 82, loss: 0.011871181729833409\n",
      "epoch: 83, loss: 0.011666030647356114\n",
      "epoch: 84, loss: 0.011599588327829232\n",
      "epoch: 85, loss: 0.011594468555627414\n",
      "epoch: 86, loss: 0.011462565103789728\n",
      "epoch: 87, loss: 0.01136637883506804\n",
      "epoch: 88, loss: 0.011666274301518904\n",
      "epoch: 89, loss: 0.011761244673635783\n",
      "epoch: 90, loss: 0.011751392009694646\n",
      "epoch: 91, loss: 0.011941567155828484\n",
      "epoch: 92, loss: 0.011567062631887818\n",
      "epoch: 93, loss: 0.011512395899876297\n",
      "epoch: 94, loss: 0.011779875213616661\n",
      "epoch: 95, loss: 0.011446392740293769\n",
      "epoch: 96, loss: 0.011643244928957864\n",
      "epoch: 97, loss: 0.011805176440860822\n",
      "epoch: 98, loss: 0.011542185477402172\n",
      "epoch: 99, loss: 0.011329484129889187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fbe42e0ef84c6682ed546e5ad8bb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.011395458044639048\n",
      "epoch: 1, loss: 0.011496587908272828\n",
      "epoch: 2, loss: 0.011623204234763637\n",
      "epoch: 3, loss: 0.01158552805269232\n",
      "epoch: 4, loss: 0.011621900008657114\n",
      "epoch: 5, loss: 0.011579825815756308\n",
      "epoch: 6, loss: 0.01185262536970564\n",
      "epoch: 7, loss: 0.011615885598159641\n",
      "epoch: 8, loss: 0.01185320951515932\n",
      "epoch: 9, loss: 0.01160423213636829\n",
      "epoch: 10, loss: 0.01154905729880759\n",
      "epoch: 11, loss: 0.011639819616046328\n",
      "epoch: 12, loss: 0.011441296690776436\n",
      "epoch: 13, loss: 0.011902209654550976\n",
      "epoch: 14, loss: 0.011477121009212379\n",
      "epoch: 15, loss: 0.011518714071355724\n",
      "epoch: 16, loss: 0.011381966281987456\n",
      "epoch: 17, loss: 0.011650268387657576\n",
      "epoch: 18, loss: 0.01154835573475249\n",
      "epoch: 19, loss: 0.011780363476037734\n",
      "epoch: 20, loss: 0.01162627459332815\n",
      "epoch: 21, loss: 0.01158554859965728\n",
      "epoch: 22, loss: 0.011388225538858321\n",
      "epoch: 23, loss: 0.011406276081277948\n",
      "epoch: 24, loss: 0.01142429999733119\n",
      "epoch: 25, loss: 0.011501150683861565\n",
      "epoch: 26, loss: 0.011603640138416796\n",
      "epoch: 27, loss: 0.011346502606298194\n",
      "epoch: 28, loss: 0.011744174319784875\n",
      "epoch: 29, loss: 0.011379070886511135\n",
      "epoch: 30, loss: 0.011740096422694704\n",
      "epoch: 31, loss: 0.011497527541537758\n",
      "epoch: 32, loss: 0.011683878583756187\n",
      "epoch: 33, loss: 0.011436072102364084\n",
      "epoch: 34, loss: 0.011432950347635648\n",
      "epoch: 35, loss: 0.011407967965369352\n",
      "epoch: 36, loss: 0.011532626557572161\n",
      "epoch: 37, loss: 0.011630503910156912\n",
      "epoch: 38, loss: 0.01149553443393118\n",
      "epoch: 39, loss: 0.011462756966593832\n",
      "epoch: 40, loss: 0.011462337019880746\n",
      "epoch: 41, loss: 0.011540330190530556\n",
      "epoch: 42, loss: 0.01138433206114602\n",
      "epoch: 43, loss: 0.01178198834269359\n",
      "epoch: 44, loss: 0.01160975009758516\n",
      "epoch: 45, loss: 0.011632413256832966\n",
      "epoch: 46, loss: 0.011622025164084959\n",
      "epoch: 47, loss: 0.011781731333167232\n",
      "epoch: 48, loss: 0.011665703226628635\n",
      "epoch: 49, loss: 0.011369556728612111\n",
      "epoch: 50, loss: 0.011767719360513133\n",
      "epoch: 51, loss: 0.01177280490995084\n",
      "epoch: 52, loss: 0.011414429901412669\n",
      "epoch: 53, loss: 0.01148514793389377\n",
      "epoch: 54, loss: 0.011349645451911456\n",
      "epoch: 55, loss: 0.011682167391124446\n",
      "epoch: 56, loss: 0.011763525726954334\n",
      "epoch: 57, loss: 0.011664407307047472\n",
      "epoch: 58, loss: 0.011449572118332398\n",
      "epoch: 59, loss: 0.011621067145528545\n",
      "epoch: 60, loss: 0.011743253665154472\n",
      "epoch: 61, loss: 0.011770946942427517\n",
      "epoch: 62, loss: 0.011342809320922864\n",
      "epoch: 63, loss: 0.01168600389327452\n",
      "epoch: 64, loss: 0.011798786659395868\n",
      "epoch: 65, loss: 0.011599417252108684\n",
      "epoch: 66, loss: 0.01176808182573439\n",
      "epoch: 67, loss: 0.011892323927601577\n",
      "epoch: 68, loss: 0.011462159557069121\n",
      "epoch: 69, loss: 0.011385613351086909\n",
      "epoch: 70, loss: 0.01181328395920365\n",
      "epoch: 71, loss: 0.0117159784336468\n",
      "epoch: 72, loss: 0.011679260543861225\n",
      "epoch: 73, loss: 0.011518669955520245\n",
      "epoch: 74, loss: 0.011465485710743491\n",
      "epoch: 75, loss: 0.011450726350108329\n",
      "epoch: 76, loss: 0.011285552077935143\n",
      "epoch: 77, loss: 0.011422752219391905\n",
      "epoch: 78, loss: 0.011790442818458481\n",
      "epoch: 79, loss: 0.011540468634026156\n",
      "epoch: 80, loss: 0.011447192219990129\n",
      "epoch: 81, loss: 0.011658074163054791\n",
      "epoch: 82, loss: 0.01141855260848993\n",
      "epoch: 83, loss: 0.011520423993674889\n",
      "epoch: 84, loss: 0.011665053164268023\n",
      "epoch: 85, loss: 0.011515432832762861\n",
      "epoch: 86, loss: 0.011526796975925013\n",
      "epoch: 87, loss: 0.011874205593988474\n",
      "epoch: 88, loss: 0.011496595934646551\n",
      "epoch: 89, loss: 0.011727289135122778\n",
      "epoch: 90, loss: 0.011589235216148996\n",
      "epoch: 91, loss: 0.01174639497414568\n",
      "epoch: 92, loss: 0.011650856829147734\n",
      "epoch: 93, loss: 0.011428270899842326\n",
      "epoch: 94, loss: 0.01181856486263402\n",
      "epoch: 95, loss: 0.011832348945572942\n",
      "epoch: 96, loss: 0.011508982290960213\n",
      "epoch: 97, loss: 0.011836377115095699\n",
      "epoch: 98, loss: 0.011440231076924469\n",
      "epoch: 99, loss: 0.01157084171612125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52884b21d85f47899b09334e41431fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.012830170917000623\n",
      "epoch: 1, loss: 0.013286565079975712\n",
      "epoch: 2, loss: 0.01301810982753995\n",
      "epoch: 3, loss: 0.01273164277419426\n",
      "epoch: 4, loss: 0.01267885769286802\n",
      "epoch: 5, loss: 0.012823909658653453\n",
      "epoch: 6, loss: 0.012721512032123778\n",
      "epoch: 7, loss: 0.01264640601450535\n",
      "epoch: 8, loss: 0.012784840918092491\n",
      "epoch: 9, loss: 0.012907591716778754\n",
      "epoch: 10, loss: 0.012706501256430589\n",
      "epoch: 11, loss: 0.012931533239427294\n",
      "epoch: 12, loss: 0.01289799948993595\n",
      "epoch: 13, loss: 0.013033845884075922\n",
      "epoch: 14, loss: 0.01270486511734362\n",
      "epoch: 15, loss: 0.01265102168302442\n",
      "epoch: 16, loss: 0.012693962311738739\n",
      "epoch: 17, loss: 0.013225700332664472\n",
      "epoch: 18, loss: 0.012849707211142631\n",
      "epoch: 19, loss: 0.013038383655306785\n",
      "epoch: 20, loss: 0.012702711546881485\n",
      "epoch: 21, loss: 0.013202811422779726\n",
      "epoch: 22, loss: 0.012641849822219033\n",
      "epoch: 23, loss: 0.012882969087052623\n",
      "epoch: 24, loss: 0.013171958659441305\n",
      "epoch: 25, loss: 0.012968439417150975\n",
      "epoch: 26, loss: 0.013038660425903847\n",
      "epoch: 27, loss: 0.0127385293194051\n",
      "epoch: 28, loss: 0.01274919173528074\n",
      "epoch: 29, loss: 0.013060847410273354\n",
      "epoch: 30, loss: 0.012716499929288819\n",
      "epoch: 31, loss: 0.012911014122674577\n",
      "epoch: 32, loss: 0.013040097626820187\n",
      "epoch: 33, loss: 0.012790718810229825\n",
      "epoch: 34, loss: 0.013035351369409077\n",
      "epoch: 35, loss: 0.012974906530659403\n",
      "epoch: 36, loss: 0.012663413370976902\n",
      "epoch: 37, loss: 0.012833384615763024\n",
      "epoch: 38, loss: 0.012654908193240945\n",
      "epoch: 39, loss: 0.012798639446830622\n",
      "epoch: 40, loss: 0.013045585199293377\n",
      "epoch: 41, loss: 0.012918273226758212\n",
      "epoch: 42, loss: 0.013053000663929218\n",
      "epoch: 43, loss: 0.012900182375242255\n",
      "epoch: 44, loss: 0.01289510745063714\n",
      "epoch: 45, loss: 0.01283830738667988\n",
      "epoch: 46, loss: 0.012844416511834458\n",
      "epoch: 47, loss: 0.01295547561393062\n",
      "epoch: 48, loss: 0.013039842348409334\n",
      "epoch: 49, loss: 0.012911790592329539\n",
      "epoch: 50, loss: 0.013019159359286301\n",
      "epoch: 51, loss: 0.013064683377854762\n",
      "epoch: 52, loss: 0.01284107889295605\n",
      "epoch: 53, loss: 0.012653118117470531\n",
      "epoch: 54, loss: 0.012758180076506684\n",
      "epoch: 55, loss: 0.012757720316426677\n",
      "epoch: 56, loss: 0.012635690608446937\n",
      "epoch: 57, loss: 0.012713228237935255\n",
      "epoch: 58, loss: 0.012637070482744747\n",
      "epoch: 59, loss: 0.012669520652161927\n",
      "epoch: 60, loss: 0.012932478560322496\n",
      "epoch: 61, loss: 0.012789218313248588\n",
      "epoch: 62, loss: 0.01287462477729651\n",
      "epoch: 63, loss: 0.013118161913156815\n",
      "epoch: 64, loss: 0.012946326561499815\n",
      "epoch: 65, loss: 0.013165212934897027\n",
      "epoch: 66, loss: 0.012936667746050822\n",
      "epoch: 67, loss: 0.012904720604469752\n",
      "epoch: 68, loss: 0.012853530692197326\n",
      "epoch: 69, loss: 0.012831162091537375\n",
      "epoch: 70, loss: 0.013064103613014898\n",
      "epoch: 71, loss: 0.012926153220329195\n",
      "epoch: 72, loss: 0.013082617514533183\n",
      "epoch: 73, loss: 0.01298200655593341\n",
      "epoch: 74, loss: 0.012915536698624441\n",
      "epoch: 75, loss: 0.012790055026163856\n",
      "epoch: 76, loss: 0.012872330061842567\n",
      "epoch: 77, loss: 0.013018206789549827\n",
      "epoch: 78, loss: 0.012844669211572565\n",
      "epoch: 79, loss: 0.012971189472278212\n",
      "epoch: 80, loss: 0.012780295614817019\n",
      "epoch: 81, loss: 0.012763605687146002\n",
      "epoch: 82, loss: 0.013011511858070224\n",
      "epoch: 83, loss: 0.013131926000577653\n",
      "epoch: 84, loss: 0.01292167345586765\n",
      "epoch: 85, loss: 0.012812639344421842\n",
      "epoch: 86, loss: 0.012838082768372103\n",
      "epoch: 87, loss: 0.012838695117100073\n",
      "epoch: 88, loss: 0.012908761115236965\n",
      "epoch: 89, loss: 0.012960145526986684\n",
      "epoch: 90, loss: 0.012959865760260914\n",
      "epoch: 91, loss: 0.012967262026886169\n",
      "epoch: 92, loss: 0.013042565095220582\n",
      "epoch: 93, loss: 0.012771998868247121\n",
      "epoch: 94, loss: 0.01257430877679796\n",
      "epoch: 95, loss: 0.012769979430169458\n",
      "epoch: 96, loss: 0.012891742208885197\n",
      "epoch: 97, loss: 0.01277657281433326\n",
      "epoch: 98, loss: 0.01292623090210441\n",
      "epoch: 99, loss: 0.012825054598250304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546c812a29d141e69cf07377a6b35157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.00757305037939131\n",
      "epoch: 1, loss: 0.00810400250648579\n",
      "epoch: 2, loss: 0.008193873022896531\n",
      "epoch: 3, loss: 0.007331170637551201\n",
      "epoch: 4, loss: 0.00698308776212629\n",
      "epoch: 5, loss: 0.007165659768309427\n",
      "epoch: 6, loss: 0.007310049787245225\n",
      "epoch: 7, loss: 0.006760884523997457\n",
      "epoch: 8, loss: 0.00700884917138737\n",
      "epoch: 9, loss: 0.006655376276473736\n",
      "epoch: 10, loss: 0.006505260537951875\n",
      "epoch: 11, loss: 0.006508849878684826\n",
      "epoch: 12, loss: 0.007001350591818453\n",
      "epoch: 13, loss: 0.006568007832344526\n",
      "epoch: 14, loss: 0.0066333885427617\n",
      "epoch: 15, loss: 0.007127913958471139\n",
      "epoch: 16, loss: 0.006856543404083683\n",
      "epoch: 17, loss: 0.006810416019816984\n",
      "epoch: 18, loss: 0.00650199319542083\n",
      "epoch: 19, loss: 0.006731507717234488\n",
      "epoch: 20, loss: 0.006424400554131115\n",
      "epoch: 21, loss: 0.006554225432557837\n",
      "epoch: 22, loss: 0.0065547057219452155\n",
      "epoch: 23, loss: 0.00658667260102222\n",
      "epoch: 24, loss: 0.0064546770864141526\n",
      "epoch: 25, loss: 0.006649045133827209\n",
      "epoch: 26, loss: 0.006549927615791342\n",
      "epoch: 27, loss: 0.0064638593218047415\n",
      "epoch: 28, loss: 0.006412728450156258\n",
      "epoch: 29, loss: 0.006515289852910271\n",
      "epoch: 30, loss: 0.006387799963819248\n",
      "epoch: 31, loss: 0.0064448403714731\n",
      "epoch: 32, loss: 0.006530842702719236\n",
      "epoch: 33, loss: 0.0066510254120262235\n",
      "epoch: 34, loss: 0.006422156160301087\n",
      "epoch: 35, loss: 0.006575994432545907\n",
      "epoch: 36, loss: 0.006203150590234171\n",
      "epoch: 37, loss: 0.006158284120405398\n",
      "epoch: 38, loss: 0.006563138267874929\n",
      "epoch: 39, loss: 0.00651783054833063\n",
      "epoch: 40, loss: 0.006510862138914388\n",
      "epoch: 41, loss: 0.0065874058545460175\n",
      "epoch: 42, loss: 0.006302655795570573\n",
      "epoch: 43, loss: 0.006609772101179743\n",
      "epoch: 44, loss: 0.006514715005857182\n",
      "epoch: 45, loss: 0.006464159784110423\n",
      "epoch: 46, loss: 0.006333501172832046\n",
      "epoch: 47, loss: 0.006424294051412873\n",
      "epoch: 48, loss: 0.00644859430008517\n",
      "epoch: 49, loss: 0.00649862443899902\n",
      "epoch: 50, loss: 0.006448913512974149\n",
      "epoch: 51, loss: 0.006575835829316029\n",
      "epoch: 52, loss: 0.006504036178338118\n",
      "epoch: 53, loss: 0.0064494576920908556\n",
      "epoch: 54, loss: 0.006300922406397464\n",
      "epoch: 55, loss: 0.006387063089292788\n",
      "epoch: 56, loss: 0.00665803706574795\n",
      "epoch: 57, loss: 0.006543481578050475\n",
      "epoch: 58, loss: 0.006164050391298005\n",
      "epoch: 59, loss: 0.00664257941117758\n",
      "epoch: 60, loss: 0.0065779474663667774\n",
      "epoch: 61, loss: 0.006506543883553202\n",
      "epoch: 62, loss: 0.006594168431135968\n",
      "epoch: 63, loss: 0.0065234492864307565\n",
      "epoch: 64, loss: 0.006407741905738408\n",
      "epoch: 65, loss: 0.006157813458628296\n",
      "epoch: 66, loss: 0.006756178964857166\n",
      "epoch: 67, loss: 0.006465802371888239\n",
      "epoch: 68, loss: 0.006486271016485463\n",
      "epoch: 69, loss: 0.006452773496727582\n",
      "epoch: 70, loss: 0.00634766899322405\n",
      "epoch: 71, loss: 0.006187843348122588\n",
      "epoch: 72, loss: 0.0063774961073602195\n",
      "epoch: 73, loss: 0.006649050165376664\n",
      "epoch: 74, loss: 0.006559441231132144\n",
      "epoch: 75, loss: 0.006446565578063707\n",
      "epoch: 76, loss: 0.006275197975486494\n",
      "epoch: 77, loss: 0.006519001003486673\n",
      "epoch: 78, loss: 0.006523203448264898\n",
      "epoch: 79, loss: 0.006423567139572134\n",
      "epoch: 80, loss: 0.006265654171458905\n",
      "epoch: 81, loss: 0.006390773751369083\n",
      "epoch: 82, loss: 0.006505295838551085\n",
      "epoch: 83, loss: 0.006629841192681267\n",
      "epoch: 84, loss: 0.006345547208688212\n",
      "epoch: 85, loss: 0.006515384482858682\n",
      "epoch: 86, loss: 0.0062938174453956545\n",
      "epoch: 87, loss: 0.006457798042250816\n",
      "epoch: 88, loss: 0.006540667035538077\n",
      "epoch: 89, loss: 0.006666396685997978\n",
      "epoch: 90, loss: 0.006495038824939594\n",
      "epoch: 91, loss: 0.006658591396410191\n",
      "epoch: 92, loss: 0.006325432342553453\n",
      "epoch: 93, loss: 0.006124054342087638\n",
      "epoch: 94, loss: 0.006467431455797814\n",
      "epoch: 95, loss: 0.006430763605553829\n",
      "epoch: 96, loss: 0.006447780370672674\n",
      "epoch: 97, loss: 0.0065183082616798015\n",
      "epoch: 98, loss: 0.006290260378475239\n",
      "epoch: 99, loss: 0.006401627656620207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0402e2f3fe3461c840b7bd44968589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008969581782736121\n",
      "epoch: 1, loss: 0.008990403219843431\n",
      "epoch: 2, loss: 0.008950523233613623\n",
      "epoch: 3, loss: 0.008989770207221673\n",
      "epoch: 4, loss: 0.00912623588076741\n",
      "epoch: 5, loss: 0.008862800969600538\n",
      "epoch: 6, loss: 0.008962413682313139\n",
      "epoch: 7, loss: 0.008990016220524592\n",
      "epoch: 8, loss: 0.009013208473416901\n",
      "epoch: 9, loss: 0.009120436046291765\n",
      "epoch: 10, loss: 0.008966136350913322\n",
      "epoch: 11, loss: 0.009035606635332913\n",
      "epoch: 12, loss: 0.009037520082946893\n",
      "epoch: 13, loss: 0.008757138841433799\n",
      "epoch: 14, loss: 0.0088387335276872\n",
      "epoch: 15, loss: 0.008871449880655883\n",
      "epoch: 16, loss: 0.008920015591073438\n",
      "epoch: 17, loss: 0.009014859992911892\n",
      "epoch: 18, loss: 0.009217522304935322\n",
      "epoch: 19, loss: 0.00908088926186342\n",
      "epoch: 20, loss: 0.008895946925540117\n",
      "epoch: 21, loss: 0.008958517857740402\n",
      "epoch: 22, loss: 0.009140115659516973\n",
      "epoch: 23, loss: 0.008904817737129265\n",
      "epoch: 24, loss: 0.00907392866136311\n",
      "epoch: 25, loss: 0.009026199667109498\n",
      "epoch: 26, loss: 0.009063073794895325\n",
      "epoch: 27, loss: 0.008977878065123544\n",
      "epoch: 28, loss: 0.008940046127415203\n",
      "epoch: 29, loss: 0.008913452703250032\n",
      "epoch: 30, loss: 0.008876924214617887\n",
      "epoch: 31, loss: 0.008925014341722024\n",
      "epoch: 32, loss: 0.009099685838312192\n",
      "epoch: 33, loss: 0.009017203800045297\n",
      "epoch: 34, loss: 0.009036948301056377\n",
      "epoch: 35, loss: 0.009117540453078495\n",
      "epoch: 36, loss: 0.00891743529533101\n",
      "epoch: 37, loss: 0.008789365224999858\n",
      "epoch: 38, loss: 0.008854277118260253\n",
      "epoch: 39, loss: 0.00904597277946286\n",
      "epoch: 40, loss: 0.008877365481930006\n",
      "epoch: 41, loss: 0.009041349554333658\n",
      "epoch: 42, loss: 0.009075401697003362\n",
      "epoch: 43, loss: 0.008906307918912263\n",
      "epoch: 44, loss: 0.0089765588405734\n",
      "epoch: 45, loss: 0.00914201928222789\n",
      "epoch: 46, loss: 0.008805730842469554\n",
      "epoch: 47, loss: 0.009244148108071651\n",
      "epoch: 48, loss: 0.008976076844310633\n",
      "epoch: 49, loss: 0.008847262549720148\n",
      "epoch: 50, loss: 0.008917763163801927\n",
      "epoch: 51, loss: 0.008956767329693116\n",
      "epoch: 52, loss: 0.009040070040336004\n",
      "epoch: 53, loss: 0.009076367939798378\n",
      "epoch: 54, loss: 0.009023521678209065\n",
      "epoch: 55, loss: 0.008889310940837301\n",
      "epoch: 56, loss: 0.008792066961117874\n",
      "epoch: 57, loss: 0.009057260543434113\n",
      "epoch: 58, loss: 0.00903668121691738\n",
      "epoch: 59, loss: 0.008963341911969123\n",
      "epoch: 60, loss: 0.008779046374590743\n",
      "epoch: 61, loss: 0.009212519954982946\n",
      "epoch: 62, loss: 0.00901056964176464\n",
      "epoch: 63, loss: 0.009012875790794768\n",
      "epoch: 64, loss: 0.00889557890674872\n",
      "epoch: 65, loss: 0.009122168084382503\n",
      "epoch: 66, loss: 0.008951816439380113\n",
      "epoch: 67, loss: 0.00898701459717762\n",
      "epoch: 68, loss: 0.008963371214453555\n",
      "epoch: 69, loss: 0.008928028750265873\n",
      "epoch: 70, loss: 0.009043901365648302\n",
      "epoch: 71, loss: 0.008967977961252815\n",
      "epoch: 72, loss: 0.008892978617451634\n",
      "epoch: 73, loss: 0.008973669855420146\n",
      "epoch: 74, loss: 0.009018026008868456\n",
      "epoch: 75, loss: 0.008934962922820724\n",
      "epoch: 76, loss: 0.009025594990674098\n",
      "epoch: 77, loss: 0.008998377107163\n",
      "epoch: 78, loss: 0.008982202246646084\n",
      "epoch: 79, loss: 0.009036767600126014\n",
      "epoch: 80, loss: 0.008857515066258217\n",
      "epoch: 81, loss: 0.008961549372741627\n",
      "epoch: 82, loss: 0.00919663657283898\n",
      "epoch: 83, loss: 0.008964697085312634\n",
      "epoch: 84, loss: 0.008987944484862268\n",
      "epoch: 85, loss: 0.008971955157017427\n",
      "epoch: 86, loss: 0.00916278763847819\n",
      "epoch: 87, loss: 0.009059365038525594\n",
      "epoch: 88, loss: 0.009069035941801857\n",
      "epoch: 89, loss: 0.008924975966141393\n",
      "epoch: 90, loss: 0.009133118551063324\n",
      "epoch: 91, loss: 0.00901918738101074\n",
      "epoch: 92, loss: 0.008940198811048624\n",
      "epoch: 93, loss: 0.009070870285644343\n",
      "epoch: 94, loss: 0.009229243081791759\n",
      "epoch: 95, loss: 0.009036917812711577\n",
      "epoch: 96, loss: 0.008861623117749014\n",
      "epoch: 97, loss: 0.008866434796968161\n",
      "epoch: 98, loss: 0.009182640633141621\n",
      "epoch: 99, loss: 0.009024147144189454\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = loader(data_path(\"trainability_qnn_3D_reps_1\"))\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = qnn_list[i]\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1_epochs_200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d136f3302a482a9f8a0e3afba11d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82947f3a578341a39eb4922413d90c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008746134233797146\n",
      "epoch: 1, loss: 0.008937596194172405\n",
      "epoch: 2, loss: 0.009081986034377626\n",
      "epoch: 3, loss: 0.008913356035676508\n",
      "epoch: 4, loss: 0.008855881456499522\n",
      "epoch: 5, loss: 0.0090878548651503\n",
      "epoch: 6, loss: 0.00903318791048731\n",
      "epoch: 7, loss: 0.008972101147966675\n",
      "epoch: 8, loss: 0.008797889716290306\n",
      "epoch: 9, loss: 0.008860376932476042\n",
      "epoch: 10, loss: 0.009090441798964188\n",
      "epoch: 11, loss: 0.008985500336858689\n",
      "epoch: 12, loss: 0.008907518800971678\n",
      "epoch: 13, loss: 0.008848379263234084\n",
      "epoch: 14, loss: 0.009143261966099983\n",
      "epoch: 15, loss: 0.00892009940081591\n",
      "epoch: 16, loss: 0.00908063240543046\n",
      "epoch: 17, loss: 0.00896665215738498\n",
      "epoch: 18, loss: 0.008837618267906536\n",
      "epoch: 19, loss: 0.008855297073471644\n",
      "epoch: 20, loss: 0.008858821944218805\n",
      "epoch: 21, loss: 0.008822119436677232\n",
      "epoch: 22, loss: 0.008849292406866981\n",
      "epoch: 23, loss: 0.008912835486228494\n",
      "epoch: 24, loss: 0.00879436061032625\n",
      "epoch: 25, loss: 0.008715883856657634\n",
      "epoch: 26, loss: 0.008795829553032406\n",
      "epoch: 27, loss: 0.008915591886263395\n",
      "epoch: 28, loss: 0.008704512415193254\n",
      "epoch: 29, loss: 0.008793739853547882\n",
      "epoch: 30, loss: 0.008832497730038292\n",
      "epoch: 31, loss: 0.008674511082899287\n",
      "epoch: 32, loss: 0.00874758554938424\n",
      "epoch: 33, loss: 0.008702698571611829\n",
      "epoch: 34, loss: 0.008808556968460416\n",
      "epoch: 35, loss: 0.00873259450957573\n",
      "epoch: 36, loss: 0.008861391038533822\n",
      "epoch: 37, loss: 0.008667877895838848\n",
      "epoch: 38, loss: 0.008700995709396989\n",
      "epoch: 39, loss: 0.008840010989441574\n",
      "epoch: 40, loss: 0.008690169837763167\n",
      "epoch: 41, loss: 0.008865024770073844\n",
      "epoch: 42, loss: 0.008699323122790262\n",
      "epoch: 43, loss: 0.008635488045852601\n",
      "epoch: 44, loss: 0.008775405863784324\n",
      "epoch: 45, loss: 0.008716617450885209\n",
      "epoch: 46, loss: 0.008539121810434494\n",
      "epoch: 47, loss: 0.008706789103791879\n",
      "epoch: 48, loss: 0.008652062857639926\n",
      "epoch: 49, loss: 0.008707211126983524\n",
      "epoch: 50, loss: 0.008707986306198323\n",
      "epoch: 51, loss: 0.00841758773050595\n",
      "epoch: 52, loss: 0.008806008476495692\n",
      "epoch: 53, loss: 0.00851430913995128\n",
      "epoch: 54, loss: 0.00839797974476104\n",
      "epoch: 55, loss: 0.008491771454527843\n",
      "epoch: 56, loss: 0.008599271572444759\n",
      "epoch: 57, loss: 0.008453276991822224\n",
      "epoch: 58, loss: 0.008462999921148283\n",
      "epoch: 59, loss: 0.00854497780429296\n",
      "epoch: 60, loss: 0.008324934891019724\n",
      "epoch: 61, loss: 0.008660454905396173\n",
      "epoch: 62, loss: 0.008473104472052044\n",
      "epoch: 63, loss: 0.008453640798008567\n",
      "epoch: 64, loss: 0.008492435963291436\n",
      "epoch: 65, loss: 0.008440832955660124\n",
      "epoch: 66, loss: 0.008537248454681809\n",
      "epoch: 67, loss: 0.008331261834823578\n",
      "epoch: 68, loss: 0.00855088761755749\n",
      "epoch: 69, loss: 0.008447179400058654\n",
      "epoch: 70, loss: 0.00835516830183461\n",
      "epoch: 71, loss: 0.008328597251256117\n",
      "epoch: 72, loss: 0.008452819624580195\n",
      "epoch: 73, loss: 0.008631985114923166\n",
      "epoch: 74, loss: 0.008458331980848091\n",
      "epoch: 75, loss: 0.008325192697743884\n",
      "epoch: 76, loss: 0.00834691592521066\n",
      "epoch: 77, loss: 0.008353556789834371\n",
      "epoch: 78, loss: 0.00839871182109238\n",
      "epoch: 79, loss: 0.008443196823090046\n",
      "epoch: 80, loss: 0.00833863749263554\n",
      "epoch: 81, loss: 0.00850968064646305\n",
      "epoch: 82, loss: 0.00826869448639831\n",
      "epoch: 83, loss: 0.008343753861898591\n",
      "epoch: 84, loss: 0.008447925502364773\n",
      "epoch: 85, loss: 0.008506990870221156\n",
      "epoch: 86, loss: 0.0082735629290741\n",
      "epoch: 87, loss: 0.008307386637221373\n",
      "epoch: 88, loss: 0.00848868340180873\n",
      "epoch: 89, loss: 0.008317263712210357\n",
      "epoch: 90, loss: 0.00801040955722858\n",
      "epoch: 91, loss: 0.008235181214321713\n",
      "epoch: 92, loss: 0.008111509962141617\n",
      "epoch: 93, loss: 0.008160770359445756\n",
      "epoch: 94, loss: 0.00792817084663418\n",
      "epoch: 95, loss: 0.007743119643158864\n",
      "epoch: 96, loss: 0.007770692251205103\n",
      "epoch: 97, loss: 0.007910897050262839\n",
      "epoch: 98, loss: 0.007620907653085989\n",
      "epoch: 99, loss: 0.007577857941494868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b1d2f8c0fc483f93ef0047c0d14459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.009055371535323993\n",
      "epoch: 1, loss: 0.009067331949588534\n",
      "epoch: 2, loss: 0.009041857263811407\n",
      "epoch: 3, loss: 0.009077083198140027\n",
      "epoch: 4, loss: 0.008988829505578772\n",
      "epoch: 5, loss: 0.008939845944554194\n",
      "epoch: 6, loss: 0.008926057478245427\n",
      "epoch: 7, loss: 0.00904165863835962\n",
      "epoch: 8, loss: 0.009083069868847906\n",
      "epoch: 9, loss: 0.008917295097869414\n",
      "epoch: 10, loss: 0.009097310799236083\n",
      "epoch: 11, loss: 0.008977048601755277\n",
      "epoch: 12, loss: 0.00913737384679398\n",
      "epoch: 13, loss: 0.008915762313828079\n",
      "epoch: 14, loss: 0.009095273208648126\n",
      "epoch: 15, loss: 0.008875817555474997\n",
      "epoch: 16, loss: 0.008985186227912475\n",
      "epoch: 17, loss: 0.008961790222533695\n",
      "epoch: 18, loss: 0.008931200312599032\n",
      "epoch: 19, loss: 0.008972477528718952\n",
      "epoch: 20, loss: 0.00878275723009505\n",
      "epoch: 21, loss: 0.009029806067163281\n",
      "epoch: 22, loss: 0.008893867728750617\n",
      "epoch: 23, loss: 0.008862523072425453\n",
      "epoch: 24, loss: 0.008964072447301331\n",
      "epoch: 25, loss: 0.008763632154749146\n",
      "epoch: 26, loss: 0.008921313708046271\n",
      "epoch: 27, loss: 0.008909883224371086\n",
      "epoch: 28, loss: 0.008728014740897459\n",
      "epoch: 29, loss: 0.008801937822718351\n",
      "epoch: 30, loss: 0.008877363394044309\n",
      "epoch: 31, loss: 0.00866757642197645\n",
      "epoch: 32, loss: 0.008780724767592075\n",
      "epoch: 33, loss: 0.008818333481862149\n",
      "epoch: 34, loss: 0.008800642153097267\n",
      "epoch: 35, loss: 0.00873837944951971\n",
      "epoch: 36, loss: 0.008710658194264207\n",
      "epoch: 37, loss: 0.008732079349978276\n",
      "epoch: 38, loss: 0.008680339043762864\n",
      "epoch: 39, loss: 0.008539221770609978\n",
      "epoch: 40, loss: 0.008628704610815488\n",
      "epoch: 41, loss: 0.008804276593304626\n",
      "epoch: 42, loss: 0.00873699323139827\n",
      "epoch: 43, loss: 0.008717377354711701\n",
      "epoch: 44, loss: 0.008716695966506658\n",
      "epoch: 45, loss: 0.00866969682088075\n",
      "epoch: 46, loss: 0.008682319607529925\n",
      "epoch: 47, loss: 0.008676221889980788\n",
      "epoch: 48, loss: 0.00869297358877062\n",
      "epoch: 49, loss: 0.008653529591434962\n",
      "epoch: 50, loss: 0.008827513448019667\n",
      "epoch: 51, loss: 0.008752971976630927\n",
      "epoch: 52, loss: 0.008618710279793166\n",
      "epoch: 53, loss: 0.008684106336780049\n",
      "epoch: 54, loss: 0.00872091726458616\n",
      "epoch: 55, loss: 0.008857323385522456\n",
      "epoch: 56, loss: 0.008720729858889307\n",
      "epoch: 57, loss: 0.008625506727134193\n",
      "epoch: 58, loss: 0.00863152378902312\n",
      "epoch: 59, loss: 0.008696924216344678\n",
      "epoch: 60, loss: 0.008694769757304623\n",
      "epoch: 61, loss: 0.008665128824595462\n",
      "epoch: 62, loss: 0.008565517623980024\n",
      "epoch: 63, loss: 0.008701543754048332\n",
      "epoch: 64, loss: 0.008643178842493925\n",
      "epoch: 65, loss: 0.00877093190338437\n",
      "epoch: 66, loss: 0.008743487629963869\n",
      "epoch: 67, loss: 0.008743389814486338\n",
      "epoch: 68, loss: 0.008805827795007524\n",
      "epoch: 69, loss: 0.008724420149736034\n",
      "epoch: 70, loss: 0.008607706689160315\n",
      "epoch: 71, loss: 0.008744578322560665\n",
      "epoch: 72, loss: 0.00867133465236007\n",
      "epoch: 73, loss: 0.008678243948632373\n",
      "epoch: 74, loss: 0.00868725965707072\n",
      "epoch: 75, loss: 0.00867869374105109\n",
      "epoch: 76, loss: 0.008733797129164146\n",
      "epoch: 77, loss: 0.008710559835989598\n",
      "epoch: 78, loss: 0.008532687982031408\n",
      "epoch: 79, loss: 0.0088642922259701\n",
      "epoch: 80, loss: 0.008674755662014477\n",
      "epoch: 81, loss: 0.008644846940250365\n",
      "epoch: 82, loss: 0.008724311257877239\n",
      "epoch: 83, loss: 0.008812912679913347\n",
      "epoch: 84, loss: 0.008556108689953065\n",
      "epoch: 85, loss: 0.008579312347771546\n",
      "epoch: 86, loss: 0.008886671618065027\n",
      "epoch: 87, loss: 0.008550388135216554\n",
      "epoch: 88, loss: 0.00862330936201036\n",
      "epoch: 89, loss: 0.008713298161488612\n",
      "epoch: 90, loss: 0.0087175377649968\n",
      "epoch: 91, loss: 0.008744567044470162\n",
      "epoch: 92, loss: 0.008738442962131306\n",
      "epoch: 93, loss: 0.008510987888398434\n",
      "epoch: 94, loss: 0.008584678745963104\n",
      "epoch: 95, loss: 0.008638275290030223\n",
      "epoch: 96, loss: 0.008703516427609538\n",
      "epoch: 97, loss: 0.008763861654466652\n",
      "epoch: 98, loss: 0.00876800021934648\n",
      "epoch: 99, loss: 0.008674107972757647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5c39c21a8a43219cbdba073e645c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.00989933639338777\n",
      "epoch: 1, loss: 0.009930869123856505\n",
      "epoch: 2, loss: 0.010000345571632445\n",
      "epoch: 3, loss: 0.009994210965177172\n",
      "epoch: 4, loss: 0.009845751794893665\n",
      "epoch: 5, loss: 0.009599680874598516\n",
      "epoch: 6, loss: 0.009531611455659202\n",
      "epoch: 7, loss: 0.009504027846474743\n",
      "epoch: 8, loss: 0.009456416855735098\n",
      "epoch: 9, loss: 0.009217964603137735\n",
      "epoch: 10, loss: 0.009140590456779155\n",
      "epoch: 11, loss: 0.008983097365733675\n",
      "epoch: 12, loss: 0.009159160693452097\n",
      "epoch: 13, loss: 0.009095798064490747\n",
      "epoch: 14, loss: 0.008788174313210243\n",
      "epoch: 15, loss: 0.008884063830072118\n",
      "epoch: 16, loss: 0.008917762687263865\n",
      "epoch: 17, loss: 0.008757223377408899\n",
      "epoch: 18, loss: 0.008814979057955767\n",
      "epoch: 19, loss: 0.00874464950003251\n",
      "epoch: 20, loss: 0.008844559854578018\n",
      "epoch: 21, loss: 0.008826687763995256\n",
      "epoch: 22, loss: 0.009044861225583786\n",
      "epoch: 23, loss: 0.008644507727056092\n",
      "epoch: 24, loss: 0.008677208928179496\n",
      "epoch: 25, loss: 0.008611019148274321\n",
      "epoch: 26, loss: 0.008676611880196012\n",
      "epoch: 27, loss: 0.008616257816295195\n",
      "epoch: 28, loss: 0.008745036039060997\n",
      "epoch: 29, loss: 0.008453396470102969\n",
      "epoch: 30, loss: 0.008385918253305516\n",
      "epoch: 31, loss: 0.008658626259990991\n",
      "epoch: 32, loss: 0.008456819506570767\n",
      "epoch: 33, loss: 0.008455880504254169\n",
      "epoch: 34, loss: 0.008623059730517371\n",
      "epoch: 35, loss: 0.00859996985623747\n",
      "epoch: 36, loss: 0.008626271161875108\n",
      "epoch: 37, loss: 0.008619536394520707\n",
      "epoch: 38, loss: 0.008586918754148088\n",
      "epoch: 39, loss: 0.008409245625428767\n",
      "epoch: 40, loss: 0.008500089583215733\n",
      "epoch: 41, loss: 0.008380393316652373\n",
      "epoch: 42, loss: 0.008354758413046323\n",
      "epoch: 43, loss: 0.008292181005967002\n",
      "epoch: 44, loss: 0.008351648916544364\n",
      "epoch: 45, loss: 0.008602165593185837\n",
      "epoch: 46, loss: 0.008406745060331004\n",
      "epoch: 47, loss: 0.00829428773080734\n",
      "epoch: 48, loss: 0.008377107714525621\n",
      "epoch: 49, loss: 0.00849453979400958\n",
      "epoch: 50, loss: 0.008245859491120264\n",
      "epoch: 51, loss: 0.008389641755723543\n",
      "epoch: 52, loss: 0.008543544152356261\n",
      "epoch: 53, loss: 0.00844527446311711\n",
      "epoch: 54, loss: 0.008326567380912787\n",
      "epoch: 55, loss: 0.008497387696932176\n",
      "epoch: 56, loss: 0.008262185327223791\n",
      "epoch: 57, loss: 0.008470414258551164\n",
      "epoch: 58, loss: 0.008323394022419543\n",
      "epoch: 59, loss: 0.008548617338381874\n",
      "epoch: 60, loss: 0.00855223951633005\n",
      "epoch: 61, loss: 0.00839572282206103\n",
      "epoch: 62, loss: 0.008416725597726566\n",
      "epoch: 63, loss: 0.00827456441037262\n",
      "epoch: 64, loss: 0.008342875856003728\n",
      "epoch: 65, loss: 0.008379047580147323\n",
      "epoch: 66, loss: 0.008413252903775034\n",
      "epoch: 67, loss: 0.00861753672379595\n",
      "epoch: 68, loss: 0.008413849612939648\n",
      "epoch: 69, loss: 0.008500319991062658\n",
      "epoch: 70, loss: 0.008563855043045838\n",
      "epoch: 71, loss: 0.008324197985826667\n",
      "epoch: 72, loss: 0.00831803476063698\n",
      "epoch: 73, loss: 0.008341138094168678\n",
      "epoch: 74, loss: 0.008323198499613004\n",
      "epoch: 75, loss: 0.008320619732272947\n",
      "epoch: 76, loss: 0.008318806083412787\n",
      "epoch: 77, loss: 0.008396252505882891\n",
      "epoch: 78, loss: 0.008365886111203555\n",
      "epoch: 79, loss: 0.008230359658720446\n",
      "epoch: 80, loss: 0.00836451728948897\n",
      "epoch: 81, loss: 0.008320482407126496\n",
      "epoch: 82, loss: 0.008229466248930542\n",
      "epoch: 83, loss: 0.008346268053000823\n",
      "epoch: 84, loss: 0.008278296883398654\n",
      "epoch: 85, loss: 0.008309380788044097\n",
      "epoch: 86, loss: 0.008438746325981532\n",
      "epoch: 87, loss: 0.00847313460406833\n",
      "epoch: 88, loss: 0.008227779420071859\n",
      "epoch: 89, loss: 0.008334035952744675\n",
      "epoch: 90, loss: 0.00838891334539668\n",
      "epoch: 91, loss: 0.008285041370177932\n",
      "epoch: 92, loss: 0.008402116272642316\n",
      "epoch: 93, loss: 0.008489879821697719\n",
      "epoch: 94, loss: 0.008299923385739975\n",
      "epoch: 95, loss: 0.008349944373722074\n",
      "epoch: 96, loss: 0.008412060275690874\n",
      "epoch: 97, loss: 0.008542892581529554\n",
      "epoch: 98, loss: 0.008286627475595283\n",
      "epoch: 99, loss: 0.008141837093100854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79e60cf2f8f453c8d50abce88ab69e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.008102538306410239\n",
      "epoch: 1, loss: 0.008084062653029354\n",
      "epoch: 2, loss: 0.008156739522289512\n",
      "epoch: 3, loss: 0.007813203669178601\n",
      "epoch: 4, loss: 0.008212897488985757\n",
      "epoch: 5, loss: 0.008125856080069487\n",
      "epoch: 6, loss: 0.007953940263476969\n",
      "epoch: 7, loss: 0.008197058121678237\n",
      "epoch: 8, loss: 0.008037712353876235\n",
      "epoch: 9, loss: 0.00810022129593939\n",
      "epoch: 10, loss: 0.00807393756213914\n",
      "epoch: 11, loss: 0.008039992669074045\n",
      "epoch: 12, loss: 0.008038608868575473\n",
      "epoch: 13, loss: 0.008130989870517493\n",
      "epoch: 14, loss: 0.008054774462851567\n",
      "epoch: 15, loss: 0.008133608028869085\n",
      "epoch: 16, loss: 0.008170270317660858\n",
      "epoch: 17, loss: 0.00806886705813575\n",
      "epoch: 18, loss: 0.008170451937802643\n",
      "epoch: 19, loss: 0.00813750338581141\n",
      "epoch: 20, loss: 0.008262341522775904\n",
      "epoch: 21, loss: 0.007882458035645575\n",
      "epoch: 22, loss: 0.00792411307367969\n",
      "epoch: 23, loss: 0.008276269515941681\n",
      "epoch: 24, loss: 0.008115101675799015\n",
      "epoch: 25, loss: 0.008044732942717294\n",
      "epoch: 26, loss: 0.00812157561611271\n",
      "epoch: 27, loss: 0.008149430927724867\n",
      "epoch: 28, loss: 0.008144752533896913\n",
      "epoch: 29, loss: 0.008041478316906615\n",
      "epoch: 30, loss: 0.00790133139264754\n",
      "epoch: 31, loss: 0.008098908401694074\n",
      "epoch: 32, loss: 0.008062014041557448\n",
      "epoch: 33, loss: 0.008004565904292792\n",
      "epoch: 34, loss: 0.007881999414776044\n",
      "epoch: 35, loss: 0.0079036512579841\n",
      "epoch: 36, loss: 0.007686121440028336\n",
      "epoch: 37, loss: 0.007694189471873185\n",
      "epoch: 38, loss: 0.007485458028270771\n",
      "epoch: 39, loss: 0.007551911967631074\n",
      "epoch: 40, loss: 0.007427361434650358\n",
      "epoch: 41, loss: 0.0073419097970962004\n",
      "epoch: 42, loss: 0.007345546794320724\n",
      "epoch: 43, loss: 0.007226920424178368\n",
      "epoch: 44, loss: 0.00715559884857168\n",
      "epoch: 45, loss: 0.006948361922236656\n",
      "epoch: 46, loss: 0.007119276480784899\n",
      "epoch: 47, loss: 0.0070247709416696134\n",
      "epoch: 48, loss: 0.007180213791294009\n",
      "epoch: 49, loss: 0.00678327085990106\n",
      "epoch: 50, loss: 0.006859942693200732\n",
      "epoch: 51, loss: 0.0067868004034472035\n",
      "epoch: 52, loss: 0.006887606062432944\n",
      "epoch: 53, loss: 0.006726464525160267\n",
      "epoch: 54, loss: 0.006634471639700079\n",
      "epoch: 55, loss: 0.00648777318561598\n",
      "epoch: 56, loss: 0.006558900545645128\n",
      "epoch: 57, loss: 0.00638463309335009\n",
      "epoch: 58, loss: 0.00624292138833527\n",
      "epoch: 59, loss: 0.006384429761693356\n",
      "epoch: 60, loss: 0.006142709404460082\n",
      "epoch: 61, loss: 0.006116598313392197\n",
      "epoch: 62, loss: 0.006067079032336214\n",
      "epoch: 63, loss: 0.006001166219504206\n",
      "epoch: 64, loss: 0.006117119315031402\n",
      "epoch: 65, loss: 0.006115136350626531\n",
      "epoch: 66, loss: 0.006082104256399774\n",
      "epoch: 67, loss: 0.005916542791162993\n",
      "epoch: 68, loss: 0.006195392355016534\n",
      "epoch: 69, loss: 0.006209400297856708\n",
      "epoch: 70, loss: 0.00600559859776876\n",
      "epoch: 71, loss: 0.006050475175065127\n",
      "epoch: 72, loss: 0.006120444121882835\n",
      "epoch: 73, loss: 0.00595245914058535\n",
      "epoch: 74, loss: 0.005901333302554053\n",
      "epoch: 75, loss: 0.0058514715135471055\n",
      "epoch: 76, loss: 0.005859987964514767\n",
      "epoch: 77, loss: 0.0061185410793636475\n",
      "epoch: 78, loss: 0.005784366787456684\n",
      "epoch: 79, loss: 0.005783422036197251\n",
      "epoch: 80, loss: 0.005862200919655141\n",
      "epoch: 81, loss: 0.006008018349166213\n",
      "epoch: 82, loss: 0.005990832304550764\n",
      "epoch: 83, loss: 0.005918633924778969\n",
      "epoch: 84, loss: 0.005973870183544793\n",
      "epoch: 85, loss: 0.005918751219471384\n",
      "epoch: 86, loss: 0.005854890042444354\n",
      "epoch: 87, loss: 0.005821430871928034\n",
      "epoch: 88, loss: 0.005933701947515109\n",
      "epoch: 89, loss: 0.0058997260897838445\n",
      "epoch: 90, loss: 0.005895352201202061\n",
      "epoch: 91, loss: 0.005699588940796226\n",
      "epoch: 92, loss: 0.005822048450222123\n",
      "epoch: 93, loss: 0.005927832467614718\n",
      "epoch: 94, loss: 0.005740272723661323\n",
      "epoch: 95, loss: 0.005917958678608753\n",
      "epoch: 96, loss: 0.00573284948004207\n",
      "epoch: 97, loss: 0.005735909437303778\n",
      "epoch: 98, loss: 0.005533475367377834\n",
      "epoch: 99, loss: 0.005542696990766213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de562d078034440194421364b74c2851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.006912370323641016\n",
      "epoch: 1, loss: 0.006783292849428258\n",
      "epoch: 2, loss: 0.006907514266761174\n",
      "epoch: 3, loss: 0.007006003363531338\n",
      "epoch: 4, loss: 0.006954118155741081\n",
      "epoch: 5, loss: 0.006990239109371334\n",
      "epoch: 6, loss: 0.0070091614171556614\n",
      "epoch: 7, loss: 0.006734634441801671\n",
      "epoch: 8, loss: 0.006981041505693204\n",
      "epoch: 9, loss: 0.007019075609251479\n",
      "epoch: 10, loss: 0.006954371361974437\n",
      "epoch: 11, loss: 0.006935354210318921\n",
      "epoch: 12, loss: 0.006790149813036379\n",
      "epoch: 13, loss: 0.006579884227644361\n",
      "epoch: 14, loss: 0.006760929855448221\n",
      "epoch: 15, loss: 0.006716719564092678\n",
      "epoch: 16, loss: 0.006808398180837867\n",
      "epoch: 17, loss: 0.006874711996305145\n",
      "epoch: 18, loss: 0.006851420490279158\n",
      "epoch: 19, loss: 0.006777733609710033\n",
      "epoch: 20, loss: 0.0066962276636044855\n",
      "epoch: 21, loss: 0.006598594488226144\n",
      "epoch: 22, loss: 0.006871275516235155\n",
      "epoch: 23, loss: 0.006765660637479334\n",
      "epoch: 24, loss: 0.006614422753377147\n",
      "epoch: 25, loss: 0.006660332807216115\n",
      "epoch: 26, loss: 0.006635734162524111\n",
      "epoch: 27, loss: 0.006753211180464864\n",
      "epoch: 28, loss: 0.006511372907305745\n",
      "epoch: 29, loss: 0.006757182874498204\n",
      "epoch: 30, loss: 0.006549554403759594\n",
      "epoch: 31, loss: 0.0067390385492469606\n",
      "epoch: 32, loss: 0.0063896454780193465\n",
      "epoch: 33, loss: 0.006562551744712305\n",
      "epoch: 34, loss: 0.006469855403913747\n",
      "epoch: 35, loss: 0.006752758956049\n",
      "epoch: 36, loss: 0.006552185278352264\n",
      "epoch: 37, loss: 0.00659971975713799\n",
      "epoch: 38, loss: 0.006494557000242425\n",
      "epoch: 39, loss: 0.006444877181010662\n",
      "epoch: 40, loss: 0.006496222129292227\n",
      "epoch: 41, loss: 0.006388803281183386\n",
      "epoch: 42, loss: 0.006437307842326795\n",
      "epoch: 43, loss: 0.006552693747436743\n",
      "epoch: 44, loss: 0.00647506195475639\n",
      "epoch: 45, loss: 0.006515776724598953\n",
      "epoch: 46, loss: 0.006512803864489967\n",
      "epoch: 47, loss: 0.006562315579370286\n",
      "epoch: 48, loss: 0.006423458358782472\n",
      "epoch: 49, loss: 0.006287617179873702\n",
      "epoch: 50, loss: 0.006454617475412544\n",
      "epoch: 51, loss: 0.006428039438267142\n",
      "epoch: 52, loss: 0.006500976789131591\n",
      "epoch: 53, loss: 0.006447698142099538\n",
      "epoch: 54, loss: 0.006374119686612441\n",
      "epoch: 55, loss: 0.006485317408459477\n",
      "epoch: 56, loss: 0.0062766633473894775\n",
      "epoch: 57, loss: 0.0064615630724050134\n",
      "epoch: 58, loss: 0.006540781633815219\n",
      "epoch: 59, loss: 0.00650371781034475\n",
      "epoch: 60, loss: 0.006535807128217917\n",
      "epoch: 61, loss: 0.006498493878456578\n",
      "epoch: 62, loss: 0.0062356042388733565\n",
      "epoch: 63, loss: 0.0063424080719591495\n",
      "epoch: 64, loss: 0.006287709339012575\n",
      "epoch: 65, loss: 0.006483074659021219\n",
      "epoch: 66, loss: 0.006412472615570085\n",
      "epoch: 67, loss: 0.00645922722307808\n",
      "epoch: 68, loss: 0.006567340051082276\n",
      "epoch: 69, loss: 0.006655664721081418\n",
      "epoch: 70, loss: 0.006586402635508028\n",
      "epoch: 71, loss: 0.006744247759145773\n",
      "epoch: 72, loss: 0.006320769054875248\n",
      "epoch: 73, loss: 0.006331637449820358\n",
      "epoch: 74, loss: 0.006306252517275302\n",
      "epoch: 75, loss: 0.00642564310071893\n",
      "epoch: 76, loss: 0.006407331648148392\n",
      "epoch: 77, loss: 0.006203970235625245\n",
      "epoch: 78, loss: 0.006334664262545792\n",
      "epoch: 79, loss: 0.0064373237815477085\n",
      "epoch: 80, loss: 0.006343571735256014\n",
      "epoch: 81, loss: 0.00614485719120264\n",
      "epoch: 82, loss: 0.006497451814353346\n",
      "epoch: 83, loss: 0.006304082261552963\n",
      "epoch: 84, loss: 0.006281787741681504\n",
      "epoch: 85, loss: 0.0063526748655321815\n",
      "epoch: 86, loss: 0.006337295578608932\n",
      "epoch: 87, loss: 0.0063467446282774575\n",
      "epoch: 88, loss: 0.006321873570904578\n",
      "epoch: 89, loss: 0.006657266573655242\n",
      "epoch: 90, loss: 0.00620836445703634\n",
      "epoch: 91, loss: 0.006345788831257016\n",
      "epoch: 92, loss: 0.0064129468769671336\n",
      "epoch: 93, loss: 0.0063585722143772744\n",
      "epoch: 94, loss: 0.006288505728885017\n",
      "epoch: 95, loss: 0.006205446882832039\n",
      "epoch: 96, loss: 0.006231785020439491\n",
      "epoch: 97, loss: 0.006437160116627656\n",
      "epoch: 98, loss: 0.0064196754697424495\n",
      "epoch: 99, loss: 0.00642562601152824\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = loader(data_path(\"trainability_qnn_3D_reps_2\"))\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = qnn_list[i]\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2_epochs_200\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b26d4144394cbf806adc277b30998c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07127398614235778\n",
      "epoch: 1, loss: 0.03859135820573581\n",
      "epoch: 2, loss: 0.030764468021473118\n",
      "epoch: 3, loss: 0.027220754720571776\n",
      "epoch: 4, loss: 0.02679027360532808\n",
      "epoch: 5, loss: 0.024991331310244988\n",
      "epoch: 6, loss: 0.02285660423840059\n",
      "epoch: 7, loss: 0.02178305827616\n",
      "epoch: 8, loss: 0.02063444313487282\n",
      "epoch: 9, loss: 0.019071287919812364\n",
      "epoch: 10, loss: 0.018074138267264336\n",
      "epoch: 11, loss: 0.01821947423153643\n",
      "epoch: 12, loss: 0.01811047410409626\n",
      "epoch: 13, loss: 0.018165278904042008\n",
      "epoch: 14, loss: 0.01793703926835272\n",
      "epoch: 15, loss: 0.0176410844272219\n",
      "epoch: 16, loss: 0.01730191936388806\n",
      "epoch: 17, loss: 0.01711680167276803\n",
      "epoch: 18, loss: 0.016605563645549985\n",
      "epoch: 19, loss: 0.016833693151009348\n",
      "epoch: 20, loss: 0.01653656480817363\n",
      "epoch: 21, loss: 0.01634578939455766\n",
      "epoch: 22, loss: 0.015500163648678957\n",
      "epoch: 23, loss: 0.015482478876533165\n",
      "epoch: 24, loss: 0.015040812347437885\n",
      "epoch: 25, loss: 0.014641995641454129\n",
      "epoch: 26, loss: 0.014394075068666752\n",
      "epoch: 27, loss: 0.014355755623623753\n",
      "epoch: 28, loss: 0.014080487154378952\n",
      "epoch: 29, loss: 0.013870946024701177\n",
      "epoch: 30, loss: 0.01321675319343042\n",
      "epoch: 31, loss: 0.013284712542928302\n",
      "epoch: 32, loss: 0.013812975207626602\n",
      "epoch: 33, loss: 0.013082376842405484\n",
      "epoch: 34, loss: 0.012781918118099513\n",
      "epoch: 35, loss: 0.012533005437897078\n",
      "epoch: 36, loss: 0.012299949063708201\n",
      "epoch: 37, loss: 0.01178279889490009\n",
      "epoch: 38, loss: 0.011799504514741278\n",
      "epoch: 39, loss: 0.011753230363936095\n",
      "epoch: 40, loss: 0.01148318669894991\n",
      "epoch: 41, loss: 0.011404617104275554\n",
      "epoch: 42, loss: 0.011085245798621213\n",
      "epoch: 43, loss: 0.0110209717756383\n",
      "epoch: 44, loss: 0.01080142431157999\n",
      "epoch: 45, loss: 0.010551431544950082\n",
      "epoch: 46, loss: 0.01022685860884935\n",
      "epoch: 47, loss: 0.010194734659759187\n",
      "epoch: 48, loss: 0.010112731922624803\n",
      "epoch: 49, loss: 0.010015658149987046\n",
      "epoch: 50, loss: 0.009777368712724685\n",
      "epoch: 51, loss: 0.00962935745691862\n",
      "epoch: 52, loss: 0.009157465632813104\n",
      "epoch: 53, loss: 0.009407645121940067\n",
      "epoch: 54, loss: 0.009152155695954324\n",
      "epoch: 55, loss: 0.008977708544678983\n",
      "epoch: 56, loss: 0.009059462176713058\n",
      "epoch: 57, loss: 0.009295600400679178\n",
      "epoch: 58, loss: 0.009179611347025787\n",
      "epoch: 59, loss: 0.009160755556044434\n",
      "epoch: 60, loss: 0.00908465598430744\n",
      "epoch: 61, loss: 0.00890272555964217\n",
      "epoch: 62, loss: 0.008843974387363686\n",
      "epoch: 63, loss: 0.00898334637280458\n",
      "epoch: 64, loss: 0.008616983716344662\n",
      "epoch: 65, loss: 0.008558046599545559\n",
      "epoch: 66, loss: 0.00837412896966017\n",
      "epoch: 67, loss: 0.008567253976730834\n",
      "epoch: 68, loss: 0.008568889790803282\n",
      "epoch: 69, loss: 0.008427482350520684\n",
      "epoch: 70, loss: 0.008235921729957055\n",
      "epoch: 71, loss: 0.00838749409849234\n",
      "epoch: 72, loss: 0.008376255097200398\n",
      "epoch: 73, loss: 0.00831019341222881\n",
      "epoch: 74, loss: 0.008307300651372957\n",
      "epoch: 75, loss: 0.008517748049962017\n",
      "epoch: 76, loss: 0.008193136972364148\n",
      "epoch: 77, loss: 0.00828059884143291\n",
      "epoch: 78, loss: 0.008055399909276008\n",
      "epoch: 79, loss: 0.00854882524006499\n",
      "epoch: 80, loss: 0.00832414185289855\n",
      "epoch: 81, loss: 0.008195254367175348\n",
      "epoch: 82, loss: 0.00814587554860551\n",
      "epoch: 83, loss: 0.008341817203724523\n",
      "epoch: 84, loss: 0.008176971131014165\n",
      "epoch: 85, loss: 0.008368516643254968\n",
      "epoch: 86, loss: 0.008021733825216886\n",
      "epoch: 87, loss: 0.008148046155408295\n",
      "epoch: 88, loss: 0.008166051515700224\n",
      "epoch: 89, loss: 0.008124633761861807\n",
      "epoch: 90, loss: 0.008204955893212186\n",
      "epoch: 91, loss: 0.008166536355564205\n",
      "epoch: 92, loss: 0.00811548466543255\n",
      "epoch: 93, loss: 0.008154988840484932\n",
      "epoch: 94, loss: 0.008153688870625125\n",
      "epoch: 95, loss: 0.008072130890368814\n",
      "epoch: 96, loss: 0.008032695569624994\n",
      "epoch: 97, loss: 0.007923371814855041\n",
      "epoch: 98, loss: 0.008035921755790479\n",
      "epoch: 99, loss: 0.008009592401396204\n",
      "epoch: 100, loss: 0.008189166615755134\n",
      "epoch: 101, loss: 0.008098028563117674\n",
      "epoch: 102, loss: 0.008064212522692868\n",
      "epoch: 103, loss: 0.008112002031641847\n",
      "epoch: 104, loss: 0.008008016403928793\n",
      "epoch: 105, loss: 0.007746250376729857\n",
      "epoch: 106, loss: 0.008001006015907634\n",
      "epoch: 107, loss: 0.007955005809549099\n",
      "epoch: 108, loss: 0.00790449467082992\n",
      "epoch: 109, loss: 0.007980192314111704\n",
      "epoch: 110, loss: 0.0077471706675193925\n",
      "epoch: 111, loss: 0.007733869061241272\n",
      "epoch: 112, loss: 0.007601284459700286\n",
      "epoch: 113, loss: 0.007670760423477429\n",
      "epoch: 114, loss: 0.007992122282589829\n",
      "epoch: 115, loss: 0.007869446126919592\n",
      "epoch: 116, loss: 0.00758906633356865\n",
      "epoch: 117, loss: 0.007942123426162585\n",
      "epoch: 118, loss: 0.007695582426859256\n",
      "epoch: 119, loss: 0.007904425980198703\n",
      "epoch: 120, loss: 0.008003513698127261\n",
      "epoch: 121, loss: 0.007653786205901985\n",
      "epoch: 122, loss: 0.00780950082756412\n",
      "epoch: 123, loss: 0.007807538024179907\n",
      "epoch: 124, loss: 0.00771937908007464\n",
      "epoch: 125, loss: 0.007693975527279134\n",
      "epoch: 126, loss: 0.00775865524357152\n",
      "epoch: 127, loss: 0.007631917093810377\n",
      "epoch: 128, loss: 0.00790046843456402\n",
      "epoch: 129, loss: 0.007717330772571667\n",
      "epoch: 130, loss: 0.007655912141137303\n",
      "epoch: 131, loss: 0.007710384843666772\n",
      "epoch: 132, loss: 0.007624174771960122\n",
      "epoch: 133, loss: 0.007952488494775053\n",
      "epoch: 134, loss: 0.007558165282669038\n",
      "epoch: 135, loss: 0.007536185670248753\n",
      "epoch: 136, loss: 0.007610985510826194\n",
      "epoch: 137, loss: 0.007334040419637915\n",
      "epoch: 138, loss: 0.007713332719902418\n",
      "epoch: 139, loss: 0.0075426619579636355\n",
      "epoch: 140, loss: 0.007564171344132334\n",
      "epoch: 141, loss: 0.0074798462009543045\n",
      "epoch: 142, loss: 0.007521494547932215\n",
      "epoch: 143, loss: 0.007698823164236195\n",
      "epoch: 144, loss: 0.0075299892239812805\n",
      "epoch: 145, loss: 0.007533305615638129\n",
      "epoch: 146, loss: 0.007382717558201275\n",
      "epoch: 147, loss: 0.007748171336141779\n",
      "epoch: 148, loss: 0.00738779041723676\n",
      "epoch: 149, loss: 0.007377324381110136\n",
      "epoch: 150, loss: 0.007270234765582224\n",
      "epoch: 151, loss: 0.007392174361432479\n",
      "epoch: 152, loss: 0.007425604082461191\n",
      "epoch: 153, loss: 0.007322892642505864\n",
      "epoch: 154, loss: 0.007456631048182482\n",
      "epoch: 155, loss: 0.007555716115209763\n",
      "epoch: 156, loss: 0.007209288210014147\n",
      "epoch: 157, loss: 0.007524396413538635\n",
      "epoch: 158, loss: 0.007368026610508793\n",
      "epoch: 159, loss: 0.007370614164694334\n",
      "epoch: 160, loss: 0.007570386300604503\n",
      "epoch: 161, loss: 0.007430455478110543\n",
      "epoch: 162, loss: 0.007445150611296097\n",
      "epoch: 163, loss: 0.00763944145073835\n",
      "epoch: 164, loss: 0.007381688247913993\n",
      "epoch: 165, loss: 0.007227661202952123\n",
      "epoch: 166, loss: 0.0072969986403969775\n",
      "epoch: 167, loss: 0.007242168605370586\n",
      "epoch: 168, loss: 0.007152229487313212\n",
      "epoch: 169, loss: 0.007220834900263143\n",
      "epoch: 170, loss: 0.007172383543317668\n",
      "epoch: 171, loss: 0.007255570805970278\n",
      "epoch: 172, loss: 0.007187059937168647\n",
      "epoch: 173, loss: 0.007377241215493902\n",
      "epoch: 174, loss: 0.007440540591583368\n",
      "epoch: 175, loss: 0.006860001571580015\n",
      "epoch: 176, loss: 0.007293693783435428\n",
      "epoch: 177, loss: 0.007551524499568445\n",
      "epoch: 178, loss: 0.007218872532006702\n",
      "epoch: 179, loss: 0.007310318647721132\n",
      "epoch: 180, loss: 0.007363578656602203\n",
      "epoch: 181, loss: 0.007242030043816095\n",
      "epoch: 182, loss: 0.007425672128129759\n",
      "epoch: 183, loss: 0.007509823767958967\n",
      "epoch: 184, loss: 0.0069097624817709815\n",
      "epoch: 185, loss: 0.007050152279000182\n",
      "epoch: 186, loss: 0.00738818581989817\n",
      "epoch: 187, loss: 0.007027323910178852\n",
      "epoch: 188, loss: 0.00703426752122594\n",
      "epoch: 189, loss: 0.007410379536225313\n",
      "epoch: 190, loss: 0.00710235404878366\n",
      "epoch: 191, loss: 0.0069753972223195355\n",
      "epoch: 192, loss: 0.006854468907584159\n",
      "epoch: 193, loss: 0.006902403050072018\n",
      "epoch: 194, loss: 0.006755332717503355\n",
      "epoch: 195, loss: 0.007131274175411388\n",
      "epoch: 196, loss: 0.007041146768713472\n",
      "epoch: 197, loss: 0.006890194604912853\n",
      "epoch: 198, loss: 0.006850002834324447\n",
      "epoch: 199, loss: 0.006875258314180448\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(n_qubits = [3, 4, 4],\n",
    "                         dim = [3, 4, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "\n",
    "qnn.train(x_qnn, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qnn.loss)\n",
    "#plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.train(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
