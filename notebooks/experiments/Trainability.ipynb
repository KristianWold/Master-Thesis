{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Constant Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = 0.5*np.ones((n,1))\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1_constant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2_constant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [1, 3, 1],\n",
    "                         lr = 0.1)\n",
    "    \n",
    "    dnn.train(x, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_constant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         lr = 0.1)\n",
    "    \n",
    "    dnn.train(x, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=0, b=np.pi)\n",
    "x_dnn = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [2, 6, 1],\n",
    "                     lr = 0.1)\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=5000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 5\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "\n",
    "y = 0.5*np.ones((n**3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_constant_reps_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=0, b=np.pi)\n",
    "x_dnn = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "\n",
    "y = scaler(y, a=0.1, b=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 5,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_single_circuit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [3, 6, 1],\n",
    "                     lr = 0.1)\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKvElEQVR4nO3d32vd9R3H8dfLpF1L4g+0TmpTVgciFMF2xF5YNlhxo/5Ad6mgV0JvJrRsInrpP+BksJugsonOoqggzh8raHEFfzStrbNWRykdDS10zokm6GrS9y5y2iUmbb7nm/PN58vb5wOCiedwfFH77DfnpOf7dUQIQB4XlR4AoLeIGkiGqIFkiBpIhqiBZPqbeNC+wYHov+LyJh66FvefKT1hjphy6Qmz+NuW7ZkqvWCuM43UUs/kfz7X1MTEvP/TGpnZf8XlWv3Q9iYeupZlq74uPWGO0+PLS0+YZcXxdu3pHy+9YK5vVrXnx79jv//deW/j228gGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmUpR295q+1PbR2w/1PQoAPUtGLXtPkl/kHSLpPWS7ra9vulhAOqpcqTeJOlIRByNiNOSdkq6s9lZAOqqEvUaScdnfD3W+Xez2N5me9T26NR4C9/hDnxPVIl6vlOmzDkFRESMRMRwRAz3DQ4ufhmAWqpEPSZp7YyvhySdaGYOgMWqEvVeSdfavsb2ckl3SXq52VkA6lrwxIMRMWn7fklvSOqT9GREHGp8GYBaKp1NNCJelfRqw1sA9AB/owxIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkKr2ho1vuP6Nlq75u4qFr+fSnT5WeMMeOk8OlJ8xy8PmNpSfMsvz1vaUnzHHigZtKTzjnoskL3LZ0MwAsBaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIJkFo7b9pO1Ttj9aikEAFqfKkfqPkrY2vANAjywYdUS8LenzJdgCoAd69pza9jbbo7ZHp76c6NXDAuhSz6KOiJGIGI6I4b5LBnr1sAC6xKvfQDJEDSRT5Udaz0p6R9J1tsds39f8LAB1LXje74i4eymGAOgNvv0GkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmQXf0FFHTFmnx5c38dC17Dg5XHrCHH878ePSE2aJaxr5rVDbpVtvLD1hjsnB0gv+L/rOfxtHaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSqXKBvLW237J92PYh29uXYhiAeqq8iXZS0m8jYr/tiyXts70rIj5ueBuAGhY8UkfEyYjY3/n8K0mHJa1pehiAerp6Tm17naSNkt6b57Zttkdtj06NT/RoHoBuVY7a9qCkFyTtiIgvv3t7RIxExHBEDPcNDvRyI4AuVIra9jJNB/1MRLzY7CQAi1Hl1W9LekLS4Yh4tPlJABajypF6s6R7JW2xfaDzcWvDuwDUtOCPtCJijyQvwRYAPcDfKAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiCZKuco65q/tVYcX97EQ9dy8PmNpSfMEdc08ktfm2/9d+kJs9xw9dHSE+Y4dmBD6QnnxLI4720cqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpspVL1fYft/2QduHbD+yFMMA1FPlTb3/lbQlIsY716neY/u1iHi34W0Aaqhy1cuQNN75clnn4/zv0AZQVKXn1Lb7bB+QdErSroh4b577bLM9ant0amKixzMBVFUp6oiYiogNkoYkbbJ9/Tz3GYmI4YgY7hsY6PFMAFV19ep3RHwhabekrU2MAbB4VV79vtL2ZZ3PV0q6WdInDe8CUFOVV79XS/qT7T5N/yHwXES80uwsAHVVefX7Q0ntO8cugHnxN8qAZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpsq7tLrmKal/fOH7LZXlr+8tPWGOS7feWHrCLDdcfbT0hFkeWz1aesIcrx1ZX3rCOe47/xnFOFIDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEzlqDsXnv/ANhfHA1qsmyP1dkmHmxoCoDcqRW17SNJtkh5vdg6Axap6pH5M0oOSzpzvDra32R61PTr19UQvtgGoYcGobd8u6VRE7LvQ/SJiJCKGI2K4b+VAzwYC6E6VI/VmSXfYPiZpp6Qttp9udBWA2haMOiIejoihiFgn6S5Jb0bEPY0vA1ALP6cGkunqFMERsVvS7kaWAOgJjtRAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMl29S6uqM/3SN6uiiYeu5cQDN5WeMMfkYOkFsx07sKH0hFleO7K+9IQ5vv1sZekJ58Tk+Y/HHKmBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSKbSWy8716b+StKUpMmIGG5yFID6unk/9c8j4rPGlgDoCb79BpKpGnVI+qvtfba3zXcH29tsj9oePTMx0buFALpS9dvvzRFxwvYPJe2y/UlEvD3zDhExImlEkn4wtLY95zICvmcqHakj4kTnn6ckvSRpU5OjANS3YNS2B2xffPZzSb+U9FHTwwDUU+Xb76skvWT77P3/HBGvN7oKQG0LRh0RRyXdsARbAPQAP9ICkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGUf0/nwGtv8l6Z89eKhVktp0XjT2XFjb9kjt29SrPT+KiCvnu6GRqHvF9mibzlzKngtr2x6pfZuWYg/ffgPJEDWQTNujHik94DvYc2Ft2yO1b1Pje1r9nBpA99p+pAbQJaIGkmll1La32v7U9hHbD7Vgz5O2T9luxamRba+1/Zbtw7YP2d5eeM8K2+/bPtjZ80jJPWfZ7rP9ge1XSm+Rpi80afvvtg/YHm3sv9O259S2+yT9Q9IvJI1J2ivp7oj4uOCmn0kal/RURFxfaseMPaslrY6I/Z1zsu+T9KtSv0aePn/0QESM214maY+k7RHxbok9M3b9RtKwpEsi4vaSWzp7jkkabvpCk208Um+SdCQijkbEaUk7Jd1ZclDnEkOfl9wwU0ScjIj9nc+/knRY0pqCeyIixjtfLut8FD1a2B6SdJukx0vuKKGNUa+RdHzG12Mq+Bu27Wyvk7RR0nuFd/TZPiDplKRdEVF0j6THJD0o6UzhHTMteKHJXmhj1J7n37XrOUJL2B6U9IKkHRHxZcktETEVERskDUnaZLvY0xTbt0s6FRH7Sm04j80R8RNJt0j6dedpXc+1MeoxSWtnfD0k6UShLa3Vee76gqRnIuLF0nvOiogvJO2WtLXgjM2S7ug8h90paYvtpwvukbR0F5psY9R7JV1r+xrbyyXdJenlwptapfPC1BOSDkfEoy3Yc6Xtyzqfr5R0s6RPSu2JiIcjYigi1mn698+bEXFPqT3S0l5osnVRR8SkpPslvaHpF4Cei4hDJTfZflbSO5Kusz1m+76SezR9JLpX00egA52PWwvuWS3pLdsfavoP5V0R0YofI7XIVZL22D4o6X1Jf2nqQpOt+5EWgMVp3ZEawOIQNZAMUQPJEDWQDFEDyRA1kAxRA8n8DwFWjEFmRpvZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842cc707d9ab437ea5f018a1dd2e25e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08681820257953562\n",
      "epoch: 1, loss: 0.041834626318367055\n",
      "epoch: 2, loss: 0.030764183752753152\n",
      "epoch: 3, loss: 0.030854154510674633\n",
      "epoch: 4, loss: 0.027349185144395563\n",
      "epoch: 5, loss: 0.024184273954449418\n",
      "epoch: 6, loss: 0.023463810765264218\n",
      "epoch: 7, loss: 0.02284640542904513\n",
      "epoch: 8, loss: 0.021705359648388052\n",
      "epoch: 9, loss: 0.020957780899492212\n",
      "epoch: 10, loss: 0.02056930225577983\n",
      "epoch: 11, loss: 0.020036360574940786\n",
      "epoch: 12, loss: 0.020031432065865787\n",
      "epoch: 13, loss: 0.01968681333393826\n",
      "epoch: 14, loss: 0.019643842219174652\n",
      "epoch: 15, loss: 0.019295967401079906\n",
      "epoch: 16, loss: 0.01857926016724448\n",
      "epoch: 17, loss: 0.018049415934551557\n",
      "epoch: 18, loss: 0.01731076541284274\n",
      "epoch: 19, loss: 0.016759600901455105\n",
      "epoch: 20, loss: 0.016418606605457936\n",
      "epoch: 21, loss: 0.01600053785377297\n",
      "epoch: 22, loss: 0.015413899563708377\n",
      "epoch: 23, loss: 0.015064040220536909\n",
      "epoch: 24, loss: 0.014933740495407629\n",
      "epoch: 25, loss: 0.014023004359444506\n",
      "epoch: 26, loss: 0.013388068513629513\n",
      "epoch: 27, loss: 0.012795032733355808\n",
      "epoch: 28, loss: 0.012430469527118682\n",
      "epoch: 29, loss: 0.012056835612738974\n",
      "epoch: 30, loss: 0.011775374560784209\n",
      "epoch: 31, loss: 0.011204247916537513\n",
      "epoch: 32, loss: 0.011281309343334954\n",
      "epoch: 33, loss: 0.010890239040481657\n",
      "epoch: 34, loss: 0.010873192419011989\n",
      "epoch: 35, loss: 0.011004508272162336\n",
      "epoch: 36, loss: 0.010338663285826218\n",
      "epoch: 37, loss: 0.010703310422418328\n",
      "epoch: 38, loss: 0.009922069600720585\n",
      "epoch: 39, loss: 0.010129584870746298\n",
      "epoch: 40, loss: 0.009744998066165158\n",
      "epoch: 41, loss: 0.0099778498387657\n",
      "epoch: 42, loss: 0.00978111935187768\n",
      "epoch: 43, loss: 0.009846530868928916\n",
      "epoch: 44, loss: 0.009881740010149201\n",
      "epoch: 45, loss: 0.009512176891745727\n",
      "epoch: 46, loss: 0.009489710098990363\n",
      "epoch: 47, loss: 0.009480285807231687\n",
      "epoch: 48, loss: 0.009694738235791351\n",
      "epoch: 49, loss: 0.009540389231059298\n",
      "epoch: 50, loss: 0.009190631123471845\n",
      "epoch: 51, loss: 0.009155381470556678\n",
      "epoch: 52, loss: 0.00900402047211261\n",
      "epoch: 53, loss: 0.009004460068843005\n",
      "epoch: 54, loss: 0.008935598795460452\n",
      "epoch: 55, loss: 0.008954056419994902\n",
      "epoch: 56, loss: 0.00892342373455469\n",
      "epoch: 57, loss: 0.008913858250659223\n",
      "epoch: 58, loss: 0.008898269292504549\n",
      "epoch: 59, loss: 0.00910181352140276\n",
      "epoch: 60, loss: 0.008999420201184315\n",
      "epoch: 61, loss: 0.008173223091544403\n",
      "epoch: 62, loss: 0.008415304211718067\n",
      "epoch: 63, loss: 0.00859869058556821\n",
      "epoch: 64, loss: 0.008643336887676274\n",
      "epoch: 65, loss: 0.008397571600950973\n",
      "epoch: 66, loss: 0.00886154083458874\n",
      "epoch: 67, loss: 0.008527802055761308\n",
      "epoch: 68, loss: 0.008434054651448161\n",
      "epoch: 69, loss: 0.0083574536661612\n",
      "epoch: 70, loss: 0.008522163898861844\n",
      "epoch: 71, loss: 0.008552261788075308\n",
      "epoch: 72, loss: 0.008335250365296734\n",
      "epoch: 73, loss: 0.008237819902544206\n",
      "epoch: 74, loss: 0.008378052975256026\n",
      "epoch: 75, loss: 0.00855278720320614\n",
      "epoch: 76, loss: 0.008401217989122052\n",
      "epoch: 77, loss: 0.008566040988581285\n",
      "epoch: 78, loss: 0.008234710696719235\n",
      "epoch: 79, loss: 0.008065781558470486\n",
      "epoch: 80, loss: 0.008398321287853007\n",
      "epoch: 81, loss: 0.008381695985346138\n",
      "epoch: 82, loss: 0.008416811840261329\n",
      "epoch: 83, loss: 0.008542330955394238\n",
      "epoch: 84, loss: 0.008428577934230966\n",
      "epoch: 85, loss: 0.00815431557156853\n",
      "epoch: 86, loss: 0.008384190465848377\n",
      "epoch: 87, loss: 0.008142956784707979\n",
      "epoch: 88, loss: 0.008132232204322066\n",
      "epoch: 89, loss: 0.008533932553666891\n",
      "epoch: 90, loss: 0.008037132838312842\n",
      "epoch: 91, loss: 0.008242586726166463\n",
      "epoch: 92, loss: 0.008104804169640946\n",
      "epoch: 93, loss: 0.008155236838584953\n",
      "epoch: 94, loss: 0.008124046593779402\n",
      "epoch: 95, loss: 0.00863233122008031\n",
      "epoch: 96, loss: 0.008210337737563375\n",
      "epoch: 97, loss: 0.007919114155946662\n",
      "epoch: 98, loss: 0.008279165303947835\n",
      "epoch: 99, loss: 0.008388126694352656\n",
      "epoch: 100, loss: 0.007955345823725922\n",
      "epoch: 101, loss: 0.008320806764386901\n",
      "epoch: 102, loss: 0.00794017903224525\n",
      "epoch: 103, loss: 0.008086764358080705\n",
      "epoch: 104, loss: 0.007632673711297035\n",
      "epoch: 105, loss: 0.008184168844600952\n",
      "epoch: 106, loss: 0.0077564325751146085\n",
      "epoch: 107, loss: 0.007946753084555799\n",
      "epoch: 108, loss: 0.00798411580699582\n",
      "epoch: 109, loss: 0.008060057250550586\n",
      "epoch: 110, loss: 0.008170274056728693\n",
      "epoch: 111, loss: 0.00783861938288841\n",
      "epoch: 112, loss: 0.008544709056058232\n",
      "epoch: 113, loss: 0.008014915178779973\n",
      "epoch: 114, loss: 0.008105401241630545\n",
      "epoch: 115, loss: 0.007940866578846503\n",
      "epoch: 116, loss: 0.00799821884498699\n",
      "epoch: 117, loss: 0.008273013943596023\n",
      "epoch: 118, loss: 0.007637248976277422\n",
      "epoch: 119, loss: 0.007960920950069767\n",
      "epoch: 120, loss: 0.007898446010976043\n",
      "epoch: 121, loss: 0.007881169018619575\n",
      "epoch: 122, loss: 0.00785387251036899\n",
      "epoch: 123, loss: 0.007537283139540161\n",
      "epoch: 124, loss: 0.007467055775800574\n",
      "epoch: 125, loss: 0.00793982643776672\n",
      "epoch: 126, loss: 0.007920801881188785\n",
      "epoch: 127, loss: 0.007544177009223092\n",
      "epoch: 128, loss: 0.007635690138473191\n",
      "epoch: 129, loss: 0.008139104161296908\n",
      "epoch: 130, loss: 0.007827775270285395\n",
      "epoch: 131, loss: 0.007801902980475128\n",
      "epoch: 132, loss: 0.008305770687584466\n",
      "epoch: 133, loss: 0.0077613105939506755\n",
      "epoch: 134, loss: 0.008101354896713849\n",
      "epoch: 135, loss: 0.008197488942686636\n",
      "epoch: 136, loss: 0.007754824207605702\n",
      "epoch: 137, loss: 0.008222386567117577\n",
      "epoch: 138, loss: 0.008013337176444655\n",
      "epoch: 139, loss: 0.007881364360689506\n",
      "epoch: 140, loss: 0.008279385464849144\n",
      "epoch: 141, loss: 0.007833370116968712\n",
      "epoch: 142, loss: 0.007796550054835308\n",
      "epoch: 143, loss: 0.00778350812818122\n",
      "epoch: 144, loss: 0.007886411481829123\n",
      "epoch: 145, loss: 0.007536427328185701\n",
      "epoch: 146, loss: 0.007728356585229805\n",
      "epoch: 147, loss: 0.007919565632168564\n",
      "epoch: 148, loss: 0.007773443033008813\n",
      "epoch: 149, loss: 0.007919101917480473\n",
      "epoch: 150, loss: 0.007836931846346571\n",
      "epoch: 151, loss: 0.007979970438638004\n",
      "epoch: 152, loss: 0.007721440607229219\n",
      "epoch: 153, loss: 0.007810305221010082\n",
      "epoch: 154, loss: 0.008108142280667877\n",
      "epoch: 155, loss: 0.007878462773911134\n",
      "epoch: 156, loss: 0.007766834545135984\n",
      "epoch: 157, loss: 0.00788489908593767\n",
      "epoch: 158, loss: 0.008158786761269967\n",
      "epoch: 159, loss: 0.00743523027815548\n",
      "epoch: 160, loss: 0.008189981947919355\n",
      "epoch: 161, loss: 0.007546823176592211\n",
      "epoch: 162, loss: 0.00774553388628167\n",
      "epoch: 163, loss: 0.0078002589762793135\n",
      "epoch: 164, loss: 0.007679685994428872\n",
      "epoch: 165, loss: 0.007894771908872167\n",
      "epoch: 166, loss: 0.007364966367892863\n",
      "epoch: 167, loss: 0.007872591667071776\n",
      "epoch: 168, loss: 0.007682787280255053\n",
      "epoch: 169, loss: 0.007996183221554026\n",
      "epoch: 170, loss: 0.00762770426616222\n",
      "epoch: 171, loss: 0.00811157461871555\n",
      "epoch: 172, loss: 0.007770552382556444\n",
      "epoch: 173, loss: 0.007851799076270281\n",
      "epoch: 174, loss: 0.00739428166082335\n",
      "epoch: 175, loss: 0.007939979290409186\n",
      "epoch: 176, loss: 0.007617046358572062\n",
      "epoch: 177, loss: 0.00791822080303828\n",
      "epoch: 178, loss: 0.0071934164434319225\n",
      "epoch: 179, loss: 0.007942677602171934\n",
      "epoch: 180, loss: 0.007464398484961745\n",
      "epoch: 181, loss: 0.007682890775814112\n",
      "epoch: 182, loss: 0.007596259199864506\n",
      "epoch: 183, loss: 0.007584662141103258\n",
      "epoch: 184, loss: 0.007738344832859921\n",
      "epoch: 185, loss: 0.007978696261290965\n",
      "epoch: 186, loss: 0.007402014194062927\n",
      "epoch: 187, loss: 0.007589365182980614\n",
      "epoch: 188, loss: 0.007558083407334049\n",
      "epoch: 189, loss: 0.00796934385697602\n",
      "epoch: 190, loss: 0.007479529113222815\n",
      "epoch: 191, loss: 0.0073099738952745645\n",
      "epoch: 192, loss: 0.007679215516350462\n",
      "epoch: 193, loss: 0.0077679998853042035\n",
      "epoch: 194, loss: 0.0076702303195824925\n",
      "epoch: 195, loss: 0.007625510464794173\n",
      "epoch: 196, loss: 0.007587644838367625\n",
      "epoch: 197, loss: 0.007524760724248818\n",
      "epoch: 198, loss: 0.007592898844782947\n",
      "epoch: 199, loss: 0.007584096574887293\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qnn_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1a5e7cc86b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainability_qnn_3D_deep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qnn_list' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(q_bits = [3, 4, 4],\n",
    "                     dim = [3, 4, 4, 1],\n",
    "                     reps = 2,\n",
    "                     backend=backend,\n",
    "                     shots=10000,\n",
    "                     lr = 0.1)\n",
    "\n",
    "qnn.train(x, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891de399692d4451b70785bd7a430e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.23040794405658338\n",
      "epoch: 1, loss: 0.19962964151939672\n",
      "epoch: 2, loss: 0.15928580471839227\n",
      "epoch: 3, loss: 0.11579217207718344\n",
      "epoch: 4, loss: 0.07617230656924423\n",
      "epoch: 5, loss: 0.04596884288025058\n",
      "epoch: 6, loss: 0.028546550835629465\n",
      "epoch: 7, loss: 0.023986378544234903\n",
      "epoch: 8, loss: 0.028684664563934967\n",
      "epoch: 9, loss: 0.03567006261292695\n",
      "epoch: 10, loss: 0.040136820284602116\n",
      "epoch: 11, loss: 0.041270860187530393\n",
      "epoch: 12, loss: 0.03989768065738124\n",
      "epoch: 13, loss: 0.03708761330027705\n",
      "epoch: 14, loss: 0.033695331292575716\n",
      "epoch: 15, loss: 0.03031970932452126\n",
      "epoch: 16, loss: 0.02737750144981317\n",
      "epoch: 17, loss: 0.025151397072764477\n",
      "epoch: 18, loss: 0.023791756392020958\n",
      "epoch: 19, loss: 0.023302188197625022\n",
      "epoch: 20, loss: 0.023541995667862126\n",
      "epoch: 21, loss: 0.024261577267008366\n",
      "epoch: 22, loss: 0.025165116388688995\n",
      "epoch: 23, loss: 0.025979522021630234\n",
      "epoch: 24, loss: 0.026507358737888564\n",
      "epoch: 25, loss: 0.02665214452536708\n",
      "epoch: 26, loss: 0.02641698606915403\n",
      "epoch: 27, loss: 0.025884439728489597\n",
      "epoch: 28, loss: 0.025186256897901386\n",
      "epoch: 29, loss: 0.024469804166565808\n",
      "epoch: 30, loss: 0.023866378540344856\n",
      "epoch: 31, loss: 0.023466078256501217\n",
      "epoch: 32, loss: 0.02330345491008444\n",
      "epoch: 33, loss: 0.0233566488637353\n",
      "epoch: 34, loss: 0.023559679083125968\n",
      "epoch: 35, loss: 0.023823919573106724\n",
      "epoch: 36, loss: 0.024062270243092473\n",
      "epoch: 37, loss: 0.02420939410545334\n",
      "epoch: 38, loss: 0.024233513564684603\n",
      "epoch: 39, loss: 0.024138349254955994\n",
      "epoch: 40, loss: 0.023956481342262536\n",
      "epoch: 41, loss: 0.02373706657137627\n",
      "epoch: 42, loss: 0.0235314973812957\n",
      "epoch: 43, loss: 0.023380507614193104\n",
      "epoch: 44, loss: 0.023305563651383362\n",
      "epoch: 45, loss: 0.023306177300874434\n",
      "epoch: 46, loss: 0.023363187955392176\n",
      "epoch: 47, loss: 0.023446472125751947\n",
      "epoch: 48, loss: 0.02352446180906155\n",
      "epoch: 49, loss: 0.02357265120209517\n",
      "epoch: 50, loss: 0.023578939530458375\n",
      "epoch: 51, loss: 0.023544852245380055\n",
      "epoch: 52, loss: 0.02348294110505801\n",
      "epoch: 53, loss: 0.023411623970800852\n",
      "epoch: 54, loss: 0.023349209005107667\n",
      "epoch: 55, loss: 0.023308826928432717\n",
      "epoch: 56, loss: 0.023295537618862324\n",
      "epoch: 57, loss: 0.023306129448409582\n",
      "epoch: 58, loss: 0.023331306661801273\n",
      "epoch: 59, loss: 0.023359305233143515\n",
      "epoch: 60, loss: 0.023379680730404907\n",
      "epoch: 61, loss: 0.023386132029319426\n",
      "epoch: 62, loss: 0.02337767293688534\n",
      "epoch: 63, loss: 0.023358053651434577\n",
      "epoch: 64, loss: 0.023333865545956792\n",
      "epoch: 65, loss: 0.02331208928810792\n",
      "epoch: 66, loss: 0.023297904008358912\n",
      "epoch: 67, loss: 0.023293383222342565\n",
      "epoch: 68, loss: 0.023297347467156327\n",
      "epoch: 69, loss: 0.02330624957195276\n",
      "epoch: 70, loss: 0.023315663035015005\n",
      "epoch: 71, loss: 0.02332181449797631\n",
      "epoch: 72, loss: 0.023322668057680334\n",
      "epoch: 73, loss: 0.02331828440385287\n",
      "epoch: 74, loss: 0.023310450926048606\n",
      "epoch: 75, loss: 0.023301813667667877\n",
      "epoch: 76, loss: 0.02329486860933883\n",
      "epoch: 77, loss: 0.023291162847632236\n",
      "epoch: 78, loss: 0.023290936138358396\n",
      "epoch: 79, loss: 0.023293254068790046\n",
      "epoch: 80, loss: 0.023296512782355356\n",
      "epoch: 81, loss: 0.023299088170858132\n",
      "epoch: 82, loss: 0.02329988750563436\n",
      "epoch: 83, loss: 0.023298631088687134\n",
      "epoch: 84, loss: 0.023295809893654117\n",
      "epoch: 85, loss: 0.02329238465796635\n",
      "epoch: 86, loss: 0.02328937117638856\n",
      "epoch: 87, loss: 0.023287474017425598\n",
      "epoch: 88, loss: 0.02328688926921969\n",
      "epoch: 89, loss: 0.02328731910424968\n",
      "epoch: 90, loss: 0.02328815944661393\n",
      "epoch: 91, loss: 0.02328876631654137\n",
      "epoch: 92, loss: 0.023288692792673565\n",
      "epoch: 93, loss: 0.023287815836389777\n",
      "epoch: 94, loss: 0.023286324632721827\n",
      "epoch: 95, loss: 0.023284596897644862\n",
      "epoch: 96, loss: 0.0232830264446123\n",
      "epoch: 97, loss: 0.02328187333934301\n",
      "epoch: 98, loss: 0.02328118847366521\n",
      "epoch: 99, loss: 0.02328082861459568\n",
      "epoch: 100, loss: 0.02328054165035663\n",
      "epoch: 101, loss: 0.023280078564871725\n",
      "epoch: 102, loss: 0.02327928542929448\n",
      "epoch: 103, loss: 0.02327814366906649\n",
      "epoch: 104, loss: 0.023276751625342988\n",
      "epoch: 105, loss: 0.023275264038586777\n",
      "epoch: 106, loss: 0.02327381964053329\n",
      "epoch: 107, loss: 0.02327248682884624\n",
      "epoch: 108, loss: 0.023271245575525415\n",
      "epoch: 109, loss: 0.02327000664773263\n",
      "epoch: 110, loss: 0.02326865438124463\n",
      "epoch: 111, loss: 0.023267092146030774\n",
      "epoch: 112, loss: 0.02326527191218206\n",
      "epoch: 113, loss: 0.023263198630146045\n",
      "epoch: 114, loss: 0.023260911689369012\n",
      "epoch: 115, loss: 0.023258454538707923\n",
      "epoch: 116, loss: 0.023255846508999853\n",
      "epoch: 117, loss: 0.02325306767955229\n",
      "epoch: 118, loss: 0.02325006052458985\n",
      "epoch: 119, loss: 0.02324674456655349\n",
      "epoch: 120, loss: 0.02324303549286653\n",
      "epoch: 121, loss: 0.023238859703959554\n",
      "epoch: 122, loss: 0.023234158594664198\n",
      "epoch: 123, loss: 0.02322888205540936\n",
      "epoch: 124, loss: 0.02322297527288199\n",
      "epoch: 125, loss: 0.023216365093572805\n",
      "epoch: 126, loss: 0.02320895146292409\n",
      "epoch: 127, loss: 0.02320060656379823\n",
      "epoch: 128, loss: 0.023191180832166997\n",
      "epoch: 129, loss: 0.023180512599467504\n",
      "epoch: 130, loss: 0.023168437591416357\n",
      "epoch: 131, loss: 0.02315479582526843\n",
      "epoch: 132, loss: 0.023139435718565166\n",
      "epoch: 133, loss: 0.023122217275023508\n",
      "epoch: 134, loss: 0.023103017117923924\n",
      "epoch: 135, loss: 0.023081737577129773\n",
      "epoch: 136, loss: 0.02305832032349615\n",
      "epoch: 137, loss: 0.02303276288616147\n",
      "epoch: 138, loss: 0.023005134492897632\n",
      "epoch: 139, loss: 0.022975586496247404\n",
      "epoch: 140, loss: 0.022944352382011955\n",
      "epoch: 141, loss: 0.022911733121099436\n",
      "epoch: 142, loss: 0.022878065623159777\n",
      "epoch: 143, loss: 0.022843675474186028\n",
      "epoch: 144, loss: 0.022808819799484778\n",
      "epoch: 145, loss: 0.02277363095850738\n",
      "epoch: 146, loss: 0.022738074864844544\n",
      "epoch: 147, loss: 0.022701936807324102\n",
      "epoch: 148, loss: 0.02266484181787734\n",
      "epoch: 149, loss: 0.022626307633313778\n",
      "epoch: 150, loss: 0.022585820105535358\n",
      "epoch: 151, loss: 0.02254291738070913\n",
      "epoch: 152, loss: 0.022497271516829877\n",
      "epoch: 153, loss: 0.022448762220246634\n",
      "epoch: 154, loss: 0.022397542411339885\n",
      "epoch: 155, loss: 0.022344094097979057\n",
      "epoch: 156, loss: 0.02228926159092445\n",
      "epoch: 157, loss: 0.022234227947218743\n",
      "epoch: 158, loss: 0.022180380673445306\n",
      "epoch: 159, loss: 0.022129022180694343\n",
      "epoch: 160, loss: 0.0220809562236262\n",
      "epoch: 161, loss: 0.022036122308104786\n",
      "epoch: 162, loss: 0.02199354638586213\n",
      "epoch: 163, loss: 0.021951744358066884\n",
      "epoch: 164, loss: 0.0219093468432555\n",
      "epoch: 165, loss: 0.021865468478265513\n",
      "epoch: 166, loss: 0.021819589740270543\n",
      "epoch: 167, loss: 0.021771240623940508\n",
      "epoch: 168, loss: 0.021719956923524013\n",
      "epoch: 169, loss: 0.021665613635317292\n",
      "epoch: 170, loss: 0.021608774544475212\n",
      "epoch: 171, loss: 0.021550642595858985\n",
      "epoch: 172, loss: 0.021492566029582515\n",
      "epoch: 173, loss: 0.021435449431184007\n",
      "epoch: 174, loss: 0.021379481198567047\n",
      "epoch: 175, loss: 0.02132431181681105\n",
      "epoch: 176, loss: 0.02126947506390151\n",
      "epoch: 177, loss: 0.021214713988833043\n",
      "epoch: 178, loss: 0.021160014965076274\n",
      "epoch: 179, loss: 0.02110540790989799\n",
      "epoch: 180, loss: 0.021050750173223522\n",
      "epoch: 181, loss: 0.020995679440457167\n",
      "epoch: 182, loss: 0.020939754829801396\n",
      "epoch: 183, loss: 0.020882647137663548\n",
      "epoch: 184, loss: 0.02082420750502083\n",
      "epoch: 185, loss: 0.02076435349956009\n",
      "epoch: 186, loss: 0.02070286536068202\n",
      "epoch: 187, loss: 0.020639249883225096\n",
      "epoch: 188, loss: 0.02057275101347507\n",
      "epoch: 189, loss: 0.02050244361701729\n",
      "epoch: 190, loss: 0.020427280372019817\n",
      "epoch: 191, loss: 0.020346035128510773\n",
      "epoch: 192, loss: 0.020257227398480213\n",
      "epoch: 193, loss: 0.020159197365757168\n",
      "epoch: 194, loss: 0.02005047537162848\n",
      "epoch: 195, loss: 0.019930495116686854\n",
      "epoch: 196, loss: 0.019800581619299776\n",
      "epoch: 197, loss: 0.019664968462242676\n",
      "epoch: 198, loss: 0.01953129529884596\n",
      "epoch: 199, loss: 0.0194097257751798\n",
      "epoch: 200, loss: 0.01931007219042956\n",
      "epoch: 201, loss: 0.01923778831892584\n",
      "epoch: 202, loss: 0.01919154367605441\n",
      "epoch: 203, loss: 0.019164438345114193\n",
      "epoch: 204, loss: 0.019147503156370888\n",
      "epoch: 205, loss: 0.019132665700772523\n",
      "epoch: 206, loss: 0.019114388584940776\n",
      "epoch: 207, loss: 0.019090576007604526\n",
      "epoch: 208, loss: 0.019062714296400607\n",
      "epoch: 209, loss: 0.01903472957808788\n",
      "epoch: 210, loss: 0.019010595491674716\n",
      "epoch: 211, loss: 0.018991846159760756\n",
      "epoch: 212, loss: 0.018976872179512386\n",
      "epoch: 213, loss: 0.01896262356032024\n",
      "epoch: 214, loss: 0.018946867702983653\n",
      "epoch: 215, loss: 0.018928893483593827\n",
      "epoch: 216, loss: 0.01890871943535371\n",
      "epoch: 217, loss: 0.018886317554175785\n",
      "epoch: 218, loss: 0.018861814482856974\n",
      "epoch: 219, loss: 0.01883620258092718\n",
      "epoch: 220, loss: 0.01881143013660596\n",
      "epoch: 221, loss: 0.01878946523914618\n",
      "epoch: 222, loss: 0.018771089911028344\n",
      "epoch: 223, loss: 0.01875548772964368\n",
      "epoch: 224, loss: 0.0187409246161345\n",
      "epoch: 225, loss: 0.018725865151294548\n",
      "epoch: 226, loss: 0.018709631364461916\n",
      "epoch: 227, loss: 0.018692276141141582\n",
      "epoch: 228, loss: 0.01867405483272957\n",
      "epoch: 229, loss: 0.018655091334490018\n",
      "epoch: 230, loss: 0.0186354661853331\n",
      "epoch: 231, loss: 0.018615457456956114\n",
      "epoch: 232, loss: 0.018595546793325396\n",
      "epoch: 233, loss: 0.018576128628381035\n",
      "epoch: 234, loss: 0.018557231824688356\n",
      "epoch: 235, loss: 0.01853856771129446\n",
      "epoch: 236, loss: 0.018519862486577204\n",
      "epoch: 237, loss: 0.018501119504896343\n",
      "epoch: 238, loss: 0.018482543023808876\n",
      "epoch: 239, loss: 0.01846423483794665\n",
      "epoch: 240, loss: 0.018446006757100095\n",
      "epoch: 241, loss: 0.018427490272471772\n",
      "epoch: 242, loss: 0.01840839254186786\n",
      "epoch: 243, loss: 0.018388625770738184\n",
      "epoch: 244, loss: 0.018368219417376735\n",
      "epoch: 245, loss: 0.018347165341189867\n",
      "epoch: 246, loss: 0.018325378740849996\n",
      "epoch: 247, loss: 0.01830278447251804\n",
      "epoch: 248, loss: 0.018279388691102294\n",
      "epoch: 249, loss: 0.018255238188780526\n",
      "epoch: 250, loss: 0.018230323257428814\n",
      "epoch: 251, loss: 0.018204543467843906\n",
      "epoch: 252, loss: 0.01817776773035461\n",
      "epoch: 253, loss: 0.0181499087360724\n",
      "epoch: 254, loss: 0.01812092979363775\n",
      "epoch: 255, loss: 0.01809079185414277\n",
      "epoch: 256, loss: 0.018059413273650148\n",
      "epoch: 257, loss: 0.018026688198748683\n",
      "epoch: 258, loss: 0.017992537436335946\n",
      "epoch: 259, loss: 0.017956934460083014\n",
      "epoch: 260, loss: 0.017919884525419923\n",
      "epoch: 261, loss: 0.01788138589183085\n",
      "epoch: 262, loss: 0.017841410998646355\n",
      "epoch: 263, loss: 0.01779991151895746\n",
      "epoch: 264, loss: 0.017756821902646467\n",
      "epoch: 265, loss: 0.01771204662813675\n",
      "epoch: 266, loss: 0.017665452280596808\n",
      "epoch: 267, loss: 0.01761690283963053\n",
      "epoch: 268, loss: 0.017566349719881115\n",
      "epoch: 269, loss: 0.01751393357721034\n",
      "epoch: 270, loss: 0.017460020249221365\n",
      "epoch: 271, loss: 0.01740512757820918\n",
      "epoch: 272, loss: 0.017349794537728027\n",
      "epoch: 273, loss: 0.01729449764336494\n",
      "epoch: 274, loss: 0.017239657862460296\n",
      "epoch: 275, loss: 0.017185687233939543\n",
      "epoch: 276, loss: 0.017133014191418813\n",
      "epoch: 277, loss: 0.01708207866365333\n",
      "epoch: 278, loss: 0.017033313967892136\n",
      "epoch: 279, loss: 0.01698711746050118\n",
      "epoch: 280, loss: 0.01694380191219438\n",
      "epoch: 281, loss: 0.01690353604166841\n",
      "epoch: 282, loss: 0.016866302468731276\n",
      "epoch: 283, loss: 0.016831897728491068\n",
      "epoch: 284, loss: 0.01679997449218733\n",
      "epoch: 285, loss: 0.01677010491345383\n",
      "epoch: 286, loss: 0.01674184302301924\n",
      "epoch: 287, loss: 0.01671477747031065\n",
      "epoch: 288, loss: 0.016688575260682882\n",
      "epoch: 289, loss: 0.01666301388610882\n",
      "epoch: 290, loss: 0.01663799302101336\n",
      "epoch: 291, loss: 0.01661352085907133\n",
      "epoch: 292, loss: 0.01658968308800325\n",
      "epoch: 293, loss: 0.016566610757298286\n",
      "epoch: 294, loss: 0.01654445769923041\n",
      "epoch: 295, loss: 0.016523384731447956\n",
      "epoch: 296, loss: 0.016503540645313845\n",
      "epoch: 297, loss: 0.016485035695202217\n",
      "epoch: 298, loss: 0.016467914996877372\n",
      "epoch: 299, loss: 0.016452144592242528\n",
      "epoch: 300, loss: 0.01643761714246083\n",
      "epoch: 301, loss: 0.016424173674607356\n",
      "epoch: 302, loss: 0.01641163185856647\n",
      "epoch: 303, loss: 0.016399812558028234\n",
      "epoch: 304, loss: 0.016388560479719722\n",
      "epoch: 305, loss: 0.016377757190846293\n",
      "epoch: 306, loss: 0.016367325513271462\n",
      "epoch: 307, loss: 0.016357225520941276\n",
      "epoch: 308, loss: 0.016347444626453555\n",
      "epoch: 309, loss: 0.016337985884548514\n",
      "epoch: 310, loss: 0.016328858134095677\n",
      "epoch: 311, loss: 0.0163200693970322\n",
      "epoch: 312, loss: 0.016311622912517705\n",
      "epoch: 313, loss: 0.01630351464689517\n",
      "epoch: 314, loss: 0.016295731821298\n",
      "epoch: 315, loss: 0.016288252723960717\n",
      "epoch: 316, loss: 0.01628104803301125\n",
      "epoch: 317, loss: 0.01627408330524304\n",
      "epoch: 318, loss: 0.016267321897198747\n",
      "epoch: 319, loss: 0.016260727703047644\n",
      "epoch: 320, loss: 0.016254267446860402\n",
      "epoch: 321, loss: 0.016247912453855415\n",
      "epoch: 322, loss: 0.01624163978582386\n",
      "epoch: 323, loss: 0.016235432600086374\n",
      "epoch: 324, loss: 0.016229279741793144\n",
      "epoch: 325, loss: 0.016223174790751742\n",
      "epoch: 326, loss: 0.016217114841742777\n",
      "epoch: 327, loss: 0.016211099171334956\n",
      "epoch: 328, loss: 0.016205127820264578\n",
      "epoch: 329, loss: 0.016199200165872174\n",
      "epoch: 330, loss: 0.016193313721956353\n",
      "epoch: 331, loss: 0.016187463475533834\n",
      "epoch: 332, loss: 0.016181641922874985\n",
      "epoch: 333, loss: 0.016175839687743495\n",
      "epoch: 334, loss: 0.016170046398244674\n",
      "epoch: 335, loss: 0.016164251488024257\n",
      "epoch: 336, loss: 0.01615844472352968\n",
      "epoch: 337, loss: 0.016152616405711057\n",
      "epoch: 338, loss: 0.01614675726980194\n",
      "epoch: 339, loss: 0.016140858131687866\n",
      "epoch: 340, loss: 0.016134909364054997\n",
      "epoch: 341, loss: 0.0161289003471634\n",
      "epoch: 342, loss: 0.016122819089564238\n",
      "epoch: 343, loss: 0.016116652211428648\n",
      "epoch: 344, loss: 0.01611038542570163\n",
      "epoch: 345, loss: 0.016104004569934043\n",
      "epoch: 346, loss: 0.016097497157009602\n",
      "epoch: 347, loss: 0.016090854322412595\n",
      "epoch: 348, loss: 0.01608407293720872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 349, loss: 0.0160771575389952\n",
      "epoch: 350, loss: 0.016070121649598448\n",
      "epoch: 351, loss: 0.016062988057065755\n",
      "epoch: 352, loss: 0.016055787781896556\n",
      "epoch: 353, loss: 0.016048557707712602\n",
      "epoch: 354, loss: 0.016041337148529074\n",
      "epoch: 355, loss: 0.01603416384375305\n",
      "epoch: 356, loss: 0.01602706998354917\n",
      "epoch: 357, loss: 0.01602007891683986\n",
      "epoch: 358, loss: 0.016013203191047626\n",
      "epoch: 359, loss: 0.01600644441050161\n",
      "epoch: 360, loss: 0.015999794989285165\n",
      "epoch: 361, loss: 0.01599324133759306\n",
      "epoch: 362, loss: 0.01598676764561719\n",
      "epoch: 363, loss: 0.01598035939021308\n",
      "epoch: 364, loss: 0.01597400590711001\n",
      "epoch: 365, loss: 0.01596770166355657\n",
      "epoch: 366, loss: 0.015961446151298898\n",
      "epoch: 367, loss: 0.015955242616795144\n",
      "epoch: 368, loss: 0.015949096137860937\n",
      "epoch: 369, loss: 0.015943011727479296\n",
      "epoch: 370, loss: 0.01593699306737645\n",
      "epoch: 371, loss: 0.015931042138797324\n",
      "epoch: 372, loss: 0.015925159583846944\n",
      "epoch: 373, loss: 0.015919345330402276\n",
      "epoch: 374, loss: 0.01591359899994546\n",
      "epoch: 375, loss: 0.015907919865250978\n",
      "epoch: 376, loss: 0.015902306456006557\n",
      "epoch: 377, loss: 0.015896756120657555\n",
      "epoch: 378, loss: 0.015891264841150197\n",
      "epoch: 379, loss: 0.015885827413579887\n",
      "epoch: 380, loss: 0.01588043789434386\n",
      "epoch: 381, loss: 0.015875090094155574\n",
      "epoch: 382, loss: 0.015869777915810665\n",
      "epoch: 383, loss: 0.015864495426584482\n",
      "epoch: 384, loss: 0.015859236663803736\n",
      "epoch: 385, loss: 0.015853995256583233\n",
      "epoch: 386, loss: 0.0158487640025552\n",
      "epoch: 387, loss: 0.015843534557315492\n",
      "epoch: 388, loss: 0.015838297355504946\n",
      "epoch: 389, loss: 0.015833041783158967\n",
      "epoch: 390, loss: 0.0158277565042574\n",
      "epoch: 391, loss: 0.01582242977958208\n",
      "epoch: 392, loss: 0.015817049643990482\n",
      "epoch: 393, loss: 0.01581160390836802\n",
      "epoch: 394, loss: 0.0158060800628802\n",
      "epoch: 395, loss: 0.015800465224017464\n",
      "epoch: 396, loss: 0.015794746272019617\n",
      "epoch: 397, loss: 0.015788910283139067\n",
      "epoch: 398, loss: 0.015782945300365774\n",
      "epoch: 399, loss: 0.015776841429460436\n",
      "epoch: 400, loss: 0.015770592203426154\n",
      "epoch: 401, loss: 0.01576419611816362\n",
      "epoch: 402, loss: 0.015757658178479763\n",
      "epoch: 403, loss: 0.01575099117977071\n",
      "epoch: 404, loss: 0.01574421629129636\n",
      "epoch: 405, loss: 0.015737362375245815\n",
      "epoch: 406, loss: 0.015730463523951858\n",
      "epoch: 407, loss: 0.01572355469672638\n",
      "epoch: 408, loss: 0.015716666125334365\n",
      "epoch: 409, loss: 0.015709818059565485\n",
      "epoch: 410, loss: 0.0157030178457893\n",
      "epoch: 411, loss: 0.015696260697400533\n",
      "epoch: 412, loss: 0.01568953385870124\n",
      "epoch: 413, loss: 0.015682822094954293\n",
      "epoch: 414, loss: 0.01567611180849697\n",
      "epoch: 415, loss: 0.01566939211098525\n",
      "epoch: 416, loss: 0.015662653146089812\n",
      "epoch: 417, loss: 0.0156558834596284\n",
      "epoch: 418, loss: 0.015649068334490045\n",
      "epoch: 419, loss: 0.015642189921101998\n",
      "epoch: 420, loss: 0.015635228645581613\n",
      "epoch: 421, loss: 0.01562816467463577\n",
      "epoch: 422, loss: 0.015620978451385614\n",
      "epoch: 423, loss: 0.015613650135440863\n",
      "epoch: 424, loss: 0.015606158543080983\n",
      "epoch: 425, loss: 0.015598480397777894\n",
      "epoch: 426, loss: 0.015590590299613597\n",
      "epoch: 427, loss: 0.015582461159585713\n",
      "epoch: 428, loss: 0.015574064430747226\n",
      "epoch: 429, loss: 0.01556536959361594\n",
      "epoch: 430, loss: 0.01555634288390916\n",
      "epoch: 431, loss: 0.015546945752366188\n",
      "epoch: 432, loss: 0.0155371336398062\n",
      "epoch: 433, loss: 0.015526855299873809\n",
      "epoch: 434, loss: 0.015516052418541218\n",
      "epoch: 435, loss: 0.015504659063699692\n",
      "epoch: 436, loss: 0.015492600734218254\n",
      "epoch: 437, loss: 0.015479793309866319\n",
      "epoch: 438, loss: 0.015466142674060726\n",
      "epoch: 439, loss: 0.01545154591256472\n",
      "epoch: 440, loss: 0.015435894788564865\n",
      "epoch: 441, loss: 0.0154190819268607\n",
      "epoch: 442, loss: 0.015401010117067305\n",
      "epoch: 443, loss: 0.015381605450713749\n",
      "epoch: 444, loss: 0.015360835365959002\n",
      "epoch: 445, loss: 0.015338732545794974\n",
      "epoch: 446, loss: 0.015315424423201955\n",
      "epoch: 447, loss: 0.015291165397692071\n",
      "epoch: 448, loss: 0.015266364763769906\n",
      "epoch: 449, loss: 0.015241598557812189\n",
      "epoch: 450, loss: 0.015217590179548088\n",
      "epoch: 451, loss: 0.015195146532234818\n",
      "epoch: 452, loss: 0.01517504759509962\n",
      "epoch: 453, loss: 0.015157907943364678\n",
      "epoch: 454, loss: 0.015144050372697514\n",
      "epoch: 455, loss: 0.015133438880083016\n",
      "epoch: 456, loss: 0.015125699615894113\n",
      "epoch: 457, loss: 0.015120220812291732\n",
      "epoch: 458, loss: 0.015116289544395856\n",
      "epoch: 459, loss: 0.015113215516951488\n",
      "epoch: 460, loss: 0.015110411079692411\n",
      "epoch: 461, loss: 0.015107425589916148\n",
      "epoch: 462, loss: 0.01510395188463152\n",
      "epoch: 463, loss: 0.015099824193807414\n",
      "epoch: 464, loss: 0.015095014757361544\n",
      "epoch: 465, loss: 0.01508962316994744\n",
      "epoch: 466, loss: 0.015083848991385842\n",
      "epoch: 467, loss: 0.015077945928676416\n",
      "epoch: 468, loss: 0.015072167266803617\n",
      "epoch: 469, loss: 0.015066718067183689\n",
      "epoch: 470, loss: 0.015061726607360128\n",
      "epoch: 471, loss: 0.015057238929101262\n",
      "epoch: 472, loss: 0.015053232009851687\n",
      "epoch: 473, loss: 0.01504963686431006\n",
      "epoch: 474, loss: 0.015046363071101685\n",
      "epoch: 475, loss: 0.01504331864582117\n",
      "epoch: 476, loss: 0.015040422004648012\n",
      "epoch: 477, loss: 0.01503760567492934\n",
      "epoch: 478, loss: 0.015034814198397403\n",
      "epoch: 479, loss: 0.015032000191750704\n",
      "epoch: 480, loss: 0.015029122010281348\n",
      "epoch: 481, loss: 0.015026144595835652\n",
      "epoch: 482, loss: 0.015023043042797182\n",
      "epoch: 483, loss: 0.015019806749833975\n",
      "epoch: 484, loss: 0.01501644118356349\n",
      "epoch: 485, loss: 0.015012965076189703\n",
      "epoch: 486, loss: 0.015009403413176454\n",
      "epoch: 487, loss: 0.015005779151700495\n",
      "epoch: 488, loss: 0.01500210712154119\n",
      "epoch: 489, loss: 0.014998391837250314\n",
      "epoch: 490, loss: 0.014994628775557644\n",
      "epoch: 491, loss: 0.014990807547042243\n",
      "epoch: 492, loss: 0.014986915375386226\n",
      "epoch: 493, loss: 0.014982939802139042\n",
      "epoch: 494, loss: 0.014978870203694553\n",
      "epoch: 495, loss: 0.014974698343696453\n",
      "epoch: 496, loss: 0.014970418511307012\n",
      "epoch: 497, loss: 0.014966027684885799\n",
      "epoch: 498, loss: 0.014961525799816485\n",
      "epoch: 499, loss: 0.01495691585016128\n",
      "epoch: 500, loss: 0.014952203388790518\n",
      "epoch: 501, loss: 0.014947395168978509\n",
      "epoch: 502, loss: 0.014942497243290172\n",
      "epoch: 503, loss: 0.01493751344596052\n",
      "epoch: 504, loss: 0.01493244515451274\n",
      "epoch: 505, loss: 0.014927292327378694\n",
      "epoch: 506, loss: 0.014922054818331325\n",
      "epoch: 507, loss: 0.014916732926137755\n",
      "epoch: 508, loss: 0.014911327047483094\n",
      "epoch: 509, loss: 0.014905837040778944\n",
      "epoch: 510, loss: 0.014900261747759155\n",
      "epoch: 511, loss: 0.014894598618502279\n",
      "epoch: 512, loss: 0.014888843352698319\n",
      "epoch: 513, loss: 0.014882989750028844\n",
      "epoch: 514, loss: 0.01487702991475172\n",
      "epoch: 515, loss: 0.014870954603221041\n",
      "epoch: 516, loss: 0.01486475339319673\n",
      "epoch: 517, loss: 0.014858414633153487\n",
      "epoch: 518, loss: 0.01485192538449804\n",
      "epoch: 519, loss: 0.014845271506350703\n",
      "epoch: 520, loss: 0.014838437813007685\n",
      "epoch: 521, loss: 0.014831408145587506\n",
      "epoch: 522, loss: 0.01482416530021815\n",
      "epoch: 523, loss: 0.014816690870572137\n",
      "epoch: 524, loss: 0.014808965033154965\n",
      "epoch: 525, loss: 0.01480096620106506\n",
      "epoch: 526, loss: 0.014792670499355362\n",
      "epoch: 527, loss: 0.014784051181750656\n",
      "epoch: 528, loss: 0.014775078173825833\n",
      "epoch: 529, loss: 0.014765717766326373\n",
      "epoch: 530, loss: 0.014755932292491614\n",
      "epoch: 531, loss: 0.014745679626792421\n",
      "epoch: 532, loss: 0.01473491246909772\n",
      "epoch: 533, loss: 0.014723577432465515\n",
      "epoch: 534, loss: 0.01471161391930462\n",
      "epoch: 535, loss: 0.014698952763385018\n",
      "epoch: 536, loss: 0.014685514653213116\n",
      "epoch: 537, loss: 0.014671208364597685\n",
      "epoch: 538, loss: 0.014655928802071483\n",
      "epoch: 539, loss: 0.01463955483368553\n",
      "epoch: 540, loss: 0.014621946936987951\n",
      "epoch: 541, loss: 0.014602944740838916\n",
      "epoch: 542, loss: 0.01458236461145334\n",
      "epoch: 543, loss: 0.014559997482568029\n",
      "epoch: 544, loss: 0.014535607211587136\n",
      "epoch: 545, loss: 0.014508929879081043\n",
      "epoch: 546, loss: 0.014479674559363753\n",
      "epoch: 547, loss: 0.014447526051423576\n",
      "epoch: 548, loss: 0.014412149798887929\n",
      "epoch: 549, loss: 0.014373198673504043\n",
      "epoch: 550, loss: 0.014330320332193893\n",
      "epoch: 551, loss: 0.014283162517622072\n",
      "epoch: 552, loss: 0.014231372491833124\n",
      "epoch: 553, loss: 0.014174586999565416\n",
      "epoch: 554, loss: 0.014112412367492135\n",
      "epoch: 555, loss: 0.01404440138361659\n",
      "epoch: 556, loss: 0.013970042653239156\n",
      "epoch: 557, loss: 0.01388878398577323\n",
      "epoch: 558, loss: 0.013800108100196754\n",
      "epoch: 559, loss: 0.013703664996032419\n",
      "epoch: 560, loss: 0.013599445379899436\n",
      "epoch: 561, loss: 0.013487959694460656\n",
      "epoch: 562, loss: 0.01337037044149543\n",
      "epoch: 563, loss: 0.013248515491668344\n",
      "epoch: 564, loss: 0.01312477034397671\n",
      "epoch: 565, loss: 0.013001745306267519\n",
      "epoch: 566, loss: 0.012881894707193422\n",
      "epoch: 567, loss: 0.012767184129819635\n",
      "epoch: 568, loss: 0.012658954901052613\n",
      "epoch: 569, loss: 0.012558017864159476\n",
      "epoch: 570, loss: 0.012464861912534762\n",
      "epoch: 571, loss: 0.012379793585368268\n",
      "epoch: 572, loss: 0.012302882245045124\n",
      "epoch: 573, loss: 0.01223371882591543\n",
      "epoch: 574, loss: 0.012171171530673137\n",
      "epoch: 575, loss: 0.0121134620856411\n",
      "epoch: 576, loss: 0.012058725092659796\n",
      "epoch: 577, loss: 0.012005738581895674\n",
      "epoch: 578, loss: 0.011954268870883475\n",
      "epoch: 579, loss: 0.011904786867541697\n",
      "epoch: 580, loss: 0.011857834111173589\n",
      "epoch: 581, loss: 0.011813538695500487\n",
      "epoch: 582, loss: 0.011771570204855362\n",
      "epoch: 583, loss: 0.01173142979431732\n",
      "epoch: 584, loss: 0.011692744446420898\n",
      "epoch: 585, loss: 0.011655337563680545\n",
      "epoch: 586, loss: 0.0116191133571677\n",
      "epoch: 587, loss: 0.01158391925419444\n",
      "epoch: 588, loss: 0.011549494677782879\n",
      "epoch: 589, loss: 0.011515555084342708\n",
      "epoch: 590, loss: 0.011481981519439593\n",
      "epoch: 591, loss: 0.011448943856917995\n",
      "epoch: 592, loss: 0.011416783338422989\n",
      "epoch: 593, loss: 0.011385744632107316\n",
      "epoch: 594, loss: 0.01135583589070756\n",
      "epoch: 595, loss: 0.011326918049893438\n",
      "epoch: 596, loss: 0.011298863675238599\n",
      "epoch: 597, loss: 0.01127162344839688\n",
      "epoch: 598, loss: 0.011245201507831289\n",
      "epoch: 599, loss: 0.011219609045405178\n",
      "epoch: 600, loss: 0.011194833256679141\n",
      "epoch: 601, loss: 0.011170833735644127\n",
      "epoch: 602, loss: 0.011147569686666702\n",
      "epoch: 603, loss: 0.011125034117221837\n",
      "epoch: 604, loss: 0.011103259528790054\n",
      "epoch: 605, loss: 0.011082288304749248\n",
      "epoch: 606, loss: 0.011062125266740741\n",
      "epoch: 607, loss: 0.011042706539389444\n",
      "epoch: 608, loss: 0.011023925720029993\n",
      "epoch: 609, loss: 0.011005701194238606\n",
      "epoch: 610, loss: 0.010988005127646562\n",
      "epoch: 611, loss: 0.010970830686671236\n",
      "epoch: 612, loss: 0.01095416359661555\n",
      "epoch: 613, loss: 0.010937991488597214\n",
      "epoch: 614, loss: 0.010922315582689664\n",
      "epoch: 615, loss: 0.01090714759146574\n",
      "epoch: 616, loss: 0.010892502415278175\n",
      "epoch: 617, loss: 0.01087838437004958\n",
      "epoch: 618, loss: 0.010864771456823543\n",
      "epoch: 619, loss: 0.010851614234438676\n",
      "epoch: 620, loss: 0.01083884987783036\n",
      "epoch: 621, loss: 0.010826419628081915\n",
      "epoch: 622, loss: 0.010814277758127764\n",
      "epoch: 623, loss: 0.010802385927398083\n",
      "epoch: 624, loss: 0.010790706530103241\n",
      "epoch: 625, loss: 0.01077921083529737\n",
      "epoch: 626, loss: 0.010767889424102717\n",
      "epoch: 627, loss: 0.010756748738363367\n",
      "epoch: 628, loss: 0.010745801474244662\n",
      "epoch: 629, loss: 0.01073505905512204\n",
      "epoch: 630, loss: 0.010724524946482947\n",
      "epoch: 631, loss: 0.010714192821253066\n",
      "epoch: 632, loss: 0.010704050947294805\n",
      "epoch: 633, loss: 0.010694085897147942\n",
      "epoch: 634, loss: 0.01068428229270214\n",
      "epoch: 635, loss: 0.010674620797072735\n",
      "epoch: 636, loss: 0.010665078915960403\n",
      "epoch: 637, loss: 0.010655636213132537\n",
      "epoch: 638, loss: 0.01064627681885867\n",
      "epoch: 639, loss: 0.010636985968486024\n",
      "epoch: 640, loss: 0.010627747567643496\n",
      "epoch: 641, loss: 0.01061854489501239\n",
      "epoch: 642, loss: 0.01060936105507844\n",
      "epoch: 643, loss: 0.010600179418662033\n",
      "epoch: 644, loss: 0.010590983848546918\n",
      "epoch: 645, loss: 0.010581758011600304\n",
      "epoch: 646, loss: 0.010572486060633057\n",
      "epoch: 647, loss: 0.010563155377499777\n",
      "epoch: 648, loss: 0.010553759430209498\n",
      "epoch: 649, loss: 0.010544299297158522\n",
      "epoch: 650, loss: 0.010534784249411774\n",
      "epoch: 651, loss: 0.010525232878966985\n",
      "epoch: 652, loss: 0.010515673648471975\n",
      "epoch: 653, loss: 0.010506143112512346\n",
      "epoch: 654, loss: 0.010496683591506743\n",
      "epoch: 655, loss: 0.01048734036542827\n",
      "epoch: 656, loss: 0.010478156334355117\n",
      "epoch: 657, loss: 0.010469167153175795\n",
      "epoch: 658, loss: 0.010460400422643467\n",
      "epoch: 659, loss: 0.010451876861898211\n",
      "epoch: 660, loss: 0.010443612349894861\n",
      "epoch: 661, loss: 0.01043562214716306\n",
      "epoch: 662, loss: 0.010427924149959446\n",
      "epoch: 663, loss: 0.010420537689611223\n",
      "epoch: 664, loss: 0.010413479160432206\n",
      "epoch: 665, loss: 0.010406755411307566\n",
      "epoch: 666, loss: 0.010400356548000991\n",
      "epoch: 667, loss: 0.01039425258697463\n",
      "epoch: 668, loss: 0.010388395184673459\n",
      "epoch: 669, loss: 0.010382723400589692\n",
      "epoch: 670, loss: 0.010377173405058024\n",
      "epoch: 671, loss: 0.010371689449681032\n",
      "epoch: 672, loss: 0.010366232327569799\n",
      "epoch: 673, loss: 0.010360783988179365\n",
      "epoch: 674, loss: 0.010355347362064449\n",
      "epoch: 675, loss: 0.010349941474904034\n",
      "epoch: 676, loss: 0.01034459429802475\n",
      "epoch: 677, loss: 0.010339335167140707\n",
      "epoch: 678, loss: 0.01033418807419586\n",
      "epoch: 679, loss: 0.010329167695187626\n",
      "epoch: 680, loss: 0.010324278362703863\n",
      "epoch: 681, loss: 0.01031951520447505\n",
      "epoch: 682, loss: 0.010314866933367214\n",
      "epoch: 683, loss: 0.010310318976280904\n",
      "epoch: 684, loss: 0.01030585604546074\n",
      "epoch: 685, loss: 0.010301464165905272\n",
      "epoch: 686, loss: 0.01029713180752693\n",
      "epoch: 687, loss: 0.010292850205588521\n",
      "epoch: 688, loss: 0.01028861334160606\n",
      "epoch: 689, loss: 0.01028441756933252\n",
      "epoch: 690, loss: 0.010280261107870083\n",
      "epoch: 691, loss: 0.01027614359336311\n",
      "epoch: 692, loss: 0.010272065531308841\n",
      "epoch: 693, loss: 0.010268027806025743\n",
      "epoch: 694, loss: 0.010264031287731679\n",
      "epoch: 695, loss: 0.01026007640364619\n",
      "epoch: 696, loss: 0.010256162804684343\n",
      "epoch: 697, loss: 0.010252289142146898\n",
      "epoch: 698, loss: 0.010248452942309154\n",
      "epoch: 699, loss: 0.010244650681599373\n",
      "epoch: 700, loss: 0.010240877982534227\n",
      "epoch: 701, loss: 0.010237129880579331\n",
      "epoch: 702, loss: 0.010233401173549596\n",
      "epoch: 703, loss: 0.010229686745127763\n",
      "epoch: 704, loss: 0.010225981825091608\n",
      "epoch: 705, loss: 0.01022228217143951\n",
      "epoch: 706, loss: 0.010218584121437913\n",
      "epoch: 707, loss: 0.010214884564721761\n",
      "epoch: 708, loss: 0.01021118087388021\n",
      "epoch: 709, loss: 0.010207470791426011\n",
      "epoch: 710, loss: 0.010203752316816826\n",
      "epoch: 711, loss: 0.01020002359069386\n",
      "epoch: 712, loss: 0.01019628278055238\n",
      "epoch: 713, loss: 0.010192527997917596\n",
      "epoch: 714, loss: 0.010188757231137779\n",
      "epoch: 715, loss: 0.010184968305449234\n",
      "epoch: 716, loss: 0.01018115888668437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 717, loss: 0.01017732651041019\n",
      "epoch: 718, loss: 0.010173468636502876\n",
      "epoch: 719, loss: 0.010169582712455102\n",
      "epoch: 720, loss: 0.010165666230450801\n",
      "epoch: 721, loss: 0.010161716782780526\n",
      "epoch: 722, loss: 0.010157732096631499\n",
      "epoch: 723, loss: 0.010153710042254117\n",
      "epoch: 724, loss: 0.01014964862180271\n",
      "epoch: 725, loss: 0.010145545938713677\n",
      "epoch: 726, loss: 0.010141400161686305\n",
      "epoch: 727, loss: 0.010137209490000808\n",
      "epoch: 728, loss: 0.010132972125996556\n",
      "epoch: 729, loss: 0.010128686264776518\n",
      "epoch: 730, loss: 0.010124350094371647\n",
      "epoch: 731, loss: 0.0101199618033044\n",
      "epoch: 732, loss: 0.010115519593049176\n",
      "epoch: 733, loss: 0.01011102168818013\n",
      "epoch: 734, loss: 0.010106466344135643\n",
      "epoch: 735, loss: 0.010101851851307573\n",
      "epoch: 736, loss: 0.010097176539953753\n",
      "epoch: 737, loss: 0.010092438794167735\n",
      "epoch: 738, loss: 0.010087637079250282\n",
      "epoch: 739, loss: 0.01008276999248304\n",
      "epoch: 740, loss: 0.010077836346375922\n",
      "epoch: 741, loss: 0.010072835290596537\n",
      "epoch: 742, loss: 0.010067766476923\n",
      "epoch: 743, loss: 0.01006263026192944\n",
      "epoch: 744, loss: 0.01005742793975649\n",
      "epoch: 745, loss: 0.010052161986296769\n",
      "epoch: 746, loss: 0.010046836290714957\n",
      "epoch: 747, loss: 0.010041456335703577\n",
      "epoch: 748, loss: 0.010036029293505003\n",
      "epoch: 749, loss: 0.010030563990953147\n",
      "epoch: 750, loss: 0.01002507073921578\n",
      "epoch: 751, loss: 0.010019560991365743\n",
      "epoch: 752, loss: 0.010014046901397658\n",
      "epoch: 753, loss: 0.010008540743538132\n",
      "epoch: 754, loss: 0.010003054410183801\n",
      "epoch: 755, loss: 0.009997598833786262\n",
      "epoch: 756, loss: 0.009992183871781789\n",
      "epoch: 757, loss: 0.009986818193276367\n",
      "epoch: 758, loss: 0.009981510896769817\n",
      "epoch: 759, loss: 0.00997627470358438\n",
      "epoch: 760, loss: 0.009971138508169695\n",
      "epoch: 761, loss: 0.009966180287595813\n",
      "epoch: 762, loss: 0.009961596738240897\n",
      "epoch: 763, loss: 0.009957779784314703\n",
      "epoch: 764, loss: 0.009954411345979934\n",
      "epoch: 765, loss: 0.009949543469590247\n",
      "epoch: 766, loss: 0.009941959349428296\n",
      "epoch: 767, loss: 0.009936586747998508\n",
      "epoch: 768, loss: 0.009933753199902669\n",
      "epoch: 769, loss: 0.00992861358542401\n",
      "epoch: 770, loss: 0.009922311252198859\n",
      "epoch: 771, loss: 0.00991849955542018\n",
      "epoch: 772, loss: 0.009914645201391018\n",
      "epoch: 773, loss: 0.009908936610120779\n",
      "epoch: 774, loss: 0.009904383136873803\n",
      "epoch: 775, loss: 0.009900846685970167\n",
      "epoch: 776, loss: 0.009895897263954612\n",
      "epoch: 777, loss: 0.009891065418289772\n",
      "epoch: 778, loss: 0.009887469598783937\n",
      "epoch: 779, loss: 0.009883139487221292\n",
      "epoch: 780, loss: 0.009878346656628232\n",
      "epoch: 781, loss: 0.009874573961591568\n",
      "epoch: 782, loss: 0.009870677667082548\n",
      "epoch: 783, loss: 0.00986612439143169\n",
      "epoch: 784, loss: 0.00986218184416173\n",
      "epoch: 785, loss: 0.009858532412979244\n",
      "epoch: 786, loss: 0.009854325818345786\n",
      "epoch: 787, loss: 0.009850298123186526\n",
      "epoch: 788, loss: 0.009846729319697208\n",
      "epoch: 789, loss: 0.009842882551455523\n",
      "epoch: 790, loss: 0.009838903340820463\n",
      "epoch: 791, loss: 0.009835304443015185\n",
      "epoch: 792, loss: 0.00983173534748967\n",
      "epoch: 793, loss: 0.009827936329189428\n",
      "epoch: 794, loss: 0.009824289120814551\n",
      "epoch: 795, loss: 0.009820853261735428\n",
      "epoch: 796, loss: 0.009817296551538521\n",
      "epoch: 797, loss: 0.009813680671506878\n",
      "epoch: 798, loss: 0.009810252232261164\n",
      "epoch: 799, loss: 0.009806884407864161\n",
      "epoch: 800, loss: 0.009803411162513138\n",
      "epoch: 801, loss: 0.009799972586740343\n",
      "epoch: 802, loss: 0.009796666934088804\n",
      "epoch: 803, loss: 0.009793364216274858\n",
      "epoch: 804, loss: 0.009790003975710374\n",
      "epoch: 805, loss: 0.009786692531038591\n",
      "epoch: 806, loss: 0.009783468209997535\n",
      "epoch: 807, loss: 0.00978024334454215\n",
      "epoch: 808, loss: 0.009776986594133895\n",
      "epoch: 809, loss: 0.009773765148132038\n",
      "epoch: 810, loss: 0.00977060551434988\n",
      "epoch: 811, loss: 0.00976745687580621\n",
      "epoch: 812, loss: 0.009764290474985473\n",
      "epoch: 813, loss: 0.009761139008312643\n",
      "epoch: 814, loss: 0.009758031093664228\n",
      "epoch: 815, loss: 0.009754947505152763\n",
      "epoch: 816, loss: 0.009751859438009927\n",
      "epoch: 817, loss: 0.009748771068357106\n",
      "epoch: 818, loss: 0.009745704790585785\n",
      "epoch: 819, loss: 0.00974266612948176\n",
      "epoch: 820, loss: 0.009739639745075104\n",
      "epoch: 821, loss: 0.009736612748614626\n",
      "epoch: 822, loss: 0.009733588812941847\n",
      "epoch: 823, loss: 0.009730578874863303\n",
      "epoch: 824, loss: 0.00972758645986116\n",
      "epoch: 825, loss: 0.009724605152412025\n",
      "epoch: 826, loss: 0.00972162703071641\n",
      "epoch: 827, loss: 0.009718649977881176\n",
      "epoch: 828, loss: 0.009715677393019487\n",
      "epoch: 829, loss: 0.009712713082729882\n",
      "epoch: 830, loss: 0.009709757499721534\n",
      "epoch: 831, loss: 0.009706807871247966\n",
      "epoch: 832, loss: 0.00970386064969403\n",
      "epoch: 833, loss: 0.009700913708024806\n",
      "epoch: 834, loss: 0.0096979666952731\n",
      "epoch: 835, loss: 0.009695020309023087\n",
      "epoch: 836, loss: 0.009692075126029855\n",
      "epoch: 837, loss: 0.009689131113525819\n",
      "epoch: 838, loss: 0.009686187539884353\n",
      "epoch: 839, loss: 0.00968324331852837\n",
      "epoch: 840, loss: 0.009680297365829782\n",
      "epoch: 841, loss: 0.009677348722222211\n",
      "epoch: 842, loss: 0.009674396726534253\n",
      "epoch: 843, loss: 0.00967144084687974\n",
      "epoch: 844, loss: 0.00966848078927576\n",
      "epoch: 845, loss: 0.009665516312000029\n",
      "epoch: 846, loss: 0.009662547439730296\n",
      "epoch: 847, loss: 0.009659574383245993\n",
      "epoch: 848, loss: 0.009656597953974042\n",
      "epoch: 849, loss: 0.009653619817264673\n",
      "epoch: 850, loss: 0.009650643545127445\n",
      "epoch: 851, loss: 0.00964767622171192\n",
      "epoch: 852, loss: 0.009644731986981384\n",
      "epoch: 853, loss: 0.009641838789049624\n",
      "epoch: 854, loss: 0.009639049169499524\n",
      "epoch: 855, loss: 0.00963645409749206\n",
      "epoch: 856, loss: 0.009634169101113345\n",
      "epoch: 857, loss: 0.00963221963983433\n",
      "epoch: 858, loss: 0.009630216506589616\n",
      "epoch: 859, loss: 0.009627228401621458\n",
      "epoch: 860, loss: 0.009622694831792011\n",
      "epoch: 861, loss: 0.009617781484542811\n",
      "epoch: 862, loss: 0.00961419347420971\n",
      "epoch: 863, loss: 0.009612069860565495\n",
      "epoch: 864, loss: 0.009610013212808377\n",
      "epoch: 865, loss: 0.009606732799427496\n",
      "epoch: 866, loss: 0.009602498996177032\n",
      "epoch: 867, loss: 0.00959872697410854\n",
      "epoch: 868, loss: 0.009596046469397658\n",
      "epoch: 869, loss: 0.009593660457569033\n",
      "epoch: 870, loss: 0.00959056031851532\n",
      "epoch: 871, loss: 0.009586773269252499\n",
      "epoch: 872, loss: 0.009583184524740183\n",
      "epoch: 873, loss: 0.009580272778373212\n",
      "epoch: 874, loss: 0.009577601062115741\n",
      "epoch: 875, loss: 0.009574527021839615\n",
      "epoch: 876, loss: 0.009571006478913284\n",
      "epoch: 877, loss: 0.009567549038568506\n",
      "epoch: 878, loss: 0.009564487907896846\n",
      "epoch: 879, loss: 0.009561624992448422\n",
      "epoch: 880, loss: 0.009558573171199158\n",
      "epoch: 881, loss: 0.009555232116585212\n",
      "epoch: 882, loss: 0.009551852355058484\n",
      "epoch: 883, loss: 0.009548674900338274\n",
      "epoch: 884, loss: 0.009545668455575362\n",
      "epoch: 885, loss: 0.00954262342249296\n",
      "epoch: 886, loss: 0.009539409626425657\n",
      "epoch: 887, loss: 0.00953609748957022\n",
      "epoch: 888, loss: 0.009532839857355292\n",
      "epoch: 889, loss: 0.009529700166417552\n",
      "epoch: 890, loss: 0.009526608567977069\n",
      "epoch: 891, loss: 0.009523458779516982\n",
      "epoch: 892, loss: 0.009520215629620124\n",
      "epoch: 893, loss: 0.009516929455445322\n",
      "epoch: 894, loss: 0.009513671794797719\n",
      "epoch: 895, loss: 0.009510464429598353\n",
      "epoch: 896, loss: 0.009507270888877808\n",
      "epoch: 897, loss: 0.009504038823226299\n",
      "epoch: 898, loss: 0.009500744363260049\n",
      "epoch: 899, loss: 0.009497402659915652\n",
      "epoch: 900, loss: 0.009494044311789207\n",
      "epoch: 901, loss: 0.009490684943637617\n",
      "epoch: 902, loss: 0.009487313678458039\n",
      "epoch: 903, loss: 0.00948390358879652\n",
      "epoch: 904, loss: 0.009480429519246289\n",
      "epoch: 905, loss: 0.009476880930135738\n",
      "epoch: 906, loss: 0.009473259085492554\n",
      "epoch: 907, loss: 0.009469567827788565\n",
      "epoch: 908, loss: 0.009465802701691898\n",
      "epoch: 909, loss: 0.009461948446731983\n",
      "epoch: 910, loss: 0.009457982905235855\n",
      "epoch: 911, loss: 0.009453882316278375\n",
      "epoch: 912, loss: 0.009449627798434416\n",
      "epoch: 913, loss: 0.0094452039187679\n",
      "epoch: 914, loss: 0.009440599390747028\n",
      "epoch: 915, loss: 0.009435802915282112\n",
      "epoch: 916, loss: 0.009430804136494157\n",
      "epoch: 917, loss: 0.009425595346569442\n",
      "epoch: 918, loss: 0.00942017288849777\n",
      "epoch: 919, loss: 0.009414541707064735\n",
      "epoch: 920, loss: 0.009408712987388863\n",
      "epoch: 921, loss: 0.009402707100969422\n",
      "epoch: 922, loss: 0.009396549038317848\n",
      "epoch: 923, loss: 0.00939027139183847\n",
      "epoch: 924, loss: 0.009383910428587694\n",
      "epoch: 925, loss: 0.009377514346001151\n",
      "epoch: 926, loss: 0.009371138291216864\n",
      "epoch: 927, loss: 0.009364881224300103\n",
      "epoch: 928, loss: 0.009358875143899836\n",
      "epoch: 929, loss: 0.009353466109738261\n",
      "epoch: 930, loss: 0.009348978265213134\n",
      "epoch: 931, loss: 0.009346177019612946\n",
      "epoch: 932, loss: 0.009342328141823465\n",
      "epoch: 933, loss: 0.009336212595573223\n",
      "epoch: 934, loss: 0.009326505344576796\n",
      "epoch: 935, loss: 0.009320815248491422\n",
      "epoch: 936, loss: 0.009319575718155226\n",
      "epoch: 937, loss: 0.009317213490538845\n",
      "epoch: 938, loss: 0.009312147734543478\n",
      "epoch: 939, loss: 0.009307245680711602\n",
      "epoch: 940, loss: 0.00930546141759714\n",
      "epoch: 941, loss: 0.009304623817560656\n",
      "epoch: 942, loss: 0.009301489471841548\n",
      "epoch: 943, loss: 0.009297443554645199\n",
      "epoch: 944, loss: 0.009294865936523749\n",
      "epoch: 945, loss: 0.00929339216418809\n",
      "epoch: 946, loss: 0.009290976138009032\n",
      "epoch: 947, loss: 0.009287245529250872\n",
      "epoch: 948, loss: 0.009283887000805369\n",
      "epoch: 949, loss: 0.009281598088643393\n",
      "epoch: 950, loss: 0.009279350105245747\n",
      "epoch: 951, loss: 0.00927633647709271\n",
      "epoch: 952, loss: 0.009273035397729189\n",
      "epoch: 953, loss: 0.00927024772476509\n",
      "epoch: 954, loss: 0.00926810330070361\n",
      "epoch: 955, loss: 0.009266011800554006\n",
      "epoch: 956, loss: 0.009263510976773337\n",
      "epoch: 957, loss: 0.009260853244464254\n",
      "epoch: 958, loss: 0.009258680511042774\n",
      "epoch: 959, loss: 0.009257231230308367\n",
      "epoch: 960, loss: 0.009256299870157483\n",
      "epoch: 961, loss: 0.009255303443492228\n",
      "epoch: 962, loss: 0.009254738865139264\n",
      "epoch: 963, loss: 0.009254703326158438\n",
      "epoch: 964, loss: 0.009255825257372589\n",
      "epoch: 965, loss: 0.009257303468640468\n",
      "epoch: 966, loss: 0.009258664963634151\n",
      "epoch: 967, loss: 0.009258993993396228\n",
      "epoch: 968, loss: 0.009257969385099733\n",
      "epoch: 969, loss: 0.009254316391408711\n",
      "epoch: 970, loss: 0.009249164912358053\n",
      "epoch: 971, loss: 0.009242893755392091\n",
      "epoch: 972, loss: 0.00923624442453584\n",
      "epoch: 973, loss: 0.009230229097722455\n",
      "epoch: 974, loss: 0.009225501029768648\n",
      "epoch: 975, loss: 0.009221822064302302\n",
      "epoch: 976, loss: 0.00921915168491642\n",
      "epoch: 977, loss: 0.009217442837965831\n",
      "epoch: 978, loss: 0.00921630987280443\n",
      "epoch: 979, loss: 0.009215637435901716\n",
      "epoch: 980, loss: 0.00921566173279965\n",
      "epoch: 981, loss: 0.00921659394397838\n",
      "epoch: 982, loss: 0.009218825787909484\n",
      "epoch: 983, loss: 0.009223307314473914\n",
      "epoch: 984, loss: 0.009230899426510697\n",
      "epoch: 985, loss: 0.009242658603692229\n",
      "epoch: 986, loss: 0.009258162714742847\n",
      "epoch: 987, loss: 0.009275466250752073\n",
      "epoch: 988, loss: 0.009286832025217162\n",
      "epoch: 989, loss: 0.009282549480669177\n",
      "epoch: 990, loss: 0.009257754474374137\n",
      "epoch: 991, loss: 0.009222999294037573\n",
      "epoch: 992, loss: 0.009196391187615897\n",
      "epoch: 993, loss: 0.009190237597529238\n",
      "epoch: 994, loss: 0.009200874833857411\n",
      "epoch: 995, loss: 0.009216495000258772\n",
      "epoch: 996, loss: 0.009224256059824474\n",
      "epoch: 997, loss: 0.009218069389606034\n",
      "epoch: 998, loss: 0.009201330553349433\n",
      "epoch: 999, loss: 0.009184417700075524\n"
     ]
    }
   ],
   "source": [
    "x = scaler(x, mode=\"standard\")\n",
    "\n",
    "dnn = sequential_dnn(dim = [3, 6, 5, 1], lr = 0.1)\n",
    "\n",
    "dnn.train(x, y, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qnn.loss)\n",
    "plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
