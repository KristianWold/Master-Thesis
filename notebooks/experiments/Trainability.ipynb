{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.5, 0.01)-gaussian(x, 0.2, 0.01) - gaussian(x, 0.8, 0.01) \n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_qnn, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e5aa9e7b324b5a98e5f56d5b59d7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1323c656aba841b7a8e16b5f4307639f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.15369222831778623\n",
      "epoch: 1, loss: 0.12208509365613653\n",
      "epoch: 2, loss: 0.10056557230256587\n",
      "epoch: 3, loss: 0.08793734228161769\n",
      "epoch: 4, loss: 0.07797110098134745\n",
      "epoch: 5, loss: 0.06700043841494065\n",
      "epoch: 6, loss: 0.05770790533655073\n",
      "epoch: 7, loss: 0.05141550853768334\n",
      "epoch: 8, loss: 0.04580491423423064\n",
      "epoch: 9, loss: 0.03886285432013398\n",
      "epoch: 10, loss: 0.03188263986433291\n",
      "epoch: 11, loss: 0.02614546570939757\n",
      "epoch: 12, loss: 0.02149772603759839\n",
      "epoch: 13, loss: 0.018619523666665565\n",
      "epoch: 14, loss: 0.01784537656869776\n",
      "epoch: 15, loss: 0.017564328323453823\n",
      "epoch: 16, loss: 0.017302863801686136\n",
      "epoch: 17, loss: 0.016142919266751016\n",
      "epoch: 18, loss: 0.015143859529852158\n",
      "epoch: 19, loss: 0.015749363288939916\n",
      "epoch: 20, loss: 0.017049352855453934\n",
      "epoch: 21, loss: 0.017701185492414458\n",
      "epoch: 22, loss: 0.015979539527611702\n",
      "epoch: 23, loss: 0.014823229041111465\n",
      "epoch: 24, loss: 0.014934766000597937\n",
      "epoch: 25, loss: 0.015224847241302903\n",
      "epoch: 26, loss: 0.014785884307297317\n",
      "epoch: 27, loss: 0.013719881143145677\n",
      "epoch: 28, loss: 0.012844891115044223\n",
      "epoch: 29, loss: 0.01358120835657316\n",
      "epoch: 30, loss: 0.013270883969934083\n",
      "epoch: 31, loss: 0.012850014302085863\n",
      "epoch: 32, loss: 0.012331961013609328\n",
      "epoch: 33, loss: 0.012420954768738137\n",
      "epoch: 34, loss: 0.0128232349279297\n",
      "epoch: 35, loss: 0.012759039603812075\n",
      "epoch: 36, loss: 0.01243974876034093\n",
      "epoch: 37, loss: 0.012379164836761531\n",
      "epoch: 38, loss: 0.012883256635502792\n",
      "epoch: 39, loss: 0.012952880284571533\n",
      "epoch: 40, loss: 0.01220549099674837\n",
      "epoch: 41, loss: 0.012444708493585331\n",
      "epoch: 42, loss: 0.012402463691358178\n",
      "epoch: 43, loss: 0.01229839270845596\n",
      "epoch: 44, loss: 0.012665238918705874\n",
      "epoch: 45, loss: 0.011564048331802248\n",
      "epoch: 46, loss: 0.012221853915849411\n",
      "epoch: 47, loss: 0.012089907512821494\n",
      "epoch: 48, loss: 0.011849727573642022\n",
      "epoch: 49, loss: 0.01167160635969323\n",
      "epoch: 50, loss: 0.011887305630655676\n",
      "epoch: 51, loss: 0.011531000228692792\n",
      "epoch: 52, loss: 0.011641301723026886\n",
      "epoch: 53, loss: 0.011298255411620323\n",
      "epoch: 54, loss: 0.011359476166345013\n",
      "epoch: 55, loss: 0.010672291246005619\n",
      "epoch: 56, loss: 0.010927350215690579\n",
      "epoch: 57, loss: 0.010775646539423638\n",
      "epoch: 58, loss: 0.009971467452767164\n",
      "epoch: 59, loss: 0.00992141476056728\n",
      "epoch: 60, loss: 0.009650806559016978\n",
      "epoch: 61, loss: 0.009262078704078195\n",
      "epoch: 62, loss: 0.00908840330595569\n",
      "epoch: 63, loss: 0.008887090677046811\n",
      "epoch: 64, loss: 0.00847212064042993\n",
      "epoch: 65, loss: 0.008729502906572654\n",
      "epoch: 66, loss: 0.008181459345661045\n",
      "epoch: 67, loss: 0.007976952744447883\n",
      "epoch: 68, loss: 0.007649453116140091\n",
      "epoch: 69, loss: 0.007611121620433238\n",
      "epoch: 70, loss: 0.0071600196532096835\n",
      "epoch: 71, loss: 0.006863391664158593\n",
      "epoch: 72, loss: 0.006629191811592634\n",
      "epoch: 73, loss: 0.0069216710570216165\n",
      "epoch: 74, loss: 0.006832690472700674\n",
      "epoch: 75, loss: 0.007011985548158668\n",
      "epoch: 76, loss: 0.006881017675609793\n",
      "epoch: 77, loss: 0.006728229593939107\n",
      "epoch: 78, loss: 0.006606894556272066\n",
      "epoch: 79, loss: 0.006708394533514871\n",
      "epoch: 80, loss: 0.006659962802270184\n",
      "epoch: 81, loss: 0.006524860086051655\n",
      "epoch: 82, loss: 0.006630500740404563\n",
      "epoch: 83, loss: 0.006467033321440472\n",
      "epoch: 84, loss: 0.006561725407723722\n",
      "epoch: 85, loss: 0.006633782694521569\n",
      "epoch: 86, loss: 0.006592744941694143\n",
      "epoch: 87, loss: 0.006432935157966474\n",
      "epoch: 88, loss: 0.006450420350890334\n",
      "epoch: 89, loss: 0.006219756298917329\n",
      "epoch: 90, loss: 0.006321524054741381\n",
      "epoch: 91, loss: 0.006483981732535793\n",
      "epoch: 92, loss: 0.006388190455823092\n",
      "epoch: 93, loss: 0.006417360738316396\n",
      "epoch: 94, loss: 0.0064440208470242675\n",
      "epoch: 95, loss: 0.006368349996103996\n",
      "epoch: 96, loss: 0.0064253972856135\n",
      "epoch: 97, loss: 0.00633265137162008\n",
      "epoch: 98, loss: 0.006574202778149653\n",
      "epoch: 99, loss: 0.006365736909397236\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa365eeddd24927a6665a21460d0755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f519e18d784ce2a358336cdbd3af56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1445368595146391\n",
      "epoch: 1, loss: 0.06841401516435647\n",
      "epoch: 2, loss: 0.047479166450129374\n",
      "epoch: 3, loss: 0.045477873104951744\n",
      "epoch: 4, loss: 0.03933610239178473\n",
      "epoch: 5, loss: 0.03750499747974437\n",
      "epoch: 6, loss: 0.039526133813479206\n",
      "epoch: 7, loss: 0.0426253032454786\n",
      "epoch: 8, loss: 0.04160680561318068\n",
      "epoch: 9, loss: 0.03805711114891128\n",
      "epoch: 10, loss: 0.03366188888845172\n",
      "epoch: 11, loss: 0.031047359818558266\n",
      "epoch: 12, loss: 0.030246718247898766\n",
      "epoch: 13, loss: 0.03073652858823144\n",
      "epoch: 14, loss: 0.030738794098927408\n",
      "epoch: 15, loss: 0.029195094060621245\n",
      "epoch: 16, loss: 0.027868953835251342\n",
      "epoch: 17, loss: 0.02684252751258179\n",
      "epoch: 18, loss: 0.026942857147041237\n",
      "epoch: 19, loss: 0.026822450436171597\n",
      "epoch: 20, loss: 0.02683538055039833\n",
      "epoch: 21, loss: 0.026631527200020252\n",
      "epoch: 22, loss: 0.026636493709978518\n",
      "epoch: 23, loss: 0.02657727143227913\n",
      "epoch: 24, loss: 0.026511761620906876\n",
      "epoch: 25, loss: 0.02575586195063696\n",
      "epoch: 26, loss: 0.02454424379502913\n",
      "epoch: 27, loss: 0.024677682828931777\n",
      "epoch: 28, loss: 0.023954925588765073\n",
      "epoch: 29, loss: 0.023718994734526835\n",
      "epoch: 30, loss: 0.023411368437418555\n",
      "epoch: 31, loss: 0.022979987760884727\n",
      "epoch: 32, loss: 0.022781540945867308\n",
      "epoch: 33, loss: 0.022083504957487125\n",
      "epoch: 34, loss: 0.02194164239928387\n",
      "epoch: 35, loss: 0.021632490452425598\n",
      "epoch: 36, loss: 0.020885040369657264\n",
      "epoch: 37, loss: 0.02073035737564929\n",
      "epoch: 38, loss: 0.02067632996669114\n",
      "epoch: 39, loss: 0.02031843050736932\n",
      "epoch: 40, loss: 0.019844241022730042\n",
      "epoch: 41, loss: 0.01948502239056733\n",
      "epoch: 42, loss: 0.01909557314254045\n",
      "epoch: 43, loss: 0.018728434909471244\n",
      "epoch: 44, loss: 0.01854077737992477\n",
      "epoch: 45, loss: 0.018344201775971083\n",
      "epoch: 46, loss: 0.01726788187391619\n",
      "epoch: 47, loss: 0.01722408060854447\n",
      "epoch: 48, loss: 0.017199630576440583\n",
      "epoch: 49, loss: 0.017014523161721928\n",
      "epoch: 50, loss: 0.01644861703682716\n",
      "epoch: 51, loss: 0.015667429195797757\n",
      "epoch: 52, loss: 0.015645936100375808\n",
      "epoch: 53, loss: 0.01558048139056128\n",
      "epoch: 54, loss: 0.015321434083143506\n",
      "epoch: 55, loss: 0.014574036023512886\n",
      "epoch: 56, loss: 0.014294192469683688\n",
      "epoch: 57, loss: 0.013898498319327865\n",
      "epoch: 58, loss: 0.0136633496475689\n",
      "epoch: 59, loss: 0.013206708050062115\n",
      "epoch: 60, loss: 0.01273620823472156\n",
      "epoch: 61, loss: 0.01253353275583015\n",
      "epoch: 62, loss: 0.01187142212213039\n",
      "epoch: 63, loss: 0.011119618752983798\n",
      "epoch: 64, loss: 0.010224187125473863\n",
      "epoch: 65, loss: 0.00960188966869362\n",
      "epoch: 66, loss: 0.008584532960140624\n",
      "epoch: 67, loss: 0.007681081124764857\n",
      "epoch: 68, loss: 0.006684131282372646\n",
      "epoch: 69, loss: 0.006106963863231819\n",
      "epoch: 70, loss: 0.005861441789480678\n",
      "epoch: 71, loss: 0.005612063190925629\n",
      "epoch: 72, loss: 0.005263979265696138\n",
      "epoch: 73, loss: 0.005148315032895089\n",
      "epoch: 74, loss: 0.005271669732068817\n",
      "epoch: 75, loss: 0.004979656256918734\n",
      "epoch: 76, loss: 0.004684284360862051\n",
      "epoch: 77, loss: 0.004538628574508689\n",
      "epoch: 78, loss: 0.00443020567848084\n",
      "epoch: 79, loss: 0.004328948688904438\n",
      "epoch: 80, loss: 0.004125720933847965\n",
      "epoch: 81, loss: 0.004082029960349815\n",
      "epoch: 82, loss: 0.004142523442142467\n",
      "epoch: 83, loss: 0.0040259060630431324\n",
      "epoch: 84, loss: 0.004035078666017012\n",
      "epoch: 85, loss: 0.004065351078331775\n",
      "epoch: 86, loss: 0.0038327382707832357\n",
      "epoch: 87, loss: 0.0038066017033387234\n",
      "epoch: 88, loss: 0.003915130125016504\n",
      "epoch: 89, loss: 0.003882555181913843\n",
      "epoch: 90, loss: 0.0037605409562264064\n",
      "epoch: 91, loss: 0.00362845979468524\n",
      "epoch: 92, loss: 0.0036080753786797083\n",
      "epoch: 93, loss: 0.003515569286482998\n",
      "epoch: 94, loss: 0.0033281127266178924\n",
      "epoch: 95, loss: 0.0032228769404655765\n",
      "epoch: 96, loss: 0.0032100327794261363\n",
      "epoch: 97, loss: 0.0030467413932744134\n",
      "epoch: 98, loss: 0.0031003495757551767\n",
      "epoch: 99, loss: 0.002966834330339066\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(1):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98a61e1bfa140d38f8462912eae4c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008e0705ce6140c0b304e52c47c87c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06888349877810346\n",
      "epoch: 1, loss: 0.04170716387217396\n",
      "epoch: 2, loss: 0.043755696919734334\n",
      "epoch: 3, loss: 0.0453672583612064\n",
      "epoch: 4, loss: 0.04274064408140063\n",
      "epoch: 5, loss: 0.03873392912509385\n",
      "epoch: 6, loss: 0.03408471229252329\n",
      "epoch: 7, loss: 0.03244407722240227\n",
      "epoch: 8, loss: 0.031560987084855746\n",
      "epoch: 9, loss: 0.030312792199642162\n",
      "epoch: 10, loss: 0.02816102409869636\n",
      "epoch: 11, loss: 0.026351246356797454\n",
      "epoch: 12, loss: 0.027056061156669375\n",
      "epoch: 13, loss: 0.026790699569576054\n",
      "epoch: 14, loss: 0.02656153377497302\n",
      "epoch: 15, loss: 0.02566933974551545\n",
      "epoch: 16, loss: 0.025348860695189407\n",
      "epoch: 17, loss: 0.02480803673936906\n",
      "epoch: 18, loss: 0.023732094606699592\n",
      "epoch: 19, loss: 0.022265464436078318\n",
      "epoch: 20, loss: 0.020810488726352053\n",
      "epoch: 21, loss: 0.01916129491303538\n",
      "epoch: 22, loss: 0.018121649154711445\n",
      "epoch: 23, loss: 0.016592859461028463\n",
      "epoch: 24, loss: 0.014778237972468314\n",
      "epoch: 25, loss: 0.013481882429890417\n",
      "epoch: 26, loss: 0.012969959865064027\n",
      "epoch: 27, loss: 0.012646285180985746\n",
      "epoch: 28, loss: 0.013337360503096536\n",
      "epoch: 29, loss: 0.01320113792560657\n",
      "epoch: 30, loss: 0.012214617402801407\n",
      "epoch: 31, loss: 0.011830726605319551\n",
      "epoch: 32, loss: 0.012246419485327835\n",
      "epoch: 33, loss: 0.012714191416246348\n",
      "epoch: 34, loss: 0.01293390015322965\n",
      "epoch: 35, loss: 0.012734987812387077\n",
      "epoch: 36, loss: 0.012655777005068087\n",
      "epoch: 37, loss: 0.012321055925739923\n",
      "epoch: 38, loss: 0.01208326687344586\n",
      "epoch: 39, loss: 0.012548557776760918\n",
      "epoch: 40, loss: 0.012194926911999956\n",
      "epoch: 41, loss: 0.012165015334631766\n",
      "epoch: 42, loss: 0.01213995229415313\n",
      "epoch: 43, loss: 0.011868096067180112\n",
      "epoch: 44, loss: 0.011664909671401073\n",
      "epoch: 45, loss: 0.011474881630851301\n",
      "epoch: 46, loss: 0.011242921180137868\n",
      "epoch: 47, loss: 0.01171513979827138\n",
      "epoch: 48, loss: 0.011971770643071224\n",
      "epoch: 49, loss: 0.011011444969593135\n",
      "epoch: 50, loss: 0.011475215700458903\n",
      "epoch: 51, loss: 0.011362458612602482\n",
      "epoch: 52, loss: 0.011168140486467653\n",
      "epoch: 53, loss: 0.011601886641629615\n",
      "epoch: 54, loss: 0.011352916455317044\n",
      "epoch: 55, loss: 0.011331106933986283\n",
      "epoch: 56, loss: 0.011208760089935614\n",
      "epoch: 57, loss: 0.011247801385893224\n",
      "epoch: 58, loss: 0.01123267055069269\n",
      "epoch: 59, loss: 0.011326252546312775\n",
      "epoch: 60, loss: 0.011183634790515548\n",
      "epoch: 61, loss: 0.010848970841594877\n",
      "epoch: 62, loss: 0.010910864902562713\n",
      "epoch: 63, loss: 0.011170940893228998\n",
      "epoch: 64, loss: 0.01102649400049355\n",
      "epoch: 65, loss: 0.01138772214098292\n",
      "epoch: 66, loss: 0.010886362059659916\n",
      "epoch: 67, loss: 0.011004212492311089\n",
      "epoch: 68, loss: 0.010785932713671566\n",
      "epoch: 69, loss: 0.010826883700863946\n",
      "epoch: 70, loss: 0.010921296502164805\n",
      "epoch: 71, loss: 0.010855027576544319\n",
      "epoch: 72, loss: 0.010796949656865142\n",
      "epoch: 73, loss: 0.01084890065304257\n",
      "epoch: 74, loss: 0.010731260457163914\n",
      "epoch: 75, loss: 0.011098010582613083\n",
      "epoch: 76, loss: 0.011123352356016358\n",
      "epoch: 77, loss: 0.010974628670853472\n",
      "epoch: 78, loss: 0.010949933007224084\n",
      "epoch: 79, loss: 0.010883171991860063\n",
      "epoch: 80, loss: 0.010881068027420464\n",
      "epoch: 81, loss: 0.011064865076319153\n",
      "epoch: 82, loss: 0.010751066370545782\n",
      "epoch: 83, loss: 0.010720347056399843\n",
      "epoch: 84, loss: 0.010781917755151938\n",
      "epoch: 85, loss: 0.01104763661709033\n",
      "epoch: 86, loss: 0.010866761219710828\n",
      "epoch: 87, loss: 0.011063134009940367\n",
      "epoch: 88, loss: 0.01103561678423916\n",
      "epoch: 89, loss: 0.010595528890926478\n",
      "epoch: 90, loss: 0.010977700623258303\n",
      "epoch: 91, loss: 0.011401486048516197\n",
      "epoch: 92, loss: 0.010612174890434709\n",
      "epoch: 93, loss: 0.010704152461785386\n",
      "epoch: 94, loss: 0.011008226272866784\n",
      "epoch: 95, loss: 0.010817969960600471\n",
      "epoch: 96, loss: 0.010817431220695449\n",
      "epoch: 97, loss: 0.011145407632834427\n",
      "epoch: 98, loss: 0.01103128867034173\n",
      "epoch: 99, loss: 0.010749457814318906\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5261857ffe14c57aff8e9b837278b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c948ab5a6bf4b4296ecfb4cfe5ec51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09344144196765708\n",
      "epoch: 1, loss: 0.03847494491469224\n",
      "epoch: 2, loss: 0.028334790012879325\n",
      "epoch: 3, loss: 0.02495161731408011\n",
      "epoch: 4, loss: 0.024239566616645038\n",
      "epoch: 5, loss: 0.023856188574478104\n",
      "epoch: 6, loss: 0.02182787111281192\n",
      "epoch: 7, loss: 0.018893749163490278\n",
      "epoch: 8, loss: 0.019241795566183603\n",
      "epoch: 9, loss: 0.020946829747305967\n",
      "epoch: 10, loss: 0.02187300112929908\n",
      "epoch: 11, loss: 0.020225437488098557\n",
      "epoch: 12, loss: 0.016985978703174122\n",
      "epoch: 13, loss: 0.015029558187575716\n",
      "epoch: 14, loss: 0.014204430495884402\n",
      "epoch: 15, loss: 0.014762027512510243\n",
      "epoch: 16, loss: 0.013815195434878899\n",
      "epoch: 17, loss: 0.012364054012737461\n",
      "epoch: 18, loss: 0.010042147366427967\n",
      "epoch: 19, loss: 0.009233064572284629\n",
      "epoch: 20, loss: 0.009349347363728366\n",
      "epoch: 21, loss: 0.010080882088469889\n",
      "epoch: 22, loss: 0.009836438901947138\n",
      "epoch: 23, loss: 0.008961684332381264\n",
      "epoch: 24, loss: 0.007860536838856841\n",
      "epoch: 25, loss: 0.007952361875293548\n",
      "epoch: 26, loss: 0.008067514256388289\n",
      "epoch: 27, loss: 0.007995107376177958\n",
      "epoch: 28, loss: 0.00768837015499336\n",
      "epoch: 29, loss: 0.007185839742360403\n",
      "epoch: 30, loss: 0.006877211117314891\n",
      "epoch: 31, loss: 0.006819337472911798\n",
      "epoch: 32, loss: 0.007187569872596547\n",
      "epoch: 33, loss: 0.007072830411336486\n",
      "epoch: 34, loss: 0.007017108076191716\n",
      "epoch: 35, loss: 0.006117456905194945\n",
      "epoch: 36, loss: 0.006195593788193752\n",
      "epoch: 37, loss: 0.006440986049055284\n",
      "epoch: 38, loss: 0.006372356201828083\n",
      "epoch: 39, loss: 0.006085963462304624\n",
      "epoch: 40, loss: 0.006131424063524015\n",
      "epoch: 41, loss: 0.005791214948876103\n",
      "epoch: 42, loss: 0.005958164740090866\n",
      "epoch: 43, loss: 0.006110775943742363\n",
      "epoch: 44, loss: 0.005821269262934612\n",
      "epoch: 45, loss: 0.005555436885668199\n",
      "epoch: 46, loss: 0.00566520802803962\n",
      "epoch: 47, loss: 0.005965273203002579\n",
      "epoch: 48, loss: 0.005920993908021497\n",
      "epoch: 49, loss: 0.00585789067219229\n",
      "epoch: 50, loss: 0.005531506516078494\n",
      "epoch: 51, loss: 0.005560345949678111\n",
      "epoch: 52, loss: 0.005484526296263953\n",
      "epoch: 53, loss: 0.005330947139358371\n",
      "epoch: 54, loss: 0.005182183765136736\n",
      "epoch: 55, loss: 0.005145986057055696\n",
      "epoch: 56, loss: 0.005161621211204524\n",
      "epoch: 57, loss: 0.0053791192708095605\n",
      "epoch: 58, loss: 0.005055031532940062\n",
      "epoch: 59, loss: 0.005013850309199617\n",
      "epoch: 60, loss: 0.004967883260894872\n",
      "epoch: 61, loss: 0.0048807672192603705\n",
      "epoch: 62, loss: 0.004893151911946371\n",
      "epoch: 63, loss: 0.005061962376725173\n",
      "epoch: 64, loss: 0.0048841142366571775\n",
      "epoch: 65, loss: 0.004667361386019775\n",
      "epoch: 66, loss: 0.0046644428420184285\n",
      "epoch: 67, loss: 0.004473217223747193\n",
      "epoch: 68, loss: 0.004627465300975067\n",
      "epoch: 69, loss: 0.00474351358050594\n",
      "epoch: 70, loss: 0.004536619513882515\n",
      "epoch: 71, loss: 0.004528383989088985\n",
      "epoch: 72, loss: 0.0046248392175883115\n",
      "epoch: 73, loss: 0.004316877103968292\n",
      "epoch: 74, loss: 0.004365436606351274\n",
      "epoch: 75, loss: 0.004430334585470659\n",
      "epoch: 76, loss: 0.004392076449437608\n",
      "epoch: 77, loss: 0.004525193352402143\n",
      "epoch: 78, loss: 0.0042998502508126354\n",
      "epoch: 79, loss: 0.0042464055262159016\n",
      "epoch: 80, loss: 0.0042141739568711785\n",
      "epoch: 81, loss: 0.004394476346067636\n",
      "epoch: 82, loss: 0.004530264277480292\n",
      "epoch: 83, loss: 0.004328159117754549\n",
      "epoch: 84, loss: 0.004271188465589083\n",
      "epoch: 85, loss: 0.0042032862854409145\n",
      "epoch: 86, loss: 0.004301428985634836\n",
      "epoch: 87, loss: 0.0042502401014594\n",
      "epoch: 88, loss: 0.004293470225664026\n",
      "epoch: 89, loss: 0.0041178072123667815\n",
      "epoch: 90, loss: 0.004084493184223764\n",
      "epoch: 91, loss: 0.004270067111339903\n",
      "epoch: 92, loss: 0.004229165299874443\n",
      "epoch: 93, loss: 0.004344144591460303\n",
      "epoch: 94, loss: 0.004012697190590982\n",
      "epoch: 95, loss: 0.004077130816344105\n",
      "epoch: 96, loss: 0.004434541712360079\n",
      "epoch: 97, loss: 0.0041469689528505225\n",
      "epoch: 98, loss: 0.00427356899874385\n",
      "epoch: 99, loss: 0.004142683089464516\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(1):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d551be12464f2b94dc281e6dfe8458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183c24d9c661427cbafc3cf8e9bc2c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09503080111188264\n",
      "epoch: 1, loss: 0.07131605341456128\n",
      "epoch: 2, loss: 0.05539257297823354\n",
      "epoch: 3, loss: 0.04538109141473199\n",
      "epoch: 4, loss: 0.040166461106920744\n",
      "epoch: 5, loss: 0.036581912672133406\n",
      "epoch: 6, loss: 0.032114736520677055\n",
      "epoch: 7, loss: 0.029519013548821457\n",
      "epoch: 8, loss: 0.029390661543155807\n",
      "epoch: 9, loss: 0.029852533316691878\n",
      "epoch: 10, loss: 0.029911282064185488\n",
      "epoch: 11, loss: 0.030425309633529258\n",
      "epoch: 12, loss: 0.030589562716429323\n",
      "epoch: 13, loss: 0.03068986841226526\n",
      "epoch: 14, loss: 0.029724535768319193\n",
      "epoch: 15, loss: 0.02815935853515242\n",
      "epoch: 16, loss: 0.0273586572666215\n",
      "epoch: 17, loss: 0.027149160357953204\n",
      "epoch: 18, loss: 0.02732380740905134\n",
      "epoch: 19, loss: 0.027826753518855777\n",
      "epoch: 20, loss: 0.026736642497674285\n",
      "epoch: 21, loss: 0.0256362054139416\n",
      "epoch: 22, loss: 0.024637133565924944\n",
      "epoch: 23, loss: 0.02443166603913308\n",
      "epoch: 24, loss: 0.02358062692154877\n",
      "epoch: 25, loss: 0.02353253292606469\n",
      "epoch: 26, loss: 0.023030713519849626\n",
      "epoch: 27, loss: 0.022445733072574716\n",
      "epoch: 28, loss: 0.022045560172803354\n",
      "epoch: 29, loss: 0.021288819903489218\n",
      "epoch: 30, loss: 0.02083823002517385\n",
      "epoch: 31, loss: 0.020831161486643858\n",
      "epoch: 32, loss: 0.02043414351284849\n",
      "epoch: 33, loss: 0.020053975480213287\n",
      "epoch: 34, loss: 0.020069192250763552\n",
      "epoch: 35, loss: 0.019785519822282507\n",
      "epoch: 36, loss: 0.01953215894915729\n",
      "epoch: 37, loss: 0.019514255718495542\n",
      "epoch: 38, loss: 0.019422530874645176\n",
      "epoch: 39, loss: 0.019333870724669036\n",
      "epoch: 40, loss: 0.018976469007733997\n",
      "epoch: 41, loss: 0.018900999270833347\n",
      "epoch: 42, loss: 0.018513027187907097\n",
      "epoch: 43, loss: 0.018638540726085513\n",
      "epoch: 44, loss: 0.018110676325319885\n",
      "epoch: 45, loss: 0.018063881358812612\n",
      "epoch: 46, loss: 0.01755536159171703\n",
      "epoch: 47, loss: 0.01658408641193667\n",
      "epoch: 48, loss: 0.015901022297832895\n",
      "epoch: 49, loss: 0.014874138006870895\n",
      "epoch: 50, loss: 0.013507712267883035\n",
      "epoch: 51, loss: 0.012158268614812005\n",
      "epoch: 52, loss: 0.010862734306863754\n",
      "epoch: 53, loss: 0.01021816262894144\n",
      "epoch: 54, loss: 0.010313775424998924\n",
      "epoch: 55, loss: 0.01060574959394728\n",
      "epoch: 56, loss: 0.010284593093442894\n",
      "epoch: 57, loss: 0.009756039313056068\n",
      "epoch: 58, loss: 0.009652744772641938\n",
      "epoch: 59, loss: 0.009709121919535426\n",
      "epoch: 60, loss: 0.00908887255810407\n",
      "epoch: 61, loss: 0.009279318257149626\n",
      "epoch: 62, loss: 0.00914465714858266\n",
      "epoch: 63, loss: 0.009464256449058009\n",
      "epoch: 64, loss: 0.009362484241527965\n",
      "epoch: 65, loss: 0.009024991381982147\n",
      "epoch: 66, loss: 0.008881605356245584\n",
      "epoch: 67, loss: 0.008880192492277414\n",
      "epoch: 68, loss: 0.009010546395288239\n",
      "epoch: 69, loss: 0.00887951947760071\n",
      "epoch: 70, loss: 0.008542233818110987\n",
      "epoch: 71, loss: 0.008667139987023275\n",
      "epoch: 72, loss: 0.008343019460944463\n",
      "epoch: 73, loss: 0.008347544863242405\n",
      "epoch: 74, loss: 0.0080683146029679\n",
      "epoch: 75, loss: 0.007984183635433927\n",
      "epoch: 76, loss: 0.007904460645312519\n",
      "epoch: 77, loss: 0.008184639643657912\n",
      "epoch: 78, loss: 0.007794432358755012\n",
      "epoch: 79, loss: 0.007418694076320519\n",
      "epoch: 80, loss: 0.007305453503411775\n",
      "epoch: 81, loss: 0.007294431541908323\n",
      "epoch: 82, loss: 0.007298157025022608\n",
      "epoch: 83, loss: 0.0071459069711777\n",
      "epoch: 84, loss: 0.006879718484251036\n",
      "epoch: 85, loss: 0.006990224847724009\n",
      "epoch: 86, loss: 0.0068776591475157986\n",
      "epoch: 87, loss: 0.006683023214811949\n",
      "epoch: 88, loss: 0.0069084833899225135\n",
      "epoch: 89, loss: 0.006789228031101073\n",
      "epoch: 90, loss: 0.006794804819115346\n",
      "epoch: 91, loss: 0.006654470944089597\n",
      "epoch: 92, loss: 0.006454210606885445\n",
      "epoch: 93, loss: 0.007029719221541812\n",
      "epoch: 94, loss: 0.006724372151919241\n",
      "epoch: 95, loss: 0.006723080210047413\n",
      "epoch: 96, loss: 0.006615971343955251\n",
      "epoch: 97, loss: 0.006771375516118588\n",
      "epoch: 98, loss: 0.006451635746704489\n",
      "epoch: 99, loss: 0.006624818045739811\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps = 1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a969af6826c744769ccbeb7e3ab59364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e82a7550a0748c694439b659fb774ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.04929826013089703\n",
      "epoch: 1, loss: 0.031981478279415304\n",
      "epoch: 2, loss: 0.024375019904732606\n",
      "epoch: 3, loss: 0.021731959567876387\n",
      "epoch: 4, loss: 0.02175370798865817\n",
      "epoch: 5, loss: 0.022650170500369323\n",
      "epoch: 6, loss: 0.023383846321195608\n",
      "epoch: 7, loss: 0.02317652981565656\n",
      "epoch: 8, loss: 0.02290383975880445\n",
      "epoch: 9, loss: 0.022275046638121056\n",
      "epoch: 10, loss: 0.021996478110726296\n",
      "epoch: 11, loss: 0.0214250106453271\n",
      "epoch: 12, loss: 0.020719401625186432\n",
      "epoch: 13, loss: 0.01998516272853687\n",
      "epoch: 14, loss: 0.019070362821057705\n",
      "epoch: 15, loss: 0.017605081274046617\n",
      "epoch: 16, loss: 0.015176424650923297\n",
      "epoch: 17, loss: 0.012464493072753132\n",
      "epoch: 18, loss: 0.010324536917289786\n",
      "epoch: 19, loss: 0.009549416798254686\n",
      "epoch: 20, loss: 0.009447713508234472\n",
      "epoch: 21, loss: 0.00923994430357399\n",
      "epoch: 22, loss: 0.009105743478709967\n",
      "epoch: 23, loss: 0.009068213425285266\n",
      "epoch: 24, loss: 0.00886527972086152\n",
      "epoch: 25, loss: 0.009051811703317298\n",
      "epoch: 26, loss: 0.00918969228216809\n",
      "epoch: 27, loss: 0.008892516454612694\n",
      "epoch: 28, loss: 0.008598133386422326\n",
      "epoch: 29, loss: 0.00822752095144814\n",
      "epoch: 30, loss: 0.007817288259130481\n",
      "epoch: 31, loss: 0.0077717824688353695\n",
      "epoch: 32, loss: 0.007480971003190089\n",
      "epoch: 33, loss: 0.007293474707400304\n",
      "epoch: 34, loss: 0.007161640601765442\n",
      "epoch: 35, loss: 0.007029530558076698\n",
      "epoch: 36, loss: 0.0072470465661002405\n",
      "epoch: 37, loss: 0.007158741234461081\n",
      "epoch: 38, loss: 0.007173874936861213\n",
      "epoch: 39, loss: 0.007272091580317893\n",
      "epoch: 40, loss: 0.007027466999139876\n",
      "epoch: 41, loss: 0.006829512320578877\n",
      "epoch: 42, loss: 0.00700332736883006\n",
      "epoch: 43, loss: 0.006948813034039885\n",
      "epoch: 44, loss: 0.0069688197805941\n",
      "epoch: 45, loss: 0.006825605482201382\n",
      "epoch: 46, loss: 0.006608045578976795\n",
      "epoch: 47, loss: 0.006636858865246049\n",
      "epoch: 48, loss: 0.006678565576863059\n",
      "epoch: 49, loss: 0.00664425866937179\n",
      "epoch: 50, loss: 0.006496328620452646\n",
      "epoch: 51, loss: 0.006521816980095435\n",
      "epoch: 52, loss: 0.006429987287923065\n",
      "epoch: 53, loss: 0.006517710155612156\n",
      "epoch: 54, loss: 0.006213913846228535\n",
      "epoch: 55, loss: 0.006274886261617228\n",
      "epoch: 56, loss: 0.006265057476980718\n",
      "epoch: 57, loss: 0.006154611518699855\n",
      "epoch: 58, loss: 0.006229250673777122\n",
      "epoch: 59, loss: 0.006227277295969208\n",
      "epoch: 60, loss: 0.005939082891905705\n",
      "epoch: 61, loss: 0.006048337097774\n",
      "epoch: 62, loss: 0.006238854047112889\n",
      "epoch: 63, loss: 0.005984438295837863\n",
      "epoch: 64, loss: 0.0058464531457049825\n",
      "epoch: 65, loss: 0.005951990984237897\n",
      "epoch: 66, loss: 0.005973768932931034\n",
      "epoch: 67, loss: 0.0059310630793304045\n",
      "epoch: 68, loss: 0.005799203376131918\n",
      "epoch: 69, loss: 0.005908606679505729\n",
      "epoch: 70, loss: 0.006057574423744688\n",
      "epoch: 71, loss: 0.005959411809326126\n",
      "epoch: 72, loss: 0.006016037837671375\n",
      "epoch: 73, loss: 0.006017586950292859\n",
      "epoch: 74, loss: 0.0059230410284953645\n",
      "epoch: 75, loss: 0.005810869730045242\n",
      "epoch: 76, loss: 0.006120822292443874\n",
      "epoch: 77, loss: 0.005877993790150232\n",
      "epoch: 78, loss: 0.006002088253180302\n",
      "epoch: 79, loss: 0.0058710274528698154\n",
      "epoch: 80, loss: 0.005973597058350283\n",
      "epoch: 81, loss: 0.005806601037814052\n",
      "epoch: 82, loss: 0.005932065099214504\n",
      "epoch: 83, loss: 0.0059197257504203776\n",
      "epoch: 84, loss: 0.005861304619297605\n",
      "epoch: 85, loss: 0.005839051011930801\n",
      "epoch: 86, loss: 0.0059301079217481565\n",
      "epoch: 87, loss: 0.005830174652155816\n",
      "epoch: 88, loss: 0.0058894844460912425\n",
      "epoch: 89, loss: 0.005806486027350582\n",
      "epoch: 90, loss: 0.005899762050846151\n",
      "epoch: 91, loss: 0.005869707201585609\n",
      "epoch: 92, loss: 0.0059079399971462615\n",
      "epoch: 93, loss: 0.005813926357609428\n",
      "epoch: 94, loss: 0.0059865371924497385\n",
      "epoch: 95, loss: 0.00584776689901479\n",
      "epoch: 96, loss: 0.005859044425150121\n",
      "epoch: 97, loss: 0.005936910664117463\n",
      "epoch: 98, loss: 0.00608599571005442\n",
      "epoch: 99, loss: 0.005851227829012068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(1):\n",
    "    dnn = sequential_dnn(dim = [3, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(q_bits = [3, 4, 4],\n",
    "                     dim = [3, 4, 4, 1],\n",
    "                     reps = 2,\n",
    "                     backend=backend,\n",
    "                     shots=10000,\n",
    "                     lr = 0.1)\n",
    "\n",
    "qnn.train(x, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scaler(x, mode=\"standard\")\n",
    "\n",
    "dnn = sequential_dnn(dim = [3, 6, 5, 1], lr = 0.1)\n",
    "\n",
    "dnn.train(x, y, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qnn.loss)\n",
    "plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.train(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
