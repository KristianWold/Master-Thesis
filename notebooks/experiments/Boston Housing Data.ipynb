{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qiskit import Aer\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing as mp\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(args):\n",
    "    model = args[0]\n",
    "    x = args[1]\n",
    "    y = args[2]\n",
    "    verbose = args[3]\n",
    "    \n",
    "    model.train(x, y, verbose = verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "x = data.data\n",
    "x_scaled = scaler(x, mode=\"standard\")\n",
    "\n",
    "y = data.target.reshape(-1, 1)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "x_pca = pca.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qcn = scaler(x_pca, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x_pca, mode=\"standard\")\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train_qcn, x_test_qcn, y_train, y_test = train_test_split(x_qcn, y, train_size=100, test_size=100)\n",
    "np.random.seed(42)\n",
    "x_train_dnn, x_test_dnn, y_train, y_test = train_test_split(x_dnn, y, train_size=100, test_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    dnn = sequential_dnn(dim = [4, 5, 1])\n",
    "    dnn.train(x_train_dnn, y_train, epochs = 100)\n",
    "    dnn_list.append(dnn)\n",
    "    \n",
    "saver(dnn_list, data_path(\"boston_dnn_pca\"))\n",
    "plt.plot(dnn_list[0].loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dnn_list[7].loss[-1])\n",
    "\n",
    "y_pred = dnn_list[0].predict(x_test_dnn)\n",
    "loss = np.mean((y_pred - y_test)**2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "qcn_list = []\n",
    "for i in range(10):\n",
    "    qcn = sequential_qnn(n_qubits = [4, 4],\n",
    "                         dim = [4, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         shots=0)\n",
    "    \n",
    "    qcn_list.append([qcn, x_train_qcn, y_train, False])\n",
    "\n",
    "qcn_list[0][3] = True    \n",
    "    \n",
    "with mp.Pool(10) as p:\n",
    "    qcn_list = p.map(parallel, qcn_list)     \n",
    "    \n",
    "saver(qcn_list, data_path(\"boston_qcn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcn_list = loader(data_path(\"boston_qcn\"))\n",
    "qcn_list[4].loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcn_list = loader(data_path(\"boston_qcn\"))\n",
    "\n",
    "y_pred = qcn_list[4].predict(x_test_qcn)\n",
    "loss = np.mean((y_pred - y_test)**2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "x = data.data\n",
    "x_scaled = scaler(x, mode=\"standard\")\n",
    "\n",
    "y = data.target.reshape(-1, 1)\n",
    "y = scaler(y, a=0, b=1)\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, train_size=100, test_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08094445181935711\n",
      "epoch: 1, loss: 0.04430139091471586\n",
      "epoch: 2, loss: 0.04104623898302903\n",
      "epoch: 3, loss: 0.03427491433839565\n",
      "epoch: 4, loss: 0.03379318669585971\n",
      "epoch: 5, loss: 0.03152923445378866\n",
      "epoch: 6, loss: 0.028273221413863903\n",
      "epoch: 7, loss: 0.0257601287509094\n",
      "epoch: 8, loss: 0.024544366442578673\n",
      "epoch: 9, loss: 0.02396476393658406\n",
      "epoch: 10, loss: 0.02273546121795646\n",
      "epoch: 11, loss: 0.02165854369000865\n",
      "epoch: 12, loss: 0.021174715952551118\n",
      "epoch: 13, loss: 0.02081952765350094\n",
      "epoch: 14, loss: 0.019343210151085966\n",
      "epoch: 15, loss: 0.01804258416592602\n",
      "epoch: 16, loss: 0.017961632583743833\n",
      "epoch: 17, loss: 0.017986743903271124\n",
      "epoch: 18, loss: 0.017480213437854823\n",
      "epoch: 19, loss: 0.016723541367531635\n",
      "epoch: 20, loss: 0.015844677252690273\n",
      "epoch: 21, loss: 0.01480547605136676\n",
      "epoch: 22, loss: 0.014108761463250373\n",
      "epoch: 23, loss: 0.013990363149577342\n",
      "epoch: 24, loss: 0.013851762462249163\n",
      "epoch: 25, loss: 0.013495983907331992\n",
      "epoch: 26, loss: 0.01305231034390054\n",
      "epoch: 27, loss: 0.012808584562958705\n",
      "epoch: 28, loss: 0.01269927281624188\n",
      "epoch: 29, loss: 0.01231654829004838\n",
      "epoch: 30, loss: 0.011691523314434665\n",
      "epoch: 31, loss: 0.011335780033377277\n",
      "epoch: 32, loss: 0.01122925345002827\n",
      "epoch: 33, loss: 0.010966837225232999\n",
      "epoch: 34, loss: 0.010609070891653603\n",
      "epoch: 35, loss: 0.010375665642240663\n",
      "epoch: 36, loss: 0.01022927303260067\n",
      "epoch: 37, loss: 0.00999157104562149\n",
      "epoch: 38, loss: 0.009701687723273334\n",
      "epoch: 39, loss: 0.009482683759421368\n",
      "epoch: 40, loss: 0.009277819939421048\n",
      "epoch: 41, loss: 0.009030008763760278\n",
      "epoch: 42, loss: 0.008799343655272686\n",
      "epoch: 43, loss: 0.008642371737310613\n",
      "epoch: 44, loss: 0.008470671179191883\n",
      "epoch: 45, loss: 0.008257093991133802\n",
      "epoch: 46, loss: 0.008110355366415472\n",
      "epoch: 47, loss: 0.007980225265938709\n",
      "epoch: 48, loss: 0.007847360516928102\n",
      "epoch: 49, loss: 0.007758800698916619\n",
      "epoch: 50, loss: 0.00767162873210884\n",
      "epoch: 51, loss: 0.007580030191504913\n",
      "epoch: 52, loss: 0.007506635084969971\n",
      "epoch: 53, loss: 0.007405852403387011\n",
      "epoch: 54, loss: 0.007270354874181952\n",
      "epoch: 55, loss: 0.007158487948424852\n",
      "epoch: 56, loss: 0.007072052572170207\n",
      "epoch: 57, loss: 0.006987762734333151\n",
      "epoch: 58, loss: 0.006904852488543349\n",
      "epoch: 59, loss: 0.006828351395483474\n",
      "epoch: 60, loss: 0.006754459063117207\n",
      "epoch: 61, loss: 0.006697795539823772\n",
      "epoch: 62, loss: 0.0066425478685050365\n",
      "epoch: 63, loss: 0.006571796783953666\n",
      "epoch: 64, loss: 0.006496602573969909\n",
      "epoch: 65, loss: 0.006425560503163345\n",
      "epoch: 66, loss: 0.006349572581392181\n",
      "epoch: 67, loss: 0.0062714074835092115\n",
      "epoch: 68, loss: 0.006196235343137354\n",
      "epoch: 69, loss: 0.006128170292250473\n",
      "epoch: 70, loss: 0.006067579449497473\n",
      "epoch: 71, loss: 0.0060031953194845415\n",
      "epoch: 72, loss: 0.005936656366184586\n",
      "epoch: 73, loss: 0.0058713238854964074\n",
      "epoch: 74, loss: 0.0058023329460082105\n",
      "epoch: 75, loss: 0.005732223475146198\n",
      "epoch: 76, loss: 0.0056541475327142095\n",
      "epoch: 77, loss: 0.005552934999475559\n",
      "epoch: 78, loss: 0.0054197462054263856\n",
      "epoch: 79, loss: 0.005262975563134833\n",
      "epoch: 80, loss: 0.0051240195458306815\n",
      "epoch: 81, loss: 0.004998567228501367\n",
      "epoch: 82, loss: 0.004937354875376593\n",
      "epoch: 83, loss: 0.004919674129053345\n",
      "epoch: 84, loss: 0.004779173227054599\n",
      "epoch: 85, loss: 0.004760849193982954\n",
      "epoch: 86, loss: 0.004797131814350124\n",
      "epoch: 87, loss: 0.004717864214420015\n",
      "epoch: 88, loss: 0.004651056015879205\n",
      "epoch: 89, loss: 0.004661559941105694\n",
      "epoch: 90, loss: 0.004621015373737745\n",
      "epoch: 91, loss: 0.004595029025447291\n",
      "epoch: 92, loss: 0.004581199892140684\n",
      "epoch: 93, loss: 0.004563297648068369\n",
      "epoch: 94, loss: 0.0045157773684629445\n",
      "epoch: 95, loss: 0.004500129356159053\n",
      "epoch: 96, loss: 0.0044825107317086905\n",
      "epoch: 97, loss: 0.004427170533702165\n",
      "epoch: 98, loss: 0.004409847912418529\n",
      "epoch: 99, loss: 0.004398645477573036\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "hybrid_list = []\n",
    "for i in range(10):\n",
    "    layer1 = Dense(n_features = 13, \n",
    "                   n_targets = 4,\n",
    "                   activation = Tanh(),\n",
    "                   scale=np.pi)\n",
    "\n",
    "    layer2 = QLayer(n_qubits = 4,\n",
    "                    n_features = 4, \n",
    "                    n_targets = 4, \n",
    "                    encoder = Encoder(), \n",
    "                    ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                    sampler=Parity(),\n",
    "                    shots=0)\n",
    "\n",
    "    layer3 = QLayer(n_qubits = 4,\n",
    "                    n_features = 4, \n",
    "                    n_targets = 1, \n",
    "                    encoder = Encoder(), \n",
    "                    ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                    sampler=Parity(),\n",
    "                    shots=0)\n",
    "\n",
    "    layers = [layer1, layer2, layer3]\n",
    "    hybrid = NeuralNetwork(layers)\n",
    "    \n",
    "    hybrid_list.append([hybrid, x_train, y_train, False])\n",
    "\n",
    "hybrid_list[0][3] = True    \n",
    "    \n",
    "with mp.Pool(10) as p:\n",
    "    hybrid_list = p.map(parallel, hybrid_list)     \n",
    "    \n",
    "saver(hybrid_list, data_path(\"boston_hybrid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hybrid_list[0].loss[-1])\n",
    "y_pred = hybrid_list[0].predict(x_test)\n",
    "loss = np.mean((y_pred - y_test)**2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    dnn = sequential_dnn(dim = [13, 5, 5, 1])\n",
    "    dnn.train(x_train, y_train, epochs = 100)\n",
    "    dnn_list.append(dnn)\n",
    "    \n",
    "saver(dnn_list, data_path(\"boston_dnn_full\"))\n",
    "plt.plot(dnn_list[0].loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dnn_list[0].loss[-1])\n",
    "y_pred = dnn_list[0].predict(x_test)\n",
    "loss = np.mean((y_pred - y_test)**2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
