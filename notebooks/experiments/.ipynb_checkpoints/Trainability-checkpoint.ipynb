{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Constant Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = 0.5*np.ones((n,1))\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1_constant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2_constant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [1, 3, 1],\n",
    "                         lr = 0.1)\n",
    "    \n",
    "    dnn.train(x, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D_constant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [1, 5, 1],\n",
    "                         lr = 0.1)\n",
    "    \n",
    "    dnn.train(x, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=0, b=np.pi)\n",
    "x_dnn = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [2, 6, 1],\n",
    "                     lr = 0.1)\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=5000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 5\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "\n",
    "y = 0.5*np.ones((n**3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_constant_reps_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=0, b=np.pi)\n",
    "x_dnn = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "\n",
    "y = scaler(y, a=0.1, b=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 5,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_single_circuit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 1,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 2,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in range(10):\n",
    "    qnn = sequential_qnn(q_bits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)\n",
    "    qnn.train(x, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(10):\n",
    "    dnn = sequential_dnn(dim = [3, 6, 1],\n",
    "                     lr = 0.1)\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842cc707d9ab437ea5f018a1dd2e25e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08681820257953562\n",
      "epoch: 1, loss: 0.041834626318367055\n",
      "epoch: 2, loss: 0.030764183752753152\n",
      "epoch: 3, loss: 0.030854154510674633\n",
      "epoch: 4, loss: 0.027349185144395563\n",
      "epoch: 5, loss: 0.024184273954449418\n",
      "epoch: 6, loss: 0.023463810765264218\n",
      "epoch: 7, loss: 0.02284640542904513\n",
      "epoch: 8, loss: 0.021705359648388052\n",
      "epoch: 9, loss: 0.020957780899492212\n",
      "epoch: 10, loss: 0.02056930225577983\n",
      "epoch: 11, loss: 0.020036360574940786\n",
      "epoch: 12, loss: 0.020031432065865787\n",
      "epoch: 13, loss: 0.01968681333393826\n",
      "epoch: 14, loss: 0.019643842219174652\n",
      "epoch: 15, loss: 0.019295967401079906\n",
      "epoch: 16, loss: 0.01857926016724448\n",
      "epoch: 17, loss: 0.018049415934551557\n",
      "epoch: 18, loss: 0.01731076541284274\n",
      "epoch: 19, loss: 0.016759600901455105\n",
      "epoch: 20, loss: 0.016418606605457936\n",
      "epoch: 21, loss: 0.01600053785377297\n",
      "epoch: 22, loss: 0.015413899563708377\n",
      "epoch: 23, loss: 0.015064040220536909\n",
      "epoch: 24, loss: 0.014933740495407629\n",
      "epoch: 25, loss: 0.014023004359444506\n",
      "epoch: 26, loss: 0.013388068513629513\n",
      "epoch: 27, loss: 0.012795032733355808\n",
      "epoch: 28, loss: 0.012430469527118682\n",
      "epoch: 29, loss: 0.012056835612738974\n",
      "epoch: 30, loss: 0.011775374560784209\n",
      "epoch: 31, loss: 0.011204247916537513\n",
      "epoch: 32, loss: 0.011281309343334954\n",
      "epoch: 33, loss: 0.010890239040481657\n",
      "epoch: 34, loss: 0.010873192419011989\n",
      "epoch: 35, loss: 0.011004508272162336\n",
      "epoch: 36, loss: 0.010338663285826218\n",
      "epoch: 37, loss: 0.010703310422418328\n",
      "epoch: 38, loss: 0.009922069600720585\n",
      "epoch: 39, loss: 0.010129584870746298\n",
      "epoch: 40, loss: 0.009744998066165158\n",
      "epoch: 41, loss: 0.0099778498387657\n",
      "epoch: 42, loss: 0.00978111935187768\n",
      "epoch: 43, loss: 0.009846530868928916\n",
      "epoch: 44, loss: 0.009881740010149201\n",
      "epoch: 45, loss: 0.009512176891745727\n",
      "epoch: 46, loss: 0.009489710098990363\n",
      "epoch: 47, loss: 0.009480285807231687\n",
      "epoch: 48, loss: 0.009694738235791351\n",
      "epoch: 49, loss: 0.009540389231059298\n",
      "epoch: 50, loss: 0.009190631123471845\n",
      "epoch: 51, loss: 0.009155381470556678\n",
      "epoch: 52, loss: 0.00900402047211261\n",
      "epoch: 53, loss: 0.009004460068843005\n",
      "epoch: 54, loss: 0.008935598795460452\n",
      "epoch: 55, loss: 0.008954056419994902\n",
      "epoch: 56, loss: 0.00892342373455469\n",
      "epoch: 57, loss: 0.008913858250659223\n",
      "epoch: 58, loss: 0.008898269292504549\n",
      "epoch: 59, loss: 0.00910181352140276\n",
      "epoch: 60, loss: 0.008999420201184315\n",
      "epoch: 61, loss: 0.008173223091544403\n",
      "epoch: 62, loss: 0.008415304211718067\n",
      "epoch: 63, loss: 0.00859869058556821\n",
      "epoch: 64, loss: 0.008643336887676274\n",
      "epoch: 65, loss: 0.008397571600950973\n",
      "epoch: 66, loss: 0.00886154083458874\n",
      "epoch: 67, loss: 0.008527802055761308\n",
      "epoch: 68, loss: 0.008434054651448161\n",
      "epoch: 69, loss: 0.0083574536661612\n",
      "epoch: 70, loss: 0.008522163898861844\n",
      "epoch: 71, loss: 0.008552261788075308\n",
      "epoch: 72, loss: 0.008335250365296734\n",
      "epoch: 73, loss: 0.008237819902544206\n",
      "epoch: 74, loss: 0.008378052975256026\n",
      "epoch: 75, loss: 0.00855278720320614\n",
      "epoch: 76, loss: 0.008401217989122052\n",
      "epoch: 77, loss: 0.008566040988581285\n",
      "epoch: 78, loss: 0.008234710696719235\n",
      "epoch: 79, loss: 0.008065781558470486\n",
      "epoch: 80, loss: 0.008398321287853007\n",
      "epoch: 81, loss: 0.008381695985346138\n",
      "epoch: 82, loss: 0.008416811840261329\n",
      "epoch: 83, loss: 0.008542330955394238\n",
      "epoch: 84, loss: 0.008428577934230966\n",
      "epoch: 85, loss: 0.00815431557156853\n",
      "epoch: 86, loss: 0.008384190465848377\n",
      "epoch: 87, loss: 0.008142956784707979\n",
      "epoch: 88, loss: 0.008132232204322066\n",
      "epoch: 89, loss: 0.008533932553666891\n",
      "epoch: 90, loss: 0.008037132838312842\n",
      "epoch: 91, loss: 0.008242586726166463\n",
      "epoch: 92, loss: 0.008104804169640946\n",
      "epoch: 93, loss: 0.008155236838584953\n",
      "epoch: 94, loss: 0.008124046593779402\n",
      "epoch: 95, loss: 0.00863233122008031\n",
      "epoch: 96, loss: 0.008210337737563375\n",
      "epoch: 97, loss: 0.007919114155946662\n",
      "epoch: 98, loss: 0.008279165303947835\n",
      "epoch: 99, loss: 0.008388126694352656\n",
      "epoch: 100, loss: 0.007955345823725922\n",
      "epoch: 101, loss: 0.008320806764386901\n",
      "epoch: 102, loss: 0.00794017903224525\n",
      "epoch: 103, loss: 0.008086764358080705\n",
      "epoch: 104, loss: 0.007632673711297035\n",
      "epoch: 105, loss: 0.008184168844600952\n",
      "epoch: 106, loss: 0.0077564325751146085\n",
      "epoch: 107, loss: 0.007946753084555799\n",
      "epoch: 108, loss: 0.00798411580699582\n",
      "epoch: 109, loss: 0.008060057250550586\n",
      "epoch: 110, loss: 0.008170274056728693\n",
      "epoch: 111, loss: 0.00783861938288841\n",
      "epoch: 112, loss: 0.008544709056058232\n",
      "epoch: 113, loss: 0.008014915178779973\n",
      "epoch: 114, loss: 0.008105401241630545\n",
      "epoch: 115, loss: 0.007940866578846503\n",
      "epoch: 116, loss: 0.00799821884498699\n",
      "epoch: 117, loss: 0.008273013943596023\n",
      "epoch: 118, loss: 0.007637248976277422\n",
      "epoch: 119, loss: 0.007960920950069767\n",
      "epoch: 120, loss: 0.007898446010976043\n",
      "epoch: 121, loss: 0.007881169018619575\n",
      "epoch: 122, loss: 0.00785387251036899\n",
      "epoch: 123, loss: 0.007537283139540161\n",
      "epoch: 124, loss: 0.007467055775800574\n",
      "epoch: 125, loss: 0.00793982643776672\n",
      "epoch: 126, loss: 0.007920801881188785\n",
      "epoch: 127, loss: 0.007544177009223092\n",
      "epoch: 128, loss: 0.007635690138473191\n",
      "epoch: 129, loss: 0.008139104161296908\n",
      "epoch: 130, loss: 0.007827775270285395\n",
      "epoch: 131, loss: 0.007801902980475128\n",
      "epoch: 132, loss: 0.008305770687584466\n",
      "epoch: 133, loss: 0.0077613105939506755\n",
      "epoch: 134, loss: 0.008101354896713849\n",
      "epoch: 135, loss: 0.008197488942686636\n",
      "epoch: 136, loss: 0.007754824207605702\n",
      "epoch: 137, loss: 0.008222386567117577\n",
      "epoch: 138, loss: 0.008013337176444655\n",
      "epoch: 139, loss: 0.007881364360689506\n",
      "epoch: 140, loss: 0.008279385464849144\n",
      "epoch: 141, loss: 0.007833370116968712\n",
      "epoch: 142, loss: 0.007796550054835308\n",
      "epoch: 143, loss: 0.00778350812818122\n",
      "epoch: 144, loss: 0.007886411481829123\n",
      "epoch: 145, loss: 0.007536427328185701\n",
      "epoch: 146, loss: 0.007728356585229805\n",
      "epoch: 147, loss: 0.007919565632168564\n",
      "epoch: 148, loss: 0.007773443033008813\n",
      "epoch: 149, loss: 0.007919101917480473\n",
      "epoch: 150, loss: 0.007836931846346571\n",
      "epoch: 151, loss: 0.007979970438638004\n",
      "epoch: 152, loss: 0.007721440607229219\n",
      "epoch: 153, loss: 0.007810305221010082\n",
      "epoch: 154, loss: 0.008108142280667877\n",
      "epoch: 155, loss: 0.007878462773911134\n",
      "epoch: 156, loss: 0.007766834545135984\n",
      "epoch: 157, loss: 0.00788489908593767\n",
      "epoch: 158, loss: 0.008158786761269967\n",
      "epoch: 159, loss: 0.00743523027815548\n",
      "epoch: 160, loss: 0.008189981947919355\n",
      "epoch: 161, loss: 0.007546823176592211\n",
      "epoch: 162, loss: 0.00774553388628167\n",
      "epoch: 163, loss: 0.0078002589762793135\n",
      "epoch: 164, loss: 0.007679685994428872\n",
      "epoch: 165, loss: 0.007894771908872167\n",
      "epoch: 166, loss: 0.007364966367892863\n",
      "epoch: 167, loss: 0.007872591667071776\n",
      "epoch: 168, loss: 0.007682787280255053\n",
      "epoch: 169, loss: 0.007996183221554026\n",
      "epoch: 170, loss: 0.00762770426616222\n",
      "epoch: 171, loss: 0.00811157461871555\n",
      "epoch: 172, loss: 0.007770552382556444\n",
      "epoch: 173, loss: 0.007851799076270281\n",
      "epoch: 174, loss: 0.00739428166082335\n",
      "epoch: 175, loss: 0.007939979290409186\n",
      "epoch: 176, loss: 0.007617046358572062\n",
      "epoch: 177, loss: 0.00791822080303828\n",
      "epoch: 178, loss: 0.0071934164434319225\n",
      "epoch: 179, loss: 0.007942677602171934\n",
      "epoch: 180, loss: 0.007464398484961745\n",
      "epoch: 181, loss: 0.007682890775814112\n",
      "epoch: 182, loss: 0.007596259199864506\n",
      "epoch: 183, loss: 0.007584662141103258\n",
      "epoch: 184, loss: 0.007738344832859921\n",
      "epoch: 185, loss: 0.007978696261290965\n",
      "epoch: 186, loss: 0.007402014194062927\n",
      "epoch: 187, loss: 0.007589365182980614\n",
      "epoch: 188, loss: 0.007558083407334049\n",
      "epoch: 189, loss: 0.00796934385697602\n",
      "epoch: 190, loss: 0.007479529113222815\n",
      "epoch: 191, loss: 0.0073099738952745645\n",
      "epoch: 192, loss: 0.007679215516350462\n",
      "epoch: 193, loss: 0.0077679998853042035\n",
      "epoch: 194, loss: 0.0076702303195824925\n",
      "epoch: 195, loss: 0.007625510464794173\n",
      "epoch: 196, loss: 0.007587644838367625\n",
      "epoch: 197, loss: 0.007524760724248818\n",
      "epoch: 198, loss: 0.007592898844782947\n",
      "epoch: 199, loss: 0.007584096574887293\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qnn_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1a5e7cc86b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainability_qnn_3D_deep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qnn_list' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(q_bits = [3, 4, 4],\n",
    "                     dim = [3, 4, 4, 1],\n",
    "                     reps = 2,\n",
    "                     backend=backend,\n",
    "                     shots=10000,\n",
    "                     lr = 0.1)\n",
    "\n",
    "qnn.train(x, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e6d6de929441888785e3fe947b9d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1187357473551163\n",
      "epoch: 1, loss: 0.11325189755098873\n",
      "epoch: 2, loss: 0.10773612170797335\n",
      "epoch: 3, loss: 0.10220606216008322\n",
      "epoch: 4, loss: 0.09668111987466756\n",
      "epoch: 5, loss: 0.09118268771555905\n",
      "epoch: 6, loss: 0.0857340358544433\n",
      "epoch: 7, loss: 0.08036007385699936\n",
      "epoch: 8, loss: 0.07508709720500706\n",
      "epoch: 9, loss: 0.06994250149843964\n",
      "epoch: 10, loss: 0.06495447240386261\n",
      "epoch: 11, loss: 0.06015160700942809\n",
      "epoch: 12, loss: 0.05556240862245742\n",
      "epoch: 13, loss: 0.051214661851951976\n",
      "epoch: 14, loss: 0.04713472942595149\n",
      "epoch: 15, loss: 0.0433468018192336\n",
      "epoch: 16, loss: 0.03987211949286894\n",
      "epoch: 17, loss: 0.03672818617766283\n",
      "epoch: 18, loss: 0.03392799611927549\n",
      "epoch: 19, loss: 0.03147930642230271\n",
      "epoch: 20, loss: 0.02938399659919193\n",
      "epoch: 21, loss: 0.0276375689388953\n",
      "epoch: 22, loss: 0.02622885169347271\n",
      "epoch: 23, loss: 0.02513996834124017\n",
      "epoch: 24, loss: 0.02434662729832\n",
      "epoch: 25, loss: 0.023818766242956115\n",
      "epoch: 26, loss: 0.023521554885945303\n",
      "epoch: 27, loss: 0.023416723117400333\n",
      "epoch: 28, loss: 0.023464142158978272\n",
      "epoch: 29, loss: 0.023623547718994737\n",
      "epoch: 30, loss: 0.023856264012553943\n",
      "epoch: 31, loss: 0.02412678086916449\n",
      "epoch: 32, loss: 0.024404056520459322\n",
      "epoch: 33, loss: 0.024662455260490913\n",
      "epoch: 34, loss: 0.024882273848777784\n",
      "epoch: 35, loss: 0.025049857265190472\n",
      "epoch: 36, loss: 0.02515734535280502\n",
      "epoch: 37, loss: 0.025202120617925594\n",
      "epoch: 38, loss: 0.025186041727229085\n",
      "epoch: 39, loss: 0.025114548582816094\n",
      "epoch: 40, loss: 0.024995716966763663\n",
      "epoch: 41, loss: 0.02483932777716886\n",
      "epoch: 42, loss: 0.024656001245411747\n",
      "epoch: 43, loss: 0.024456432432031435\n",
      "epoch: 44, loss: 0.024250751797731552\n",
      "epoch: 45, loss: 0.02404802398233719\n",
      "epoch: 46, loss: 0.02385588893347446\n",
      "epoch: 47, loss: 0.02368034194426278\n",
      "epoch: 48, loss: 0.023525642801512966\n",
      "epoch: 49, loss: 0.023394339068258208\n",
      "epoch: 50, loss: 0.023287384596223298\n",
      "epoch: 51, loss: 0.023204331799321617\n",
      "epoch: 52, loss: 0.02314357510093529\n",
      "epoch: 53, loss: 0.02310262328709924\n",
      "epoch: 54, loss: 0.023078380127346337\n",
      "epoch: 55, loss: 0.023067415323967593\n",
      "epoch: 56, loss: 0.023066211296568783\n",
      "epoch: 57, loss: 0.023071375145276463\n",
      "epoch: 58, loss: 0.023079809019636313\n",
      "epoch: 59, loss: 0.023088835760775986\n",
      "epoch: 60, loss: 0.0230962798680636\n",
      "epoch: 61, loss: 0.023100506438833435\n",
      "epoch: 62, loss: 0.023100422689396265\n",
      "epoch: 63, loss: 0.02309544799888997\n",
      "epoch: 64, loss: 0.023085459179375965\n",
      "epoch: 65, loss: 0.023070717945204575\n",
      "epoch: 66, loss: 0.023051787420065546\n",
      "epoch: 67, loss: 0.02302944406712963\n",
      "epoch: 68, loss: 0.02300459073386663\n",
      "epoch: 69, loss: 0.022978175635933937\n",
      "epoch: 70, loss: 0.022951121122196726\n",
      "epoch: 71, loss: 0.022924265016903898\n",
      "epoch: 72, loss: 0.022898316272590338\n",
      "epoch: 73, loss: 0.022873825633440897\n",
      "epoch: 74, loss: 0.022851171047005073\n",
      "epoch: 75, loss: 0.022830556713121904\n",
      "epoch: 76, loss: 0.022812023958903537\n",
      "epoch: 77, loss: 0.02279547160643123\n",
      "epoch: 78, loss: 0.022780683174046263\n",
      "epoch: 79, loss: 0.02276735812901566\n",
      "epoch: 80, loss: 0.022755144481715558\n",
      "epoch: 81, loss: 0.022743670258921204\n",
      "epoch: 82, loss: 0.022732571784379086\n",
      "epoch: 83, loss: 0.022721517187893413\n",
      "epoch: 84, loss: 0.022710224113972614\n",
      "epoch: 85, loss: 0.02269847116102321\n",
      "epoch: 86, loss: 0.022686103108670527\n",
      "epoch: 87, loss: 0.02267303044725675\n",
      "epoch: 88, loss: 0.022659224082506684\n",
      "epoch: 89, loss: 0.022644706333250973\n",
      "epoch: 90, loss: 0.022629539465681723\n",
      "epoch: 91, loss: 0.022613813019085036\n",
      "epoch: 92, loss: 0.02259763108934576\n",
      "epoch: 93, loss: 0.022581100568262076\n",
      "epoch: 94, loss: 0.022564321113260292\n",
      "epoch: 95, loss: 0.022547377369181887\n",
      "epoch: 96, loss: 0.022530333706056154\n",
      "epoch: 97, loss: 0.022513231495768067\n",
      "epoch: 98, loss: 0.022496088743427362\n",
      "epoch: 99, loss: 0.02247890172791959\n",
      "epoch: 100, loss: 0.02246164819694943\n",
      "epoch: 101, loss: 0.022444291606015434\n",
      "epoch: 102, loss: 0.022426785884785274\n",
      "epoch: 103, loss: 0.022409080251294615\n",
      "epoch: 104, loss: 0.022391123664836646\n",
      "epoch: 105, loss: 0.022372868601602838\n",
      "epoch: 106, loss: 0.022354273942104038\n",
      "epoch: 107, loss: 0.02233530686586187\n",
      "epoch: 108, loss: 0.022315943747937786\n",
      "epoch: 109, loss: 0.02229617013658805\n",
      "epoch: 110, loss: 0.022275979956891442\n",
      "epoch: 111, loss: 0.02225537412901388\n",
      "epoch: 112, loss: 0.022234358811380122\n",
      "epoch: 113, loss: 0.022212943479794543\n",
      "epoch: 114, loss: 0.022191139036357714\n",
      "epoch: 115, loss: 0.022168956110795737\n",
      "epoch: 116, loss: 0.022146403676107946\n",
      "epoch: 117, loss: 0.022123488054981055\n",
      "epoch: 118, loss: 0.022100212347729558\n",
      "epoch: 119, loss: 0.022076576270563173\n",
      "epoch: 120, loss: 0.022052576357893643\n",
      "epoch: 121, loss: 0.02202820645633543\n",
      "epoch: 122, loss: 0.022003458422132492\n",
      "epoch: 123, loss: 0.021978322928034696\n",
      "epoch: 124, loss: 0.021952790289309826\n",
      "epoch: 125, loss: 0.021926851230030384\n",
      "epoch: 126, loss: 0.021900497527923\n",
      "epoch: 127, loss: 0.021873722496554632\n",
      "epoch: 128, loss: 0.02184652128508235\n",
      "epoch: 129, loss: 0.02181889099604853\n",
      "epoch: 130, loss: 0.021790830638989785\n",
      "epoch: 131, loss: 0.021762340950684952\n",
      "epoch: 132, loss: 0.021733424121010442\n",
      "epoch: 133, loss: 0.021704083466482685\n",
      "epoch: 134, loss: 0.021674323092042553\n",
      "epoch: 135, loss: 0.021644147576278108\n",
      "epoch: 136, loss: 0.021613561707167328\n",
      "epoch: 137, loss: 0.02158257028576401\n",
      "epoch: 138, loss: 0.021551178005250488\n",
      "epoch: 139, loss: 0.021519389403517306\n",
      "epoch: 140, loss: 0.02148720887975605\n",
      "epoch: 141, loss: 0.021454640760042\n",
      "epoch: 142, loss: 0.02142168939380962\n",
      "epoch: 143, loss: 0.021388359262466135\n",
      "epoch: 144, loss: 0.021354655082876674\n",
      "epoch: 145, loss: 0.021320581891627437\n",
      "epoch: 146, loss: 0.02128614510025278\n",
      "epoch: 147, loss: 0.021251350516380677\n",
      "epoch: 148, loss: 0.021216204330417373\n",
      "epoch: 149, loss: 0.02118071307145814\n",
      "epoch: 150, loss: 0.021144883539204608\n",
      "epoch: 151, loss: 0.021108722720577417\n",
      "epoch: 152, loss: 0.02107223770037681\n",
      "epoch: 153, loss: 0.02103543557485272\n",
      "epoch: 154, loss: 0.020998323375604585\n",
      "epoch: 155, loss: 0.020960908009126828\n",
      "epoch: 156, loss: 0.02092319621487271\n",
      "epoch: 157, loss: 0.02088519454225216\n",
      "epoch: 158, loss: 0.020846909344791578\n",
      "epoch: 159, loss: 0.02080834678798547\n",
      "epoch: 160, loss: 0.020769512866296337\n",
      "epoch: 161, loss: 0.020730413424355977\n",
      "epoch: 162, loss: 0.020691054177651852\n",
      "epoch: 163, loss: 0.020651440728737355\n",
      "epoch: 164, loss: 0.020611578576130024\n",
      "epoch: 165, loss: 0.020571473114375526\n",
      "epoch: 166, loss: 0.020531129625078418\n",
      "epoch: 167, loss: 0.020490553259874634\n",
      "epoch: 168, loss: 0.02044974901722622\n",
      "epoch: 169, loss: 0.020408721715484874\n",
      "epoch: 170, loss: 0.02036747596487766\n",
      "epoch: 171, loss: 0.02032601614094218\n",
      "epoch: 172, loss: 0.020284346361545975\n",
      "epoch: 173, loss: 0.020242470469053232\n",
      "epoch: 174, loss: 0.02020039201855234\n",
      "epoch: 175, loss: 0.02015811427242412\n",
      "epoch: 176, loss: 0.020115640200992012\n",
      "epoch: 177, loss: 0.020072972488606844\n",
      "epoch: 178, loss: 0.020030113544306\n",
      "epoch: 179, loss: 0.019987065516147744\n",
      "epoch: 180, loss: 0.019943830308430208\n",
      "epoch: 181, loss: 0.019900409601217466\n",
      "epoch: 182, loss: 0.0198568048718598\n",
      "epoch: 183, loss: 0.019813017418458475\n",
      "epoch: 184, loss: 0.019769048385441336\n",
      "epoch: 185, loss: 0.01972489879155084\n",
      "epoch: 186, loss: 0.019680569560583838\n",
      "epoch: 187, loss: 0.019636061555161257\n",
      "epoch: 188, loss: 0.019591375613660138\n",
      "epoch: 189, loss: 0.01954651259023524\n",
      "epoch: 190, loss: 0.01950147339762331\n",
      "epoch: 191, loss: 0.019456259052192757\n",
      "epoch: 192, loss: 0.019410870720503567\n",
      "epoch: 193, loss: 0.019365309766497082\n",
      "epoch: 194, loss: 0.019319577798356594\n",
      "epoch: 195, loss: 0.019273676714067454\n",
      "epoch: 196, loss: 0.019227608744755334\n",
      "epoch: 197, loss: 0.01918137649497793\n",
      "epoch: 198, loss: 0.019134982979273765\n",
      "epoch: 199, loss: 0.019088431654412825\n",
      "epoch: 200, loss: 0.01904172644693256\n",
      "epoch: 201, loss: 0.018994871775667825\n",
      "epoch: 202, loss: 0.018947872569087542\n",
      "epoch: 203, loss: 0.01890073427733309\n",
      "epoch: 204, loss: 0.018853462878916514\n",
      "epoch: 205, loss: 0.01880606488208549\n",
      "epoch: 206, loss: 0.01875854732090377\n",
      "epoch: 207, loss: 0.018710917746136552\n",
      "epoch: 208, loss: 0.018663184211073935\n",
      "epoch: 209, loss: 0.018615355252474958\n",
      "epoch: 210, loss: 0.01856743986686872\n",
      "epoch: 211, loss: 0.018519447482506172\n",
      "epoch: 212, loss: 0.018471387927312673\n",
      "epoch: 213, loss: 0.01842327139324438\n",
      "epoch: 214, loss: 0.01837510839749853\n",
      "epoch: 215, loss: 0.018326909741067807\n",
      "epoch: 216, loss: 0.018278686465162217\n",
      "epoch: 217, loss: 0.018230449806050302\n",
      "epoch: 218, loss: 0.018182211148896503\n",
      "epoch: 219, loss: 0.01813398198119425\n",
      "epoch: 220, loss: 0.01808577384641557\n",
      "epoch: 221, loss: 0.018037598298515613\n",
      "epoch: 222, loss: 0.01798946685794143\n",
      "epoch: 223, loss: 0.01794139096979347\n",
      "epoch: 224, loss: 0.01789338196477049\n",
      "epoch: 225, loss: 0.017845451023488407\n",
      "epoch: 226, loss: 0.017797609144697198\n",
      "epoch: 227, loss: 0.017749867117825915\n",
      "epoch: 228, loss: 0.017702235500165458\n",
      "epoch: 229, loss: 0.01765472459885648\n",
      "epoch: 230, loss: 0.01760734445769306\n",
      "epoch: 231, loss: 0.017560104848591194\n",
      "epoch: 232, loss: 0.01751301526741515\n",
      "epoch: 233, loss: 0.01746608493371549\n",
      "epoch: 234, loss: 0.017419322793818873\n",
      "epoch: 235, loss: 0.01737273752662925\n",
      "epoch: 236, loss: 0.01732633755145709\n",
      "epoch: 237, loss: 0.017280131037188378\n",
      "epoch: 238, loss: 0.01723412591213682\n",
      "epoch: 239, loss: 0.017188329873985452\n",
      "epoch: 240, loss: 0.017142750399311544\n",
      "epoch: 241, loss: 0.01709739475229247\n",
      "epoch: 242, loss: 0.017052269992302475\n",
      "epoch: 243, loss: 0.01700738298022289\n",
      "epoch: 244, loss: 0.016962740383394383\n",
      "epoch: 245, loss: 0.016918348679234557\n",
      "epoch: 246, loss: 0.016874214157623583\n",
      "epoch: 247, loss: 0.016830342922222474\n",
      "epoch: 248, loss: 0.01678674089093289\n",
      "epoch: 249, loss: 0.016743413795733506\n",
      "epoch: 250, loss: 0.016700367182138322\n",
      "epoch: 251, loss: 0.016657606408517336\n",
      "epoch: 252, loss: 0.01661513664550304\n",
      "epoch: 253, loss: 0.016572962875678603\n",
      "epoch: 254, loss: 0.016531089893708318\n",
      "epoch: 255, loss: 0.016489522307030366\n",
      "epoch: 256, loss: 0.016448264537188447\n",
      "epoch: 257, loss: 0.016407320821835155\n",
      "epoch: 258, loss: 0.01636669521739819\n",
      "epoch: 259, loss: 0.016326391602362808\n",
      "epoch: 260, loss: 0.01628641368109216\n",
      "epoch: 261, loss: 0.016246764988082165\n",
      "epoch: 262, loss: 0.01620744889253083\n",
      "epoch: 263, loss: 0.01616846860309278\n",
      "epoch: 264, loss: 0.016129827172688432\n",
      "epoch: 265, loss: 0.016091527503242737\n",
      "epoch: 266, loss: 0.0160535723502396\n",
      "epoch: 267, loss: 0.01601596432699345\n",
      "epoch: 268, loss: 0.01597870590855795\n",
      "epoch: 269, loss: 0.015941799435211724\n",
      "epoch: 270, loss: 0.015905247115481164\n",
      "epoch: 271, loss: 0.01586905102868003\n",
      "epoch: 272, loss: 0.015833213126963598\n",
      "epoch: 273, loss: 0.015797735236911147\n",
      "epoch: 274, loss: 0.01576261906066409\n",
      "epoch: 275, loss: 0.015727866176658216\n",
      "epoch: 276, loss: 0.015693478039996635\n",
      "epoch: 277, loss: 0.01565945598251566\n",
      "epoch: 278, loss: 0.01562580121259904\n",
      "epoch: 279, loss: 0.01559251481479669\n",
      "epoch: 280, loss: 0.015559597749302736\n",
      "epoch: 281, loss: 0.015527050851344804\n",
      "epoch: 282, loss: 0.015494874830532197\n",
      "epoch: 283, loss: 0.015463070270205192\n",
      "epoch: 284, loss: 0.015431637626822245\n",
      "epoch: 285, loss: 0.0154005772294157\n",
      "epoch: 286, loss: 0.015369889279141118\n",
      "epoch: 287, loss: 0.015339573848939992\n",
      "epoch: 288, loss: 0.015309630883331015\n",
      "epoch: 289, loss: 0.015280060198341132\n",
      "epoch: 290, loss: 0.015250861481584466\n",
      "epoch: 291, loss: 0.01522203429249469\n",
      "epoch: 292, loss: 0.015193578062714617\n",
      "epoch: 293, loss: 0.015165492096645363\n",
      "epoch: 294, loss: 0.01513777557215661\n",
      "epoch: 295, loss: 0.01511042754145863\n",
      "epoch: 296, loss: 0.01508344693213656\n",
      "epoch: 297, loss: 0.015056832548346711\n",
      "epoch: 298, loss: 0.015030583072174631\n",
      "epoch: 299, loss: 0.015004697065153996\n",
      "epoch: 300, loss: 0.014979172969945121\n",
      "epoch: 301, loss: 0.014954009112171148\n",
      "epoch: 302, loss: 0.014929203702409214\n",
      "epoch: 303, loss: 0.014904754838333048\n",
      "epoch: 304, loss: 0.014880660507002335\n",
      "epoch: 305, loss: 0.01485691858729298\n",
      "epoch: 306, loss: 0.014833526852461297\n",
      "epoch: 307, loss: 0.01481048297283368\n",
      "epoch: 308, loss: 0.014787784518612304\n",
      "epoch: 309, loss: 0.014765428962786155\n",
      "epoch: 310, loss: 0.014743413684135826\n",
      "epoch: 311, loss: 0.014721735970319751\n",
      "epoch: 312, loss: 0.01470039302102911\n",
      "epoch: 313, loss: 0.014679381951198342\n",
      "epoch: 314, loss: 0.014658699794258424\n",
      "epoch: 315, loss: 0.014638343505420178\n",
      "epoch: 316, loss: 0.014618309964975672\n",
      "epoch: 317, loss: 0.014598595981606363\n",
      "epoch: 318, loss: 0.01457919829568776\n",
      "epoch: 319, loss: 0.01456011358258139\n",
      "epoch: 320, loss: 0.014541338455906147\n",
      "epoch: 321, loss: 0.014522869470782353\n",
      "epoch: 322, loss: 0.014504703127043259\n",
      "epoch: 323, loss: 0.014486835872409846\n",
      "epoch: 324, loss: 0.014469264105626302\n",
      "epoch: 325, loss: 0.01445198417955452\n",
      "epoch: 326, loss: 0.014434992404227128\n",
      "epoch: 327, loss: 0.014418285049859604\n",
      "epoch: 328, loss: 0.014401858349822607\n",
      "epoch: 329, loss: 0.014385708503576698\n",
      "epoch: 330, loss: 0.014369831679571702\n",
      "epoch: 331, loss: 0.014354224018113808\n",
      "epoch: 332, loss: 0.014338881634203316\n",
      "epoch: 333, loss: 0.014323800620346328\n",
      "epoch: 334, loss: 0.014308977049343556\n",
      "epoch: 335, loss: 0.01429440697705925\n",
      "epoch: 336, loss: 0.014280086445173045\n",
      "epoch: 337, loss: 0.01426601148391725\n",
      "epoch: 338, loss: 0.01425217811480158\n",
      "epoch: 339, loss: 0.014238582353326949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 340, loss: 0.014225220211689428\n",
      "epoch: 341, loss: 0.014212087701474755\n",
      "epoch: 342, loss: 0.01419918083634343\n",
      "epoch: 343, loss: 0.014186495634705556\n",
      "epoch: 344, loss: 0.014174028122384157\n",
      "epoch: 345, loss: 0.014161774335264945\n",
      "epoch: 346, loss: 0.014149730321930029\n",
      "epoch: 347, loss: 0.014137892146272323\n",
      "epoch: 348, loss: 0.014126255890087038\n",
      "epoch: 349, loss: 0.014114817655635978\n",
      "epoch: 350, loss: 0.014103573568180023\n",
      "epoch: 351, loss: 0.014092519778474692\n",
      "epoch: 352, loss: 0.014081652465223445\n",
      "epoch: 353, loss: 0.014070967837482993\n",
      "epoch: 354, loss: 0.014060462137014848\n",
      "epoch: 355, loss: 0.014050131640577047\n",
      "epoch: 356, loss: 0.014039972662150112\n",
      "epoch: 357, loss: 0.014029981555091253\n",
      "epoch: 358, loss: 0.014020154714210887\n",
      "epoch: 359, loss: 0.014010488577765852\n",
      "epoch: 360, loss: 0.014000979629363811\n",
      "epoch: 361, loss: 0.01399162439977372\n",
      "epoch: 362, loss: 0.013982419468637557\n",
      "epoch: 363, loss: 0.013973361466078934\n",
      "epoch: 364, loss: 0.013964447074204668\n",
      "epoch: 365, loss: 0.013955673028495845\n",
      "epoch: 366, loss: 0.013947036119085475\n",
      "epoch: 367, loss: 0.013938533191920267\n",
      "epoch: 368, loss: 0.013930161149804792\n",
      "epoch: 369, loss: 0.013921916953326575\n",
      "epoch: 370, loss: 0.013913797621661532\n",
      "epoch: 371, loss: 0.013905800233259442\n",
      "epoch: 372, loss: 0.013897921926409859\n",
      "epoch: 373, loss: 0.01389015989968935\n",
      "epoch: 374, loss: 0.013882511412291385\n",
      "epoch: 375, loss: 0.013874973784240798\n",
      "epoch: 376, loss: 0.013867544396495126\n",
      "epoch: 377, loss: 0.013860220690935576\n",
      "epoch: 378, loss: 0.013853000170250764\n",
      "epoch: 379, loss: 0.013845880397716755\n",
      "epoch: 380, loss: 0.013838858996877264\n",
      "epoch: 381, loss: 0.013831933651128128\n",
      "epoch: 382, loss: 0.013825102103210493\n",
      "epoch: 383, loss: 0.013818362154617304\n",
      "epoch: 384, loss: 0.013811711664917974\n",
      "epoch: 385, loss: 0.013805148551006164\n",
      "epoch: 386, loss: 0.013798670786275787\n",
      "epoch: 387, loss: 0.013792276399730427\n",
      "epoch: 388, loss: 0.013785963475031415\n",
      "epoch: 389, loss: 0.013779730149489813\n",
      "epoch: 390, loss: 0.01377357461300757\n",
      "epoch: 391, loss: 0.013767495106973128\n",
      "epoch: 392, loss: 0.013761489923116564\n",
      "epoch: 393, loss: 0.013755557402329453\n",
      "epoch: 394, loss: 0.013749695933454363\n",
      "epoch: 395, loss: 0.013743903952048902\n",
      "epoch: 396, loss: 0.013738179939129024\n",
      "epoch: 397, loss: 0.013732522419896141\n",
      "epoch: 398, loss: 0.013726929962452464\n",
      "epoch: 399, loss: 0.013721401176508758\n",
      "epoch: 400, loss: 0.013715934712088556\n",
      "epoch: 401, loss: 0.013710529258232605\n",
      "epoch: 402, loss: 0.013705183541707145\n",
      "epoch: 403, loss: 0.013699896325719475\n",
      "epoch: 404, loss: 0.013694666408643842\n",
      "epoch: 405, loss: 0.013689492622760724\n",
      "epoch: 406, loss: 0.013684373833012117\n",
      "epoch: 407, loss: 0.013679308935775415\n",
      "epoch: 408, loss: 0.013674296857658053\n",
      "epoch: 409, loss: 0.013669336554315072\n",
      "epoch: 410, loss: 0.013664427009291371\n",
      "epoch: 411, loss: 0.013659567232890298\n",
      "epoch: 412, loss: 0.013654756261070036\n",
      "epoch: 413, loss: 0.013649993154368953\n",
      "epoch: 414, loss: 0.013645276996861001\n",
      "epoch: 415, loss: 0.013640606895142028\n",
      "epoch: 416, loss: 0.013635981977347613\n",
      "epoch: 417, loss: 0.013631401392203025\n",
      "epoch: 418, loss: 0.013626864308105597\n",
      "epoch: 419, loss: 0.013622369912239776\n",
      "epoch: 420, loss: 0.013617917409724853\n",
      "epoch: 421, loss: 0.013613506022795386\n",
      "epoch: 422, loss: 0.013609134990014092\n",
      "epoch: 423, loss: 0.013604803565516922\n",
      "epoch: 424, loss: 0.013600511018289954\n",
      "epoch: 425, loss: 0.013596256631477587\n",
      "epoch: 426, loss: 0.013592039701721514\n",
      "epoch: 427, loss: 0.013587859538529813\n",
      "epoch: 428, loss: 0.013583715463675441\n",
      "epoch: 429, loss: 0.013579606810623424\n",
      "epoch: 430, loss: 0.01357553292398589\n",
      "epoch: 431, loss: 0.013571493159004131\n",
      "epoch: 432, loss: 0.013567486881056792\n",
      "epoch: 433, loss: 0.013563513465193322\n",
      "epoch: 434, loss: 0.013559572295691722\n",
      "epoch: 435, loss: 0.013555662765639678\n",
      "epoch: 436, loss: 0.013551784276538152\n",
      "epoch: 437, loss: 0.013547936237926429\n",
      "epoch: 438, loss: 0.01354411806702776\n",
      "epoch: 439, loss: 0.013540329188414578\n",
      "epoch: 440, loss: 0.01353656903369245\n",
      "epoch: 441, loss: 0.013532837041201784\n",
      "epoch: 442, loss: 0.013529132655736438\n",
      "epoch: 443, loss: 0.013525455328278327\n",
      "epoch: 444, loss: 0.013521804515747285\n",
      "epoch: 445, loss: 0.013518179680765184\n",
      "epoch: 446, loss: 0.01351458029143368\n",
      "epoch: 447, loss: 0.013511005821124748\n",
      "epoch: 448, loss: 0.013507455748283291\n",
      "epoch: 449, loss: 0.013503929556241094\n",
      "epoch: 450, loss: 0.013500426733041507\n",
      "epoch: 451, loss: 0.013496946771274177\n",
      "epoch: 452, loss: 0.013493489167919238\n",
      "epoch: 453, loss: 0.013490053424200402\n",
      "epoch: 454, loss: 0.013486639045446424\n",
      "epoch: 455, loss: 0.013483245540960373\n",
      "epoch: 456, loss: 0.013479872423896342\n",
      "epoch: 457, loss: 0.013476519211143078\n",
      "epoch: 458, loss: 0.013473185423214148\n",
      "epoch: 459, loss: 0.013469870584144297\n",
      "epoch: 460, loss: 0.013466574221391607\n",
      "epoch: 461, loss: 0.013463295865745161\n",
      "epoch: 462, loss: 0.01346003505123795\n",
      "epoch: 463, loss: 0.013456791315064724\n",
      "epoch: 464, loss: 0.013453564197504568\n",
      "epoch: 465, loss: 0.013450353241848026\n",
      "epoch: 466, loss: 0.013447157994328518\n",
      "epoch: 467, loss: 0.013443978004057992\n",
      "epoch: 468, loss: 0.013440812822966596\n",
      "epoch: 469, loss: 0.013437662005746254\n",
      "epoch: 470, loss: 0.013434525109798146\n",
      "epoch: 471, loss: 0.013431401695183877\n",
      "epoch: 472, loss: 0.013428291324580403\n",
      "epoch: 473, loss: 0.013425193563238616\n",
      "epoch: 474, loss: 0.013422107978945542\n",
      "epoch: 475, loss: 0.013419034141990228\n",
      "epoch: 476, loss: 0.013415971625133243\n",
      "epoch: 477, loss: 0.013412920003579855\n",
      "epoch: 478, loss: 0.013409878854956925\n",
      "epoch: 479, loss: 0.013406847759293525\n",
      "epoch: 480, loss: 0.01340382629900538\n",
      "epoch: 481, loss: 0.013400814058883189\n",
      "epoch: 482, loss: 0.013397810626084862\n",
      "epoch: 483, loss: 0.013394815590131823\n",
      "epoch: 484, loss: 0.013391828542909394\n",
      "epoch: 485, loss: 0.013388849078671435\n",
      "epoch: 486, loss: 0.01338587679404926\n",
      "epoch: 487, loss: 0.013382911288065001\n",
      "epoch: 488, loss: 0.013379952162149485\n",
      "epoch: 489, loss: 0.013376999020164744\n",
      "epoch: 490, loss: 0.013374051468431276\n",
      "epoch: 491, loss: 0.013371109115760164\n",
      "epoch: 492, loss: 0.013368171573490155\n",
      "epoch: 493, loss: 0.013365238455529784\n",
      "epoch: 494, loss: 0.013362309378404706\n",
      "epoch: 495, loss: 0.013359383961310265\n",
      "epoch: 496, loss: 0.01335646182616941\n",
      "epoch: 497, loss: 0.013353542597696085\n",
      "epoch: 498, loss: 0.01335062590346407\n",
      "epoch: 499, loss: 0.013347711373981441\n",
      "epoch: 500, loss: 0.013344798642770618\n",
      "epoch: 501, loss: 0.013341887346454088\n",
      "epoch: 502, loss: 0.013338977124845773\n",
      "epoch: 503, loss: 0.013336067621048124\n",
      "epoch: 504, loss: 0.013333158481554842\n",
      "epoch: 505, loss: 0.01333024935635925\n",
      "epoch: 506, loss: 0.013327339899068237\n",
      "epoch: 507, loss: 0.013324429767021728\n",
      "epoch: 508, loss: 0.013321518621417519\n",
      "epoch: 509, loss: 0.013318606127441427\n",
      "epoch: 510, loss: 0.013315691954402532\n",
      "epoch: 511, loss: 0.013312775775873371\n",
      "epoch: 512, loss: 0.013309857269834876\n",
      "epoch: 513, loss: 0.013306936118825797\n",
      "epoch: 514, loss: 0.013304012010096315\n",
      "epoch: 515, loss: 0.013301084635765636\n",
      "epoch: 516, loss: 0.013298153692983069\n",
      "epoch: 517, loss: 0.013295218884092396\n",
      "epoch: 518, loss: 0.013292279916799023\n",
      "epoch: 519, loss: 0.013289336504339458\n",
      "epoch: 520, loss: 0.013286388365652747\n",
      "epoch: 521, loss: 0.013283435225553282\n",
      "epoch: 522, loss: 0.013280476814904463\n",
      "epoch: 523, loss: 0.013277512870792716\n",
      "epoch: 524, loss: 0.013274543136701176\n",
      "epoch: 525, loss: 0.013271567362682523\n",
      "epoch: 526, loss: 0.013268585305530246\n",
      "epoch: 527, loss: 0.013265596728947771\n",
      "epoch: 528, loss: 0.013262601403714682\n",
      "epoch: 529, loss: 0.01325959910784945\n",
      "epoch: 530, loss: 0.013256589626767885\n",
      "epoch: 531, loss: 0.0132535727534367\n",
      "epoch: 532, loss: 0.013250548288521425\n",
      "epoch: 533, loss: 0.013247516040528038\n",
      "epoch: 534, loss: 0.013244475825937614\n",
      "epoch: 535, loss: 0.013241427469333334\n",
      "epoch: 536, loss: 0.013238370803519264\n",
      "epoch: 537, loss: 0.013235305669630267\n",
      "epoch: 538, loss: 0.013232231917232539\n",
      "epoch: 539, loss: 0.013229149404414211\n",
      "epoch: 540, loss: 0.013226057997865638\n",
      "epoch: 541, loss: 0.013222957572948894\n",
      "epoch: 542, loss: 0.013219848013756207\n",
      "epoch: 543, loss: 0.013216729213157082\n",
      "epoch: 544, loss: 0.013213601072833866\n",
      "epoch: 545, loss: 0.013210463503305767\n",
      "epoch: 546, loss: 0.01320731642394121\n",
      "epoch: 547, loss: 0.0132041597629587\n",
      "epoch: 548, loss: 0.013200993457416297\n",
      "epoch: 549, loss: 0.013197817453190031\n",
      "epoch: 550, loss: 0.013194631704941576\n",
      "epoch: 551, loss: 0.013191436176075676\n",
      "epoch: 552, loss: 0.013188230838687818\n",
      "epoch: 553, loss: 0.013185015673502868\n",
      "epoch: 554, loss: 0.013181790669805314\n",
      "epoch: 555, loss: 0.013178555825361935\n",
      "epoch: 556, loss: 0.013175311146337805\n",
      "epoch: 557, loss: 0.013172056647206462\n",
      "epoch: 558, loss: 0.013168792350655321\n",
      "epoch: 559, loss: 0.013165518287487278\n",
      "epoch: 560, loss: 0.01316223449651953\n",
      "epoch: 561, loss: 0.013158941024480737\n",
      "epoch: 562, loss: 0.01315563792590746\n",
      "epoch: 563, loss: 0.01315232526304103\n",
      "epoch: 564, loss: 0.013149003105725705\n",
      "epoch: 565, loss: 0.01314567153130916\n",
      "epoch: 566, loss: 0.01314233062454616\n",
      "epoch: 567, loss: 0.013138980477506166\n",
      "epoch: 568, loss: 0.0131356211894856\n",
      "epoch: 569, loss: 0.013132252866925407\n",
      "epoch: 570, loss: 0.01312887562333425\n",
      "epoch: 571, loss: 0.013125489579217768\n",
      "epoch: 572, loss: 0.013122094862013992\n",
      "epoch: 573, loss: 0.013118691606034981\n",
      "epoch: 574, loss: 0.013115279952414446\n",
      "epoch: 575, loss: 0.013111860049061126\n",
      "epoch: 576, loss: 0.013108432050617281\n",
      "epoch: 577, loss: 0.013104996118421745\n",
      "epoch: 578, loss: 0.013101552420476557\n",
      "epoch: 579, loss: 0.01309810113141617\n",
      "epoch: 580, loss: 0.013094642432478064\n",
      "epoch: 581, loss: 0.013091176511473311\n",
      "epoch: 582, loss: 0.013087703562755655\n",
      "epoch: 583, loss: 0.013084223787187379\n",
      "epoch: 584, loss: 0.01308073739210022\n",
      "epoch: 585, loss: 0.013077244591249496\n",
      "epoch: 586, loss: 0.013073745604759348\n",
      "epoch: 587, loss: 0.013070240659057293\n",
      "epoch: 588, loss: 0.013066729986795823\n",
      "epoch: 589, loss: 0.013063213826759155\n",
      "epoch: 590, loss: 0.013059692423752982\n",
      "epoch: 591, loss: 0.013056166028475313\n",
      "epoch: 592, loss: 0.013052634897366418\n",
      "epoch: 593, loss: 0.01304909929243609\n",
      "epoch: 594, loss: 0.013045559481066556\n",
      "epoch: 595, loss: 0.013042015735789492\n",
      "epoch: 596, loss: 0.013038468334035817\n",
      "epoch: 597, loss: 0.013034917557857177\n",
      "epoch: 598, loss: 0.013031363693618205\n",
      "epoch: 599, loss: 0.013027807031659032\n",
      "epoch: 600, loss: 0.013024247865927631\n",
      "epoch: 601, loss: 0.013020686493582135\n",
      "epoch: 602, loss: 0.013017123214563296\n",
      "epoch: 603, loss: 0.013013558331137912\n",
      "epoch: 604, loss: 0.013009992147414096\n",
      "epoch: 605, loss: 0.013006424968829844\n",
      "epoch: 606, loss: 0.013002857101616593\n",
      "epoch: 607, loss: 0.012999288852239846\n",
      "epoch: 608, loss: 0.01299572052681927\n",
      "epoch: 609, loss: 0.012992152430531045\n",
      "epoch: 610, loss: 0.012988584866995464\n",
      "epoch: 611, loss: 0.012985018137653202\n",
      "epoch: 612, loss: 0.012981452541133769\n",
      "epoch: 613, loss: 0.01297788837262008\n",
      "epoch: 614, loss: 0.012974325923213017\n",
      "epoch: 615, loss: 0.012970765479300317\n",
      "epoch: 616, loss: 0.012967207321933892\n",
      "epoch: 617, loss: 0.01296365172622\n",
      "epoch: 618, loss: 0.012960098960726531\n",
      "epoch: 619, loss: 0.01295654928691173\n",
      "epoch: 620, loss: 0.01295300295857845\n",
      "epoch: 621, loss: 0.012949460221358036\n",
      "epoch: 622, loss: 0.012945921312227592\n",
      "epoch: 623, loss: 0.012942386459064188\n",
      "epoch: 624, loss: 0.012938855880239263\n",
      "epoch: 625, loss: 0.012935329784256147\n",
      "epoch: 626, loss: 0.012931808369433204\n",
      "epoch: 627, loss: 0.012928291823634662\n",
      "epoch: 628, loss: 0.012924780324050878\n",
      "epoch: 629, loss: 0.01292127403702913\n",
      "epoch: 630, loss: 0.012917773117955708\n",
      "epoch: 631, loss: 0.012914277711189522\n",
      "epoch: 632, loss: 0.01291078795004693\n",
      "epoch: 633, loss: 0.012907303956837098\n",
      "epoch: 634, loss: 0.012903825842946628\n",
      "epoch: 635, loss: 0.012900353708971888\n",
      "epoch: 636, loss: 0.012896887644896875\n",
      "epoch: 637, loss: 0.012893427730314237\n",
      "epoch: 638, loss: 0.012889974034686709\n",
      "epoch: 639, loss: 0.012886526617645703\n",
      "epoch: 640, loss: 0.012883085529323941\n",
      "epoch: 641, loss: 0.012879650810718348\n",
      "epoch: 642, loss: 0.012876222494079657\n",
      "epoch: 643, loss: 0.012872800603324828\n",
      "epoch: 644, loss: 0.012869385154468407\n",
      "epoch: 645, loss: 0.01286597615606896\n",
      "epoch: 646, loss: 0.012862573609686694\n",
      "epoch: 647, loss: 0.01285917751034851\n",
      "epoch: 648, loss: 0.012855787847016837\n",
      "epoch: 649, loss: 0.012852404603058714\n",
      "epoch: 650, loss: 0.012849027756711887\n",
      "epoch: 651, loss: 0.012845657281544706\n",
      "epoch: 652, loss: 0.012842293146907095\n",
      "epoch: 653, loss: 0.012838935318369946\n",
      "epoch: 654, loss: 0.012835583758150571\n",
      "epoch: 655, loss: 0.012832238425522278\n",
      "epoch: 656, loss: 0.012828899277206209\n",
      "epoch: 657, loss: 0.012825566267744023\n",
      "epoch: 658, loss: 0.01282223934985025\n",
      "epoch: 659, loss: 0.012818918474743356\n",
      "epoch: 660, loss: 0.012815603592454882\n",
      "epoch: 661, loss: 0.012812294652116307\n",
      "epoch: 662, loss: 0.01280899160222337\n",
      "epoch: 663, loss: 0.012805694390877996\n",
      "epoch: 664, loss: 0.012802402966008052\n",
      "epoch: 665, loss: 0.012799117275565312\n",
      "epoch: 666, loss: 0.012795837267702294\n",
      "epoch: 667, loss: 0.012792562890928649\n",
      "epoch: 668, loss: 0.012789294094247982\n",
      "epoch: 669, loss: 0.012786030827276008\n",
      "epoch: 670, loss: 0.012782773040341101\n",
      "epoch: 671, loss: 0.012779520684568313\n",
      "epoch: 672, loss: 0.01277627371194798\n",
      "epoch: 673, loss: 0.012773032075390066\n",
      "epoch: 674, loss: 0.012769795728765451\n",
      "epoch: 675, loss: 0.012766564626935251\n",
      "epoch: 676, loss: 0.012763338725769402\n",
      "epoch: 677, loss: 0.012760117982155582\n",
      "epoch: 678, loss: 0.012756902353999601\n",
      "epoch: 679, loss: 0.012753691800218266\n",
      "epoch: 680, loss: 0.012750486280725783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 681, loss: 0.012747285756414603\n",
      "epoch: 682, loss: 0.012744090189131639\n",
      "epoch: 683, loss: 0.012740899541650664\n",
      "epoch: 684, loss: 0.012737713777641706\n",
      "epoch: 685, loss: 0.0127345328616381\n",
      "epoch: 686, loss: 0.01273135675900189\n",
      "epoch: 687, loss: 0.012728185435888186\n",
      "epoch: 688, loss: 0.012725018859208917\n",
      "epoch: 689, loss: 0.01272185699659658\n",
      "epoch: 690, loss: 0.012718699816368275\n",
      "epoch: 691, loss: 0.012715547287490503\n",
      "epoch: 692, loss: 0.012712399379544903\n",
      "epoch: 693, loss: 0.012709256062695337\n",
      "epoch: 694, loss: 0.012706117307656407\n",
      "epoch: 695, loss: 0.012702983085663696\n",
      "epoch: 696, loss: 0.01269985336844575\n",
      "epoch: 697, loss: 0.012696728128198035\n",
      "epoch: 698, loss: 0.01269360733755879\n",
      "epoch: 699, loss: 0.012690490969586988\n",
      "epoch: 700, loss: 0.012687378997742268\n",
      "epoch: 701, loss: 0.012684271395866928\n",
      "epoch: 702, loss: 0.012681168138169914\n",
      "epoch: 703, loss: 0.012678069199212775\n",
      "epoch: 704, loss: 0.012674974553897536\n",
      "epoch: 705, loss: 0.012671884177456395\n",
      "epoch: 706, loss: 0.012668798045443193\n",
      "epoch: 707, loss: 0.01266571613372656\n",
      "epoch: 708, loss: 0.012662638418484619\n",
      "epoch: 709, loss: 0.012659564876201212\n",
      "epoch: 710, loss: 0.012656495483663472\n",
      "epoch: 711, loss: 0.0126534302179607\n",
      "epoch: 712, loss: 0.0126503690564844\n",
      "epoch: 713, loss: 0.012647311976929398\n",
      "epoch: 714, loss: 0.012644258957295916\n",
      "epoch: 715, loss: 0.01264120997589253\n",
      "epoch: 716, loss: 0.012638165011339861\n",
      "epoch: 717, loss: 0.012635124042574995\n",
      "epoch: 718, loss: 0.012632087048856409\n",
      "epoch: 719, loss: 0.012629054009769436\n",
      "epoch: 720, loss: 0.012626024905232134\n",
      "epoch: 721, loss: 0.012622999715501413\n",
      "epoch: 722, loss: 0.0126199784211795\n",
      "epoch: 723, loss: 0.012616961003220475\n",
      "epoch: 724, loss: 0.012613947442936983\n",
      "epoch: 725, loss: 0.012610937722006941\n",
      "epoch: 726, loss: 0.012607931822480205\n",
      "epoch: 727, loss: 0.012604929726785204\n",
      "epoch: 728, loss: 0.012601931417735399\n",
      "epoch: 729, loss: 0.012598936878535548\n",
      "epoch: 730, loss: 0.012595946092787786\n",
      "epoch: 731, loss: 0.0125929590444974\n",
      "epoch: 732, loss: 0.01258997571807828\n",
      "epoch: 733, loss: 0.012586996098358083\n",
      "epoch: 734, loss: 0.012584020170582966\n",
      "epoch: 735, loss: 0.012581047920421938\n",
      "epoch: 736, loss: 0.012578079333970813\n",
      "epoch: 737, loss: 0.012575114397755646\n",
      "epoch: 738, loss: 0.012572153098735779\n",
      "epoch: 739, loss: 0.01256919542430633\n",
      "epoch: 740, loss: 0.012566241362300232\n",
      "epoch: 741, loss: 0.01256329090098974\n",
      "epoch: 742, loss: 0.01256034402908739\n",
      "epoch: 743, loss: 0.012557400735746476\n",
      "epoch: 744, loss: 0.012554461010560942\n",
      "epoch: 745, loss: 0.012551524843564747\n",
      "epoch: 746, loss: 0.012548592225230696\n",
      "epoch: 747, loss: 0.012545663146468704\n",
      "epoch: 748, loss: 0.012542737598623519\n",
      "epoch: 749, loss: 0.01253981557347189\n",
      "epoch: 750, loss: 0.012536897063219218\n",
      "epoch: 751, loss: 0.012533982060495627\n",
      "epoch: 752, loss: 0.012531070558351544\n",
      "epoch: 753, loss: 0.012528162550252719\n",
      "epoch: 754, loss: 0.012525258030074751\n",
      "epoch: 755, loss: 0.012522356992097071\n",
      "epoch: 756, loss: 0.012519459430996464\n",
      "epoch: 757, loss: 0.012516565341840088\n",
      "epoch: 758, loss: 0.012513674720077991\n",
      "epoch: 759, loss: 0.012510787561535211\n",
      "epoch: 760, loss: 0.012507903862403401\n",
      "epoch: 761, loss: 0.012505023619231994\n",
      "epoch: 762, loss: 0.012502146828919014\n",
      "epoch: 763, loss: 0.012499273488701425\n",
      "epoch: 764, loss: 0.012496403596145123\n",
      "epoch: 765, loss: 0.01249353714913456\n",
      "epoch: 766, loss: 0.012490674145862008\n",
      "epoch: 767, loss: 0.012487814584816511\n",
      "epoch: 768, loss: 0.012484958464772513\n",
      "epoch: 769, loss: 0.01248210578477821\n",
      "epoch: 770, loss: 0.012479256544143617\n",
      "epoch: 771, loss: 0.012476410742428425\n",
      "epoch: 772, loss: 0.01247356837942958\n",
      "epoch: 773, loss: 0.012470729455168715\n",
      "epoch: 774, loss: 0.012467893969879358\n",
      "epoch: 775, loss: 0.012465061923994028\n",
      "epoch: 776, loss: 0.012462233318131167\n",
      "epoch: 777, loss: 0.012459408153081952\n",
      "epoch: 778, loss: 0.012456586429797072\n",
      "epoch: 779, loss: 0.012453768149373382\n",
      "epoch: 780, loss: 0.01245095331304057\n",
      "epoch: 781, loss: 0.012448141922147764\n",
      "epoch: 782, loss: 0.012445333978150187\n",
      "epoch: 783, loss: 0.012442529482595803\n",
      "epoch: 784, loss: 0.012439728437112059\n",
      "epoch: 785, loss: 0.012436930843392662\n",
      "epoch: 786, loss: 0.012434136703184499\n",
      "epoch: 787, loss: 0.012431346018274628\n",
      "epoch: 788, loss: 0.01242855879047745\n",
      "epoch: 789, loss: 0.012425775021622041\n",
      "epoch: 790, loss: 0.01242299471353963\n",
      "epoch: 791, loss: 0.012420217868051349\n",
      "epoch: 792, loss: 0.01241744448695612\n",
      "epoch: 793, loss: 0.012414674572018856\n",
      "epoch: 794, loss: 0.0124119081249589\n",
      "epoch: 795, loss: 0.012409145147438702\n",
      "epoch: 796, loss: 0.01240638564105284\n",
      "epoch: 797, loss: 0.012403629607317319\n",
      "epoch: 798, loss: 0.012400877047659206\n",
      "epoch: 799, loss: 0.012398127963406582\n",
      "epoch: 800, loss: 0.012395382355778847\n",
      "epoch: 801, loss: 0.012392640225877407\n",
      "epoch: 802, loss: 0.01238990157467669\n",
      "epoch: 803, loss: 0.012387166403015577\n",
      "epoch: 804, loss: 0.012384434711589195\n",
      "epoch: 805, loss: 0.012381706500941116\n",
      "epoch: 806, loss: 0.012378981771455964\n",
      "epoch: 807, loss: 0.012376260523352415\n",
      "epoch: 808, loss: 0.012373542756676615\n",
      "epoch: 809, loss: 0.012370828471296004\n",
      "epoch: 810, loss: 0.012368117666893594\n",
      "epoch: 811, loss: 0.012365410342962593\n",
      "epoch: 812, loss: 0.012362706498801534\n",
      "epoch: 813, loss: 0.012360006133509759\n",
      "epoch: 814, loss: 0.012357309245983343\n",
      "epoch: 815, loss: 0.012354615834911447\n",
      "epoch: 816, loss: 0.012351925898773052\n",
      "epoch: 817, loss: 0.012349239435834132\n",
      "epoch: 818, loss: 0.012346556444145207\n",
      "epoch: 819, loss: 0.012343876921539302\n",
      "epoch: 820, loss: 0.012341200865630316\n",
      "epoch: 821, loss: 0.01233852827381173\n",
      "epoch: 822, loss: 0.012335859143255743\n",
      "epoch: 823, loss: 0.012333193470912742\n",
      "epoch: 824, loss: 0.012330531253511147\n",
      "epoch: 825, loss: 0.01232787248755759\n",
      "epoch: 826, loss: 0.012325217169337458\n",
      "epoch: 827, loss: 0.012322565294915743\n",
      "epoch: 828, loss: 0.012319916860138225\n",
      "epoch: 829, loss: 0.012317271860632951\n",
      "epoch: 830, loss: 0.012314630291812022\n",
      "epoch: 831, loss: 0.012311992148873659\n",
      "epoch: 832, loss: 0.012309357426804522\n",
      "epoch: 833, loss: 0.012306726120382317\n",
      "epoch: 834, loss: 0.012304098224178658\n",
      "epoch: 835, loss: 0.012301473732562086\n",
      "epoch: 836, loss: 0.012298852639701437\n",
      "epoch: 837, loss: 0.012296234939569283\n",
      "epoch: 838, loss: 0.012293620625945679\n",
      "epoch: 839, loss: 0.012291009692422014\n",
      "epoch: 840, loss: 0.012288402132405079\n",
      "epoch: 841, loss: 0.012285797939121287\n",
      "epoch: 842, loss: 0.012283197105620992\n",
      "epoch: 843, loss: 0.01228059962478301\n",
      "epoch: 844, loss: 0.012278005489319212\n",
      "epoch: 845, loss: 0.012275414691779228\n",
      "epoch: 846, loss: 0.012272827224555265\n",
      "epoch: 847, loss: 0.012270243079886991\n",
      "epoch: 848, loss: 0.012267662249866503\n",
      "epoch: 849, loss: 0.012265084726443344\n",
      "epoch: 850, loss: 0.012262510501429593\n",
      "epoch: 851, loss: 0.012259939566504958\n",
      "epoch: 852, loss: 0.012257371913221918\n",
      "epoch: 853, loss: 0.012254807533010907\n",
      "epoch: 854, loss: 0.01225224641718545\n",
      "epoch: 855, loss: 0.01224968855694736\n",
      "epoch: 856, loss: 0.012247133943391879\n",
      "epoch: 857, loss: 0.012244582567512832\n",
      "epoch: 858, loss: 0.012242034420207729\n",
      "epoch: 859, loss: 0.012239489492282866\n",
      "epoch: 860, loss: 0.01223694777445834\n",
      "epoch: 861, loss: 0.012234409257373053\n",
      "epoch: 862, loss: 0.012231873931589662\n",
      "epoch: 863, loss: 0.01222934178759941\n",
      "epoch: 864, loss: 0.012226812815826972\n",
      "epoch: 865, loss: 0.012224287006635162\n",
      "epoch: 866, loss: 0.012221764350329591\n",
      "epoch: 867, loss: 0.012219244837163235\n",
      "epoch: 868, loss: 0.01221672845734089\n",
      "epoch: 869, loss: 0.0122142152010236\n",
      "epoch: 870, loss: 0.012211705058332896\n",
      "epoch: 871, loss: 0.012209198019355001\n",
      "epoch: 872, loss: 0.012206694074144916\n",
      "epoch: 873, loss: 0.012204193212730375\n",
      "epoch: 874, loss: 0.012201695425115722\n",
      "epoch: 875, loss: 0.012199200701285638\n",
      "epoch: 876, loss: 0.012196709031208796\n",
      "epoch: 877, loss: 0.012194220404841364\n",
      "epoch: 878, loss: 0.012191734812130402\n",
      "epoch: 879, loss: 0.012189252243017135\n",
      "epoch: 880, loss: 0.012186772687440119\n",
      "epoch: 881, loss: 0.01218429613533826\n",
      "epoch: 882, loss: 0.012181822576653725\n",
      "epoch: 883, loss: 0.012179352001334735\n",
      "epoch: 884, loss: 0.012176884399338218\n",
      "epoch: 885, loss: 0.012174419760632337\n",
      "epoch: 886, loss: 0.012171958075198922\n",
      "epoch: 887, loss: 0.012169499333035749\n",
      "epoch: 888, loss: 0.01216704352415869\n",
      "epoch: 889, loss: 0.012164590638603782\n",
      "epoch: 890, loss: 0.012162140666429134\n",
      "epoch: 891, loss: 0.012159693597716712\n",
      "epoch: 892, loss: 0.012157249422574033\n",
      "epoch: 893, loss: 0.012154808131135722\n",
      "epoch: 894, loss: 0.012152369713564946\n",
      "epoch: 895, loss: 0.012149934160054736\n",
      "epoch: 896, loss: 0.012147501460829201\n",
      "epoch: 897, loss: 0.012145071606144617\n",
      "epoch: 898, loss: 0.012142644586290406\n",
      "epoch: 899, loss: 0.012140220391590014\n",
      "epoch: 900, loss: 0.01213779901240166\n",
      "epoch: 901, loss: 0.012135380439119004\n",
      "epoch: 902, loss: 0.012132964662171689\n",
      "epoch: 903, loss: 0.012130551672025765\n",
      "epoch: 904, loss: 0.01212814145918407\n",
      "epoch: 905, loss: 0.012125734014186451\n",
      "epoch: 906, loss: 0.012123329327609898\n",
      "epoch: 907, loss: 0.012120927390068618\n",
      "epoch: 908, loss: 0.012118528192213977\n",
      "epoch: 909, loss: 0.012116131724734356\n",
      "epoch: 910, loss: 0.012113737978354943\n",
      "epoch: 911, loss: 0.012111346943837408\n",
      "epoch: 912, loss: 0.012108958611979493\n",
      "epoch: 913, loss: 0.012106572973614553\n",
      "epoch: 914, loss: 0.012104190019610982\n",
      "epoch: 915, loss: 0.012101809740871551\n",
      "epoch: 916, loss: 0.012099432128332718\n",
      "epoch: 917, loss: 0.01209705717296381\n",
      "epoch: 918, loss: 0.012094684865766162\n",
      "epoch: 919, loss: 0.012092315197772171\n",
      "epoch: 920, loss: 0.012089948160044297\n",
      "epoch: 921, loss: 0.012087583743673976\n",
      "epoch: 922, loss: 0.012085221939780476\n",
      "epoch: 923, loss: 0.012082862739509703\n",
      "epoch: 924, loss: 0.012080506134032928\n",
      "epoch: 925, loss: 0.012078152114545467\n",
      "epoch: 926, loss: 0.012075800672265283\n",
      "epoch: 927, loss: 0.012073451798431565\n",
      "epoch: 928, loss: 0.012071105484303223\n",
      "epoch: 929, loss: 0.01206876172115735\n",
      "epoch: 930, loss: 0.012066420500287625\n",
      "epoch: 931, loss: 0.012064081813002666\n",
      "epoch: 932, loss: 0.012061745650624348\n",
      "epoch: 933, loss: 0.012059412004486058\n",
      "epoch: 934, loss: 0.01205708086593092\n",
      "epoch: 935, loss: 0.01205475222630998\n",
      "epoch: 936, loss: 0.012052426076980332\n",
      "epoch: 937, loss: 0.012050102409303228\n",
      "epoch: 938, loss: 0.012047781214642138\n",
      "epoch: 939, loss: 0.012045462484360777\n",
      "epoch: 940, loss: 0.012043146209821094\n",
      "epoch: 941, loss: 0.012040832382381232\n",
      "epoch: 942, loss: 0.012038520993393469\n",
      "epoch: 943, loss: 0.012036212034202097\n",
      "epoch: 944, loss: 0.0120339054961413\n",
      "epoch: 945, loss: 0.012031601370532994\n",
      "epoch: 946, loss: 0.012029299648684653\n",
      "epoch: 947, loss: 0.012027000321887067\n",
      "epoch: 948, loss: 0.012024703381412151\n",
      "epoch: 949, loss: 0.012022408818510667\n",
      "epoch: 950, loss: 0.01202011662440994\n",
      "epoch: 951, loss: 0.012017826790311567\n",
      "epoch: 952, loss: 0.012015539307389108\n",
      "epoch: 953, loss: 0.01201325416678574\n",
      "epoch: 954, loss: 0.012010971359611917\n",
      "epoch: 955, loss: 0.012008690876942982\n",
      "epoch: 956, loss: 0.012006412709816816\n",
      "epoch: 957, loss: 0.012004136849231406\n",
      "epoch: 958, loss: 0.012001863286142464\n",
      "epoch: 959, loss: 0.011999592011460995\n",
      "epoch: 960, loss: 0.011997323016050873\n",
      "epoch: 961, loss: 0.011995056290726388\n",
      "epoch: 962, loss: 0.011992791826249809\n",
      "epoch: 963, loss: 0.011990529613328927\n",
      "epoch: 964, loss: 0.011988269642614584\n",
      "epoch: 965, loss: 0.011986011904698212\n",
      "epoch: 966, loss: 0.011983756390109346\n",
      "epoch: 967, loss: 0.011981503089313176\n",
      "epoch: 968, loss: 0.011979251992708037\n",
      "epoch: 969, loss: 0.01197700309062295\n",
      "epoch: 970, loss: 0.01197475637331512\n",
      "epoch: 971, loss: 0.011972511830967474\n",
      "epoch: 972, loss: 0.01197026945368618\n",
      "epoch: 973, loss: 0.011968029231498159\n",
      "epoch: 974, loss: 0.011965791154348624\n",
      "epoch: 975, loss: 0.011963555212098605\n",
      "epoch: 976, loss: 0.011961321394522497\n",
      "epoch: 977, loss: 0.011959089691305595\n",
      "epoch: 978, loss: 0.01195686009204167\n",
      "epoch: 979, loss: 0.0119546325862305\n",
      "epoch: 980, loss: 0.011952407163275478\n",
      "epoch: 981, loss: 0.011950183812481186\n",
      "epoch: 982, loss: 0.011947962523051001\n",
      "epoch: 983, loss: 0.011945743284084712\n",
      "epoch: 984, loss: 0.011943526084576158\n",
      "epoch: 985, loss: 0.011941310913410878\n",
      "epoch: 986, loss: 0.01193909775936379\n",
      "epoch: 987, loss: 0.011936886611096884\n",
      "epoch: 988, loss: 0.01193467745715694\n",
      "epoch: 989, loss: 0.011932470285973282\n",
      "epoch: 990, loss: 0.011930265085855539\n",
      "epoch: 991, loss: 0.011928061844991454\n",
      "epoch: 992, loss: 0.011925860551444721\n",
      "epoch: 993, loss: 0.011923661193152848\n",
      "epoch: 994, loss: 0.011921463757925063\n",
      "epoch: 995, loss: 0.011919268233440264\n",
      "epoch: 996, loss: 0.011917074607244981\n",
      "epoch: 997, loss: 0.011914882866751438\n",
      "epoch: 998, loss: 0.011912692999235587\n",
      "epoch: 999, loss: 0.011910504991835276\n"
     ]
    }
   ],
   "source": [
    "x = scaler(x, mode=\"standard\")\n",
    "\n",
    "dnn = sequential_dnn(dim = [3, 6, 5, 1], lr = 0.1)\n",
    "\n",
    "dnn.train(x, y, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qnn.loss)\n",
    "plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
