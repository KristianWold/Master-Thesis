{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D, Gaussian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.25, 0.02) - gaussian(x, 0.75, 0.02)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_qnn, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e220afab984197b887e5c282669341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67403894b0184c81971ac1eefe83b279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09710360992010132\n",
      "epoch: 1, loss: 0.09014273076018783\n",
      "epoch: 2, loss: 0.08578024780082348\n",
      "epoch: 3, loss: 0.07793310813512677\n",
      "epoch: 4, loss: 0.07192003138881381\n",
      "epoch: 5, loss: 0.06548785819818813\n",
      "epoch: 6, loss: 0.061731348043349135\n",
      "epoch: 7, loss: 0.057307423406688394\n",
      "epoch: 8, loss: 0.05268394195057945\n",
      "epoch: 9, loss: 0.04787923706050479\n",
      "epoch: 10, loss: 0.042510512797828935\n",
      "epoch: 11, loss: 0.03681107494035281\n",
      "epoch: 12, loss: 0.032672477289998204\n",
      "epoch: 13, loss: 0.028735383763195205\n",
      "epoch: 14, loss: 0.026329717465544022\n",
      "epoch: 15, loss: 0.025288251421505436\n",
      "epoch: 16, loss: 0.024371641813306196\n",
      "epoch: 17, loss: 0.022412818975749954\n",
      "epoch: 18, loss: 0.019701093528496624\n",
      "epoch: 19, loss: 0.015499505159650267\n",
      "epoch: 20, loss: 0.011305870339045294\n",
      "epoch: 21, loss: 0.007987567748858612\n",
      "epoch: 22, loss: 0.0066913147578221924\n",
      "epoch: 23, loss: 0.0069246752438976355\n",
      "epoch: 24, loss: 0.007154679574156833\n",
      "epoch: 25, loss: 0.00706928862231808\n",
      "epoch: 26, loss: 0.006824269335961281\n",
      "epoch: 27, loss: 0.006604968067816748\n",
      "epoch: 28, loss: 0.006114087098883566\n",
      "epoch: 29, loss: 0.0056974648277728305\n",
      "epoch: 30, loss: 0.005807466875609071\n",
      "epoch: 31, loss: 0.006423524722933179\n",
      "epoch: 32, loss: 0.006774250649087979\n",
      "epoch: 33, loss: 0.007234962045429085\n",
      "epoch: 34, loss: 0.007075759513449863\n",
      "epoch: 35, loss: 0.006853237419579376\n",
      "epoch: 36, loss: 0.006512363827455664\n",
      "epoch: 37, loss: 0.006354862393931518\n",
      "epoch: 38, loss: 0.006060747477494943\n",
      "epoch: 39, loss: 0.005798210309002921\n",
      "epoch: 40, loss: 0.006082659366961839\n",
      "epoch: 41, loss: 0.005763956762019093\n",
      "epoch: 42, loss: 0.005695784455822664\n",
      "epoch: 43, loss: 0.0056158137066194755\n",
      "epoch: 44, loss: 0.005323409007924905\n",
      "epoch: 45, loss: 0.004978604479585911\n",
      "epoch: 46, loss: 0.004676918960682692\n",
      "epoch: 47, loss: 0.004695004712137594\n",
      "epoch: 48, loss: 0.004715851114591124\n",
      "epoch: 49, loss: 0.004446175946344669\n",
      "epoch: 50, loss: 0.004510632702873284\n",
      "epoch: 51, loss: 0.004711471288835633\n",
      "epoch: 52, loss: 0.004422126938095092\n",
      "epoch: 53, loss: 0.00426170140987801\n",
      "epoch: 54, loss: 0.004621776403693364\n",
      "epoch: 55, loss: 0.0045312720731623315\n",
      "epoch: 56, loss: 0.004280650421670181\n",
      "epoch: 57, loss: 0.004364096504355052\n",
      "epoch: 58, loss: 0.0045517620833738824\n",
      "epoch: 59, loss: 0.004384301967497242\n",
      "epoch: 60, loss: 0.0043497864405902895\n",
      "epoch: 61, loss: 0.004364808438259297\n",
      "epoch: 62, loss: 0.00453259698915467\n",
      "epoch: 63, loss: 0.004412266613826752\n",
      "epoch: 64, loss: 0.004557149270517227\n",
      "epoch: 65, loss: 0.004525712672575195\n",
      "epoch: 66, loss: 0.004340809215942733\n",
      "epoch: 67, loss: 0.004446265659748242\n",
      "epoch: 68, loss: 0.004511432899499672\n",
      "epoch: 69, loss: 0.004173414898748028\n",
      "epoch: 70, loss: 0.004317059225496871\n",
      "epoch: 71, loss: 0.004368192758169533\n",
      "epoch: 72, loss: 0.004458021399555249\n",
      "epoch: 73, loss: 0.004301877182475666\n",
      "epoch: 74, loss: 0.004366667363823745\n",
      "epoch: 75, loss: 0.004294593119995291\n",
      "epoch: 76, loss: 0.004603065784274082\n",
      "epoch: 77, loss: 0.004261765111600983\n",
      "epoch: 78, loss: 0.004372802983365041\n",
      "epoch: 79, loss: 0.004369301359180244\n",
      "epoch: 80, loss: 0.00427377757351624\n",
      "epoch: 81, loss: 0.004262627521166317\n",
      "epoch: 82, loss: 0.00436940345944877\n",
      "epoch: 83, loss: 0.0043059595628098316\n",
      "epoch: 84, loss: 0.004356929136348093\n",
      "epoch: 85, loss: 0.0043841032199900135\n",
      "epoch: 86, loss: 0.004204459306219\n",
      "epoch: 87, loss: 0.004191032489673705\n",
      "epoch: 88, loss: 0.0043300671822181945\n",
      "epoch: 89, loss: 0.004210216550894488\n",
      "epoch: 90, loss: 0.004475277586439352\n",
      "epoch: 91, loss: 0.004324439384607907\n",
      "epoch: 92, loss: 0.004183282471788211\n",
      "epoch: 93, loss: 0.004399664287891059\n",
      "epoch: 94, loss: 0.00432197057043599\n",
      "epoch: 95, loss: 0.004330384824583177\n",
      "epoch: 96, loss: 0.004229883899958872\n",
      "epoch: 97, loss: 0.004338323873951987\n",
      "epoch: 98, loss: 0.0042320016653496426\n",
      "epoch: 99, loss: 0.004343819112275439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def19f74897d4bf485d2b21efe12c539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1698358314767118\n",
      "epoch: 1, loss: 0.13194601763948655\n",
      "epoch: 2, loss: 0.1018464501168202\n",
      "epoch: 3, loss: 0.07965330206097451\n",
      "epoch: 4, loss: 0.06255674833900148\n",
      "epoch: 5, loss: 0.05508323137012289\n",
      "epoch: 6, loss: 0.05299872954525948\n",
      "epoch: 7, loss: 0.04811109900150293\n",
      "epoch: 8, loss: 0.04210490774321551\n",
      "epoch: 9, loss: 0.03596176460198606\n",
      "epoch: 10, loss: 0.029640408112776462\n",
      "epoch: 11, loss: 0.02409151579618667\n",
      "epoch: 12, loss: 0.023623466744578984\n",
      "epoch: 13, loss: 0.02660662289108878\n",
      "epoch: 14, loss: 0.030969990316971133\n",
      "epoch: 15, loss: 0.03325933256946927\n",
      "epoch: 16, loss: 0.03180002874720861\n",
      "epoch: 17, loss: 0.029011136500791155\n",
      "epoch: 18, loss: 0.02523810508461821\n",
      "epoch: 19, loss: 0.023847635203040812\n",
      "epoch: 20, loss: 0.022500674033571554\n",
      "epoch: 21, loss: 0.021612125016831133\n",
      "epoch: 22, loss: 0.020272646730830717\n",
      "epoch: 23, loss: 0.019366005997422123\n",
      "epoch: 24, loss: 0.019326344816396994\n",
      "epoch: 25, loss: 0.018000702753584185\n",
      "epoch: 26, loss: 0.01742628978727816\n",
      "epoch: 27, loss: 0.016394028018286863\n",
      "epoch: 28, loss: 0.016073375915657485\n",
      "epoch: 29, loss: 0.015223804060664856\n",
      "epoch: 30, loss: 0.014580607314552194\n",
      "epoch: 31, loss: 0.012989194360602165\n",
      "epoch: 32, loss: 0.01174775552117202\n",
      "epoch: 33, loss: 0.011563775860892694\n",
      "epoch: 34, loss: 0.011129690412580065\n",
      "epoch: 35, loss: 0.011130319957411901\n",
      "epoch: 36, loss: 0.011623607682640426\n",
      "epoch: 37, loss: 0.010959535795003146\n",
      "epoch: 38, loss: 0.011233358230879358\n",
      "epoch: 39, loss: 0.011171193780974531\n",
      "epoch: 40, loss: 0.011410746528242971\n",
      "epoch: 41, loss: 0.011741297435258709\n",
      "epoch: 42, loss: 0.01115644908892043\n",
      "epoch: 43, loss: 0.011104006775747435\n",
      "epoch: 44, loss: 0.010901777249149331\n",
      "epoch: 45, loss: 0.010752671530144253\n",
      "epoch: 46, loss: 0.011075689099216565\n",
      "epoch: 47, loss: 0.010736129039046173\n",
      "epoch: 48, loss: 0.010770219263285358\n",
      "epoch: 49, loss: 0.010810229173742374\n",
      "epoch: 50, loss: 0.01036356086314724\n",
      "epoch: 51, loss: 0.01098457188139295\n",
      "epoch: 52, loss: 0.010540276101823265\n",
      "epoch: 53, loss: 0.010490725873270688\n",
      "epoch: 54, loss: 0.010486687531760899\n",
      "epoch: 55, loss: 0.010392136414015575\n",
      "epoch: 56, loss: 0.010265037391667618\n",
      "epoch: 57, loss: 0.010018322387076879\n",
      "epoch: 58, loss: 0.010388272303261302\n",
      "epoch: 59, loss: 0.009977768146526345\n",
      "epoch: 60, loss: 0.01040126891630985\n",
      "epoch: 61, loss: 0.010281520834930844\n",
      "epoch: 62, loss: 0.010201670890694738\n",
      "epoch: 63, loss: 0.010187884131500709\n",
      "epoch: 64, loss: 0.010084108761058856\n",
      "epoch: 65, loss: 0.010159701026600982\n",
      "epoch: 66, loss: 0.010127675465025427\n",
      "epoch: 67, loss: 0.010352893591004282\n",
      "epoch: 68, loss: 0.010025111786464013\n",
      "epoch: 69, loss: 0.010280021062065454\n",
      "epoch: 70, loss: 0.01013362655776066\n",
      "epoch: 71, loss: 0.010293817516965358\n",
      "epoch: 72, loss: 0.010203286712043371\n",
      "epoch: 73, loss: 0.010169914943757487\n",
      "epoch: 74, loss: 0.00994510225419452\n",
      "epoch: 75, loss: 0.010374142605984602\n",
      "epoch: 76, loss: 0.010246211574567472\n",
      "epoch: 77, loss: 0.010072854998955572\n",
      "epoch: 78, loss: 0.01003615404307864\n",
      "epoch: 79, loss: 0.010056208936907073\n",
      "epoch: 80, loss: 0.010056584824026946\n",
      "epoch: 81, loss: 0.009906983058064428\n",
      "epoch: 82, loss: 0.010116464345692246\n",
      "epoch: 83, loss: 0.009895591167870064\n",
      "epoch: 84, loss: 0.010385535305893027\n",
      "epoch: 85, loss: 0.010321354660375896\n",
      "epoch: 86, loss: 0.010206247845794129\n",
      "epoch: 87, loss: 0.010119955774186364\n",
      "epoch: 88, loss: 0.010184127755057589\n",
      "epoch: 89, loss: 0.009956253324127631\n",
      "epoch: 90, loss: 0.010106292482862772\n",
      "epoch: 91, loss: 0.010247810683669107\n",
      "epoch: 92, loss: 0.010095504441243712\n",
      "epoch: 93, loss: 0.010045248190442382\n",
      "epoch: 94, loss: 0.009882367962894998\n",
      "epoch: 95, loss: 0.010279646870141268\n",
      "epoch: 96, loss: 0.010276632670907454\n",
      "epoch: 97, loss: 0.009888336578411192\n",
      "epoch: 98, loss: 0.010072563311469083\n",
      "epoch: 99, loss: 0.010134368520482821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5280c3f5c6044d2a08bd84e36db9889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.03326871928316099\n",
      "epoch: 1, loss: 0.029637140191057235\n",
      "epoch: 2, loss: 0.02485857361003139\n",
      "epoch: 3, loss: 0.019929893970564157\n",
      "epoch: 4, loss: 0.01741937204384185\n",
      "epoch: 5, loss: 0.01682722354741864\n",
      "epoch: 6, loss: 0.015940767586336467\n",
      "epoch: 7, loss: 0.014312959250152697\n",
      "epoch: 8, loss: 0.01341675214994497\n",
      "epoch: 9, loss: 0.012898443765943735\n",
      "epoch: 10, loss: 0.012842708891379624\n",
      "epoch: 11, loss: 0.012019774843036209\n",
      "epoch: 12, loss: 0.010218174339406503\n",
      "epoch: 13, loss: 0.0092224769641327\n",
      "epoch: 14, loss: 0.008485055215511827\n",
      "epoch: 15, loss: 0.008701629727942176\n",
      "epoch: 16, loss: 0.008420398803313316\n",
      "epoch: 17, loss: 0.007888774267254837\n",
      "epoch: 18, loss: 0.006794214726252511\n",
      "epoch: 19, loss: 0.006286321901068522\n",
      "epoch: 20, loss: 0.0053602101086510845\n",
      "epoch: 21, loss: 0.004910071191881074\n",
      "epoch: 22, loss: 0.004549122449902912\n",
      "epoch: 23, loss: 0.004196786085092286\n",
      "epoch: 24, loss: 0.003771464193218779\n",
      "epoch: 25, loss: 0.003550510232590519\n",
      "epoch: 26, loss: 0.003398123692048388\n",
      "epoch: 27, loss: 0.0035592234339480782\n",
      "epoch: 28, loss: 0.0035588082682300216\n",
      "epoch: 29, loss: 0.003465717066229719\n",
      "epoch: 30, loss: 0.0034706016158202522\n",
      "epoch: 31, loss: 0.0032023973262322782\n",
      "epoch: 32, loss: 0.0032511156770431314\n",
      "epoch: 33, loss: 0.002989329404208778\n",
      "epoch: 34, loss: 0.002793660977969667\n",
      "epoch: 35, loss: 0.0027724537074632167\n",
      "epoch: 36, loss: 0.002586578114330268\n",
      "epoch: 37, loss: 0.0024113224258951477\n",
      "epoch: 38, loss: 0.002382645080476912\n",
      "epoch: 39, loss: 0.0022483328078809683\n",
      "epoch: 40, loss: 0.0021779411502265648\n",
      "epoch: 41, loss: 0.002155519281143711\n",
      "epoch: 42, loss: 0.0019244658312150232\n",
      "epoch: 43, loss: 0.0019069090270119314\n",
      "epoch: 44, loss: 0.0019498300758166063\n",
      "epoch: 45, loss: 0.0019526696899748073\n",
      "epoch: 46, loss: 0.001986693674874869\n",
      "epoch: 47, loss: 0.0018655307006464456\n",
      "epoch: 48, loss: 0.0018024022901164053\n",
      "epoch: 49, loss: 0.0018313518568560075\n",
      "epoch: 50, loss: 0.0017611618919744676\n",
      "epoch: 51, loss: 0.0018047122654879161\n",
      "epoch: 52, loss: 0.00182878321128159\n",
      "epoch: 53, loss: 0.0018183882985247832\n",
      "epoch: 54, loss: 0.001692248218516168\n",
      "epoch: 55, loss: 0.0017224104056411185\n",
      "epoch: 56, loss: 0.0016870085962438707\n",
      "epoch: 57, loss: 0.001730723064096119\n",
      "epoch: 58, loss: 0.0016990676674869653\n",
      "epoch: 59, loss: 0.001690988014629246\n",
      "epoch: 60, loss: 0.0016208471529597557\n",
      "epoch: 61, loss: 0.00158129062909758\n",
      "epoch: 62, loss: 0.0016423548348110674\n",
      "epoch: 63, loss: 0.0016493187985001864\n",
      "epoch: 64, loss: 0.0015967165100902042\n",
      "epoch: 65, loss: 0.001634645672189997\n",
      "epoch: 66, loss: 0.0016630901403757133\n",
      "epoch: 67, loss: 0.0015350482570529978\n",
      "epoch: 68, loss: 0.0015586182423617495\n",
      "epoch: 69, loss: 0.0015411567619336318\n",
      "epoch: 70, loss: 0.0015299974993357418\n",
      "epoch: 71, loss: 0.0016261526802706613\n",
      "epoch: 72, loss: 0.0015638803542164337\n",
      "epoch: 73, loss: 0.0015781057651443754\n",
      "epoch: 74, loss: 0.0016393611831935543\n",
      "epoch: 75, loss: 0.0015393480121223557\n",
      "epoch: 76, loss: 0.001536524092371358\n",
      "epoch: 77, loss: 0.001551250133068338\n",
      "epoch: 78, loss: 0.0015964434641506193\n",
      "epoch: 79, loss: 0.0015802181970021062\n",
      "epoch: 80, loss: 0.0015507230860383912\n",
      "epoch: 81, loss: 0.0015202832951629764\n",
      "epoch: 82, loss: 0.0015741508939579652\n",
      "epoch: 83, loss: 0.0015894418026410149\n",
      "epoch: 84, loss: 0.001552974553543707\n",
      "epoch: 85, loss: 0.0015645427200042148\n",
      "epoch: 86, loss: 0.0014936947210537294\n",
      "epoch: 87, loss: 0.0015389550326855126\n",
      "epoch: 88, loss: 0.0015283629141802251\n",
      "epoch: 89, loss: 0.0015030539558739007\n",
      "epoch: 90, loss: 0.001524840284331337\n",
      "epoch: 91, loss: 0.0015164535415167339\n",
      "epoch: 92, loss: 0.0015593903158776375\n",
      "epoch: 93, loss: 0.0014980480247508948\n",
      "epoch: 94, loss: 0.0015919104139752707\n",
      "epoch: 95, loss: 0.001564670304180798\n",
      "epoch: 96, loss: 0.0015435148033041142\n",
      "epoch: 97, loss: 0.0015635002722156415\n",
      "epoch: 98, loss: 0.0014553934493360715\n",
      "epoch: 99, loss: 0.001510189413817507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308f76080c3642cbb1eff59623cdf8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09231437294671947\n",
      "epoch: 1, loss: 0.06989532489557854\n",
      "epoch: 2, loss: 0.0594932924000197\n",
      "epoch: 3, loss: 0.048406945786876585\n",
      "epoch: 4, loss: 0.0378978154533093\n",
      "epoch: 5, loss: 0.03287425300568644\n",
      "epoch: 6, loss: 0.03199767187941622\n",
      "epoch: 7, loss: 0.031068607413777864\n",
      "epoch: 8, loss: 0.02944389363730924\n",
      "epoch: 9, loss: 0.029749634852822015\n",
      "epoch: 10, loss: 0.02890991483808234\n",
      "epoch: 11, loss: 0.029110678486434304\n",
      "epoch: 12, loss: 0.02748864348931948\n",
      "epoch: 13, loss: 0.025584509944226123\n",
      "epoch: 14, loss: 0.022119520197441372\n",
      "epoch: 15, loss: 0.019045395118182203\n",
      "epoch: 16, loss: 0.016183967080671265\n",
      "epoch: 17, loss: 0.01423600273186901\n",
      "epoch: 18, loss: 0.01402079939209361\n",
      "epoch: 19, loss: 0.015723734892578577\n",
      "epoch: 20, loss: 0.01735310120275416\n",
      "epoch: 21, loss: 0.018650268886950393\n",
      "epoch: 22, loss: 0.019046078857198324\n",
      "epoch: 23, loss: 0.017908985170862035\n",
      "epoch: 24, loss: 0.016565703239984096\n",
      "epoch: 25, loss: 0.015759517758405308\n",
      "epoch: 26, loss: 0.015330914019188598\n",
      "epoch: 27, loss: 0.014563086542001096\n",
      "epoch: 28, loss: 0.014318948725556608\n",
      "epoch: 29, loss: 0.014905389206361374\n",
      "epoch: 30, loss: 0.014668693531376906\n",
      "epoch: 31, loss: 0.014422232042408813\n",
      "epoch: 32, loss: 0.014523697782288025\n",
      "epoch: 33, loss: 0.014655901132438238\n",
      "epoch: 34, loss: 0.014349197635991491\n",
      "epoch: 35, loss: 0.013866761887327455\n",
      "epoch: 36, loss: 0.013823931214350677\n",
      "epoch: 37, loss: 0.01402396158680117\n",
      "epoch: 38, loss: 0.013747039897401758\n",
      "epoch: 39, loss: 0.013669507648204593\n",
      "epoch: 40, loss: 0.013334683372367526\n",
      "epoch: 41, loss: 0.013453879138143434\n",
      "epoch: 42, loss: 0.013787028317966259\n",
      "epoch: 43, loss: 0.01376387117955269\n",
      "epoch: 44, loss: 0.013581645915892329\n",
      "epoch: 45, loss: 0.01371391344440512\n",
      "epoch: 46, loss: 0.013601155900103629\n",
      "epoch: 47, loss: 0.013194777211022142\n",
      "epoch: 48, loss: 0.01316538453437872\n",
      "epoch: 49, loss: 0.013242242731318504\n",
      "epoch: 50, loss: 0.01308909880057048\n",
      "epoch: 51, loss: 0.013645705650941692\n",
      "epoch: 52, loss: 0.01324103569202323\n",
      "epoch: 53, loss: 0.013189651494027566\n",
      "epoch: 54, loss: 0.013107139333339816\n",
      "epoch: 55, loss: 0.013553617388295908\n",
      "epoch: 56, loss: 0.01347297243282904\n",
      "epoch: 57, loss: 0.013274741911491101\n",
      "epoch: 58, loss: 0.01334305443894419\n",
      "epoch: 59, loss: 0.013453196731872483\n",
      "epoch: 60, loss: 0.013304174973372876\n",
      "epoch: 61, loss: 0.013190709324603764\n",
      "epoch: 62, loss: 0.013209705243138317\n",
      "epoch: 63, loss: 0.01307689301360836\n",
      "epoch: 64, loss: 0.013100085147558529\n",
      "epoch: 65, loss: 0.013146932813923713\n",
      "epoch: 66, loss: 0.013458233883611425\n",
      "epoch: 67, loss: 0.013522165631456034\n",
      "epoch: 68, loss: 0.012652270953392861\n",
      "epoch: 69, loss: 0.013233550287986402\n",
      "epoch: 70, loss: 0.013035878494964605\n",
      "epoch: 71, loss: 0.013146214796709878\n",
      "epoch: 72, loss: 0.013016201393727158\n",
      "epoch: 73, loss: 0.013307100495698589\n",
      "epoch: 74, loss: 0.01308907681348082\n",
      "epoch: 75, loss: 0.013075295428954898\n",
      "epoch: 76, loss: 0.013376672289096093\n",
      "epoch: 77, loss: 0.0131948048288538\n",
      "epoch: 78, loss: 0.013173654573091059\n",
      "epoch: 79, loss: 0.013240857604126768\n",
      "epoch: 80, loss: 0.013058651456549084\n",
      "epoch: 81, loss: 0.012905856318568165\n",
      "epoch: 82, loss: 0.012867331348661349\n",
      "epoch: 83, loss: 0.013066487027364827\n",
      "epoch: 84, loss: 0.013113719092281054\n",
      "epoch: 85, loss: 0.01308877697070772\n",
      "epoch: 86, loss: 0.013115756506046865\n",
      "epoch: 87, loss: 0.013175276662055216\n",
      "epoch: 88, loss: 0.01262879753643859\n",
      "epoch: 89, loss: 0.013249893125149488\n",
      "epoch: 90, loss: 0.013330771574084995\n",
      "epoch: 91, loss: 0.013002768267447319\n",
      "epoch: 92, loss: 0.013018889326768675\n",
      "epoch: 93, loss: 0.013107331377584606\n",
      "epoch: 94, loss: 0.013020299486797273\n",
      "epoch: 95, loss: 0.01300903660216946\n",
      "epoch: 96, loss: 0.013029130598201398\n",
      "epoch: 97, loss: 0.013114154194457607\n",
      "epoch: 98, loss: 0.013063501139204674\n",
      "epoch: 99, loss: 0.013040993086571338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e380df05cf149f283bdaec6997c2aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.16763394574496362\n",
      "epoch: 1, loss: 0.13724059330031305\n",
      "epoch: 2, loss: 0.12180842935489586\n",
      "epoch: 3, loss: 0.11770520667533228\n",
      "epoch: 4, loss: 0.11820754547203638\n",
      "epoch: 5, loss: 0.11837297040535527\n",
      "epoch: 6, loss: 0.11835501338401247\n",
      "epoch: 7, loss: 0.11687677747579228\n",
      "epoch: 8, loss: 0.11444162657298451\n",
      "epoch: 9, loss: 0.1105966034376995\n",
      "epoch: 10, loss: 0.105778198047728\n",
      "epoch: 11, loss: 0.09969556165888925\n",
      "epoch: 12, loss: 0.09194950796367252\n",
      "epoch: 13, loss: 0.08346714074547011\n",
      "epoch: 14, loss: 0.07313634126017245\n",
      "epoch: 15, loss: 0.06485099592446714\n",
      "epoch: 16, loss: 0.05755962016805709\n",
      "epoch: 17, loss: 0.051252675495985764\n",
      "epoch: 18, loss: 0.047165841253508256\n",
      "epoch: 19, loss: 0.04332065644820639\n",
      "epoch: 20, loss: 0.042686400268011584\n",
      "epoch: 21, loss: 0.042316034837145244\n",
      "epoch: 22, loss: 0.042886800835179366\n",
      "epoch: 23, loss: 0.04035741541586584\n",
      "epoch: 24, loss: 0.03564146183965786\n",
      "epoch: 25, loss: 0.02991959838394108\n",
      "epoch: 26, loss: 0.025075120521291298\n",
      "epoch: 27, loss: 0.022123037914144373\n",
      "epoch: 28, loss: 0.02052710443387973\n",
      "epoch: 29, loss: 0.016445201469354152\n",
      "epoch: 30, loss: 0.011563076215828255\n",
      "epoch: 31, loss: 0.008159716087170364\n",
      "epoch: 32, loss: 0.007275793447830565\n",
      "epoch: 33, loss: 0.007242791792270771\n",
      "epoch: 34, loss: 0.006935836837433223\n",
      "epoch: 35, loss: 0.006932181654846018\n",
      "epoch: 36, loss: 0.006696019258509897\n",
      "epoch: 37, loss: 0.0065620330843614095\n",
      "epoch: 38, loss: 0.006938820657970996\n",
      "epoch: 39, loss: 0.006824282180282959\n",
      "epoch: 40, loss: 0.006751325516299838\n",
      "epoch: 41, loss: 0.006520405585289503\n",
      "epoch: 42, loss: 0.006579379347192826\n",
      "epoch: 43, loss: 0.006310294952045422\n",
      "epoch: 44, loss: 0.0064108428725982245\n",
      "epoch: 45, loss: 0.00628093433950489\n",
      "epoch: 46, loss: 0.006661183492653944\n",
      "epoch: 47, loss: 0.0066366643518204005\n",
      "epoch: 48, loss: 0.006537320695671863\n",
      "epoch: 49, loss: 0.0066178589132893165\n",
      "epoch: 50, loss: 0.0065976500607682585\n",
      "epoch: 51, loss: 0.006330958458334169\n",
      "epoch: 52, loss: 0.006331136795907835\n",
      "epoch: 53, loss: 0.005864204008317574\n",
      "epoch: 54, loss: 0.0057191919108513165\n",
      "epoch: 55, loss: 0.005272300579428719\n",
      "epoch: 56, loss: 0.005038250086015\n",
      "epoch: 57, loss: 0.005025058587585699\n",
      "epoch: 58, loss: 0.004912136337488828\n",
      "epoch: 59, loss: 0.004981347868531055\n",
      "epoch: 60, loss: 0.0049697275147911136\n",
      "epoch: 61, loss: 0.005020165573442014\n",
      "epoch: 62, loss: 0.004684162154839875\n",
      "epoch: 63, loss: 0.004534761205499759\n",
      "epoch: 64, loss: 0.004368772198248569\n",
      "epoch: 65, loss: 0.004478931799462899\n",
      "epoch: 66, loss: 0.00422835703917114\n",
      "epoch: 67, loss: 0.004300447693973307\n",
      "epoch: 68, loss: 0.004417320285116437\n",
      "epoch: 69, loss: 0.004392943523736844\n",
      "epoch: 70, loss: 0.00432169411496682\n",
      "epoch: 71, loss: 0.0043167351802889655\n",
      "epoch: 72, loss: 0.004360328259208186\n",
      "epoch: 73, loss: 0.004191854986691457\n",
      "epoch: 74, loss: 0.004330292528727346\n",
      "epoch: 75, loss: 0.004167293257303486\n",
      "epoch: 76, loss: 0.0043272306883307375\n",
      "epoch: 77, loss: 0.0041960833173612724\n",
      "epoch: 78, loss: 0.004099165181340487\n",
      "epoch: 79, loss: 0.00421877077286796\n",
      "epoch: 80, loss: 0.004320359193878882\n",
      "epoch: 81, loss: 0.004175160873520492\n",
      "epoch: 82, loss: 0.004156093330355758\n",
      "epoch: 83, loss: 0.0041568842135490365\n",
      "epoch: 84, loss: 0.004149797944352142\n",
      "epoch: 85, loss: 0.004220046118347261\n",
      "epoch: 86, loss: 0.004107540908908426\n",
      "epoch: 87, loss: 0.0041495918875021345\n",
      "epoch: 88, loss: 0.004291645703356169\n",
      "epoch: 89, loss: 0.004065340575710455\n",
      "epoch: 90, loss: 0.004145785635681688\n",
      "epoch: 91, loss: 0.004073910228466169\n",
      "epoch: 92, loss: 0.004133115450877604\n",
      "epoch: 93, loss: 0.004053977846553131\n",
      "epoch: 94, loss: 0.004007449170046278\n",
      "epoch: 95, loss: 0.004165972457984234\n",
      "epoch: 96, loss: 0.004189621141270279\n",
      "epoch: 97, loss: 0.004088238453491954\n",
      "epoch: 98, loss: 0.004011352082452088\n",
      "epoch: 99, loss: 0.003983016648263613\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder = Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    \n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32d1cd762f64827ad2d2b4d7e16679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f038dded62714b18844ac77d6a5eff9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1990445176718433\n",
      "epoch: 1, loss: 0.14387151395364386\n",
      "epoch: 2, loss: 0.09373623484277616\n",
      "epoch: 3, loss: 0.06678137741329557\n",
      "epoch: 4, loss: 0.06555775549973668\n",
      "epoch: 5, loss: 0.06812831263497056\n",
      "epoch: 6, loss: 0.061119282643448\n",
      "epoch: 7, loss: 0.048718800345568816\n",
      "epoch: 8, loss: 0.035407853399393945\n",
      "epoch: 9, loss: 0.027140184152426156\n",
      "epoch: 10, loss: 0.02504313110640426\n",
      "epoch: 11, loss: 0.026845172049101817\n",
      "epoch: 12, loss: 0.029189919371154904\n",
      "epoch: 13, loss: 0.028875388795130054\n",
      "epoch: 14, loss: 0.025519562469205694\n",
      "epoch: 15, loss: 0.021089017549789698\n",
      "epoch: 16, loss: 0.01649619836387635\n",
      "epoch: 17, loss: 0.01361443502019186\n",
      "epoch: 18, loss: 0.013259263399713162\n",
      "epoch: 19, loss: 0.012654351798105366\n",
      "epoch: 20, loss: 0.011132888644957841\n",
      "epoch: 21, loss: 0.009176583214602324\n",
      "epoch: 22, loss: 0.0076194459216354146\n",
      "epoch: 23, loss: 0.00690299776399492\n",
      "epoch: 24, loss: 0.00728974999316413\n",
      "epoch: 25, loss: 0.008139577316186093\n",
      "epoch: 26, loss: 0.008865940114892647\n",
      "epoch: 27, loss: 0.008650162426595222\n",
      "epoch: 28, loss: 0.007997945173021818\n",
      "epoch: 29, loss: 0.0066183572051934005\n",
      "epoch: 30, loss: 0.005849594054901331\n",
      "epoch: 31, loss: 0.005494852356553348\n",
      "epoch: 32, loss: 0.005786395725767307\n",
      "epoch: 33, loss: 0.006395573975814393\n",
      "epoch: 34, loss: 0.0065901126507634565\n",
      "epoch: 35, loss: 0.0064165728995775415\n",
      "epoch: 36, loss: 0.006418643611834962\n",
      "epoch: 37, loss: 0.0058406386065988965\n",
      "epoch: 38, loss: 0.005180538850044064\n",
      "epoch: 39, loss: 0.004520449401240206\n",
      "epoch: 40, loss: 0.004189149654370085\n",
      "epoch: 41, loss: 0.004076078207743329\n",
      "epoch: 42, loss: 0.004011155693219713\n",
      "epoch: 43, loss: 0.0037649576986859812\n",
      "epoch: 44, loss: 0.0034227079513770627\n",
      "epoch: 45, loss: 0.00287741298487443\n",
      "epoch: 46, loss: 0.0024094408625540526\n",
      "epoch: 47, loss: 0.002076537003658044\n",
      "epoch: 48, loss: 0.0021565642816006904\n",
      "epoch: 49, loss: 0.002195833755807942\n",
      "epoch: 50, loss: 0.0021362410499689392\n",
      "epoch: 51, loss: 0.001984440612644748\n",
      "epoch: 52, loss: 0.0016843120187510652\n",
      "epoch: 53, loss: 0.0014221916846580893\n",
      "epoch: 54, loss: 0.0013672702780831028\n",
      "epoch: 55, loss: 0.001365689397797342\n",
      "epoch: 56, loss: 0.0012610407068495766\n",
      "epoch: 57, loss: 0.001250219287122894\n",
      "epoch: 58, loss: 0.0011621220558425062\n",
      "epoch: 59, loss: 0.0010389943065282946\n",
      "epoch: 60, loss: 0.000887595081611819\n",
      "epoch: 61, loss: 0.0008619643903218958\n",
      "epoch: 62, loss: 0.0008503535404496436\n",
      "epoch: 63, loss: 0.0008431302670900534\n",
      "epoch: 64, loss: 0.0007787728867604437\n",
      "epoch: 65, loss: 0.0008709238156291879\n",
      "epoch: 66, loss: 0.0007862313713064794\n",
      "epoch: 67, loss: 0.0009049656140673516\n",
      "epoch: 68, loss: 0.0008785946517255974\n",
      "epoch: 69, loss: 0.0008285387885497386\n",
      "epoch: 70, loss: 0.0007483607596746536\n",
      "epoch: 71, loss: 0.0006960486906147774\n",
      "epoch: 72, loss: 0.0007056752970856105\n",
      "epoch: 73, loss: 0.0006886053642336634\n",
      "epoch: 74, loss: 0.0007345815916255575\n",
      "epoch: 75, loss: 0.0006372830257389366\n",
      "epoch: 76, loss: 0.000679296022309861\n",
      "epoch: 77, loss: 0.0007464110062339687\n",
      "epoch: 78, loss: 0.0007240847198342394\n",
      "epoch: 79, loss: 0.000631817744458741\n",
      "epoch: 80, loss: 0.0006832294684068194\n",
      "epoch: 81, loss: 0.0005904539516290492\n",
      "epoch: 82, loss: 0.0006177245600753555\n",
      "epoch: 83, loss: 0.0006368629694995381\n",
      "epoch: 84, loss: 0.0005795843315386253\n",
      "epoch: 85, loss: 0.0006545084052349004\n",
      "epoch: 86, loss: 0.000636594732910998\n",
      "epoch: 87, loss: 0.0006333024922710534\n",
      "epoch: 88, loss: 0.0006412968486774688\n",
      "epoch: 89, loss: 0.0006303005919804822\n",
      "epoch: 90, loss: 0.0006003180188992257\n",
      "epoch: 91, loss: 0.0005210293350888334\n",
      "epoch: 92, loss: 0.0005828235544740743\n",
      "epoch: 93, loss: 0.0005729581069072759\n",
      "epoch: 94, loss: 0.000561362783534481\n",
      "epoch: 95, loss: 0.0006358294658792145\n",
      "epoch: 96, loss: 0.0005573461646702418\n",
      "epoch: 97, loss: 0.0006095368605173214\n",
      "epoch: 98, loss: 0.0006096211186224545\n",
      "epoch: 99, loss: 0.000556796432000004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a224e1d77e54e9e82926fb2dae66e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06806732891987273\n",
      "epoch: 1, loss: 0.048523919893759954\n",
      "epoch: 2, loss: 0.03850824359235566\n",
      "epoch: 3, loss: 0.03047752153860479\n",
      "epoch: 4, loss: 0.02668904975101326\n",
      "epoch: 5, loss: 0.021264848212545075\n",
      "epoch: 6, loss: 0.017340036854786255\n",
      "epoch: 7, loss: 0.01581516305574836\n",
      "epoch: 8, loss: 0.013685921598050554\n",
      "epoch: 9, loss: 0.012491711073738768\n",
      "epoch: 10, loss: 0.011631738634800645\n",
      "epoch: 11, loss: 0.009764002925389727\n",
      "epoch: 12, loss: 0.007274822244455788\n",
      "epoch: 13, loss: 0.006164936264467139\n",
      "epoch: 14, loss: 0.006486906633319545\n",
      "epoch: 15, loss: 0.007076319959365752\n",
      "epoch: 16, loss: 0.006510490904491298\n",
      "epoch: 17, loss: 0.005564422391737256\n",
      "epoch: 18, loss: 0.004793257761674034\n",
      "epoch: 19, loss: 0.003739347517184456\n",
      "epoch: 20, loss: 0.0030600901295922195\n",
      "epoch: 21, loss: 0.0027353583501883794\n",
      "epoch: 22, loss: 0.0024264332847143827\n",
      "epoch: 23, loss: 0.0022493889310988657\n",
      "epoch: 24, loss: 0.0021369817699039735\n",
      "epoch: 25, loss: 0.001988184321723517\n",
      "epoch: 26, loss: 0.00173696484372946\n",
      "epoch: 27, loss: 0.0014470747667055003\n",
      "epoch: 28, loss: 0.0012489513599583707\n",
      "epoch: 29, loss: 0.0010750677984835589\n",
      "epoch: 30, loss: 0.0011133634668960355\n",
      "epoch: 31, loss: 0.0011094500934308493\n",
      "epoch: 32, loss: 0.001096131098160302\n",
      "epoch: 33, loss: 0.0008309336346535781\n",
      "epoch: 34, loss: 0.0007076711804779353\n",
      "epoch: 35, loss: 0.0006487428425674284\n",
      "epoch: 36, loss: 0.0005270809154210438\n",
      "epoch: 37, loss: 0.0004821114257257425\n",
      "epoch: 38, loss: 0.0005224709394860772\n",
      "epoch: 39, loss: 0.0006332627395107879\n",
      "epoch: 40, loss: 0.0004449062801186436\n",
      "epoch: 41, loss: 0.00038566406217893596\n",
      "epoch: 42, loss: 0.0004817545266330458\n",
      "epoch: 43, loss: 0.0004617311770658491\n",
      "epoch: 44, loss: 0.00040150247988003857\n",
      "epoch: 45, loss: 0.00041018583742419794\n",
      "epoch: 46, loss: 0.000408212710638536\n",
      "epoch: 47, loss: 0.0004277927381337674\n",
      "epoch: 48, loss: 0.000399188883148412\n",
      "epoch: 49, loss: 0.0003104785920021233\n",
      "epoch: 50, loss: 0.00033938569214130005\n",
      "epoch: 51, loss: 0.00031583398085642116\n",
      "epoch: 52, loss: 0.0002828233328633552\n",
      "epoch: 53, loss: 0.0002932888665461818\n",
      "epoch: 54, loss: 0.0002419720969274977\n",
      "epoch: 55, loss: 0.000282191902927053\n",
      "epoch: 56, loss: 0.0002748970434127456\n",
      "epoch: 57, loss: 0.00022212695762607038\n",
      "epoch: 58, loss: 0.0002583284961820356\n",
      "epoch: 59, loss: 0.0002847570613484801\n",
      "epoch: 60, loss: 0.00021047395418211947\n",
      "epoch: 61, loss: 0.00020600971456471003\n",
      "epoch: 62, loss: 0.0002760758204471702\n",
      "epoch: 63, loss: 0.00022504248062125043\n",
      "epoch: 64, loss: 0.0002729970861239661\n",
      "epoch: 65, loss: 0.00024448652900453725\n",
      "epoch: 66, loss: 0.00022433410240175033\n",
      "epoch: 67, loss: 0.00025447880510852963\n",
      "epoch: 68, loss: 0.00021371402117150703\n",
      "epoch: 69, loss: 0.0002971952412714417\n",
      "epoch: 70, loss: 0.00021954374940792002\n",
      "epoch: 71, loss: 0.00023380863523766649\n",
      "epoch: 72, loss: 0.00022062527230515188\n",
      "epoch: 73, loss: 0.00023732629629673113\n",
      "epoch: 74, loss: 0.00022662344811473238\n",
      "epoch: 75, loss: 0.0002084173194073172\n",
      "epoch: 76, loss: 0.0002685480287846114\n",
      "epoch: 77, loss: 0.00019586766850906924\n",
      "epoch: 78, loss: 0.00023545254744380763\n",
      "epoch: 79, loss: 0.00018766008968471723\n",
      "epoch: 80, loss: 0.0002192571051586607\n",
      "epoch: 81, loss: 0.00019473274301504736\n",
      "epoch: 82, loss: 0.00019699770486295435\n",
      "epoch: 83, loss: 0.00017296233121792028\n",
      "epoch: 84, loss: 0.00025873048506514494\n",
      "epoch: 85, loss: 0.00015662997432656678\n",
      "epoch: 86, loss: 0.00023255755535396043\n",
      "epoch: 87, loss: 0.0002220200529255384\n",
      "epoch: 88, loss: 0.00014726146434262879\n",
      "epoch: 89, loss: 0.00017674107471877788\n",
      "epoch: 90, loss: 0.00020305295154983833\n",
      "epoch: 91, loss: 0.0002101741788569952\n",
      "epoch: 92, loss: 0.0002238120608711066\n",
      "epoch: 93, loss: 0.00015700001772421077\n",
      "epoch: 94, loss: 0.00019918139456806219\n",
      "epoch: 95, loss: 0.00023529773518863857\n",
      "epoch: 96, loss: 0.00019827267383587915\n",
      "epoch: 97, loss: 0.00021862165385226775\n",
      "epoch: 98, loss: 0.00020451660984952103\n",
      "epoch: 99, loss: 0.0002131234701383268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d894f5ad5ea749cea6f65ec79386d3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.21694068639594136\n",
      "epoch: 1, loss: 0.13580352319254455\n",
      "epoch: 2, loss: 0.11815435459277626\n",
      "epoch: 3, loss: 0.12816744043262593\n",
      "epoch: 4, loss: 0.11872886541322733\n",
      "epoch: 5, loss: 0.09146688297801328\n",
      "epoch: 6, loss: 0.0639312843760242\n",
      "epoch: 7, loss: 0.040299764934213854\n",
      "epoch: 8, loss: 0.02710181041980716\n",
      "epoch: 9, loss: 0.021826717816904893\n",
      "epoch: 10, loss: 0.02372565966234022\n",
      "epoch: 11, loss: 0.026872516886843504\n",
      "epoch: 12, loss: 0.024318455531925288\n",
      "epoch: 13, loss: 0.016647527758338585\n",
      "epoch: 14, loss: 0.01263815735392868\n",
      "epoch: 15, loss: 0.014774117846204297\n",
      "epoch: 16, loss: 0.017218420845583466\n",
      "epoch: 17, loss: 0.015109134049271193\n",
      "epoch: 18, loss: 0.011875130048937501\n",
      "epoch: 19, loss: 0.010672145942674209\n",
      "epoch: 20, loss: 0.010696732936466795\n",
      "epoch: 21, loss: 0.009924812220776135\n",
      "epoch: 22, loss: 0.007476018647620283\n",
      "epoch: 23, loss: 0.004981425909268674\n",
      "epoch: 24, loss: 0.0035332455377565025\n",
      "epoch: 25, loss: 0.0038344368509564087\n",
      "epoch: 26, loss: 0.00476222166277655\n",
      "epoch: 27, loss: 0.005494945789496234\n",
      "epoch: 28, loss: 0.005673591632265741\n",
      "epoch: 29, loss: 0.00528884541047567\n",
      "epoch: 30, loss: 0.004760673163215796\n",
      "epoch: 31, loss: 0.004142161374305881\n",
      "epoch: 32, loss: 0.0034386468456079904\n",
      "epoch: 33, loss: 0.002880178645139829\n",
      "epoch: 34, loss: 0.0026584228092549358\n",
      "epoch: 35, loss: 0.00282527924590594\n",
      "epoch: 36, loss: 0.002962824029209884\n",
      "epoch: 37, loss: 0.0029987062334823045\n",
      "epoch: 38, loss: 0.0027260279316750013\n",
      "epoch: 39, loss: 0.001978057140320958\n",
      "epoch: 40, loss: 0.001552405802638323\n",
      "epoch: 41, loss: 0.0015085286578143042\n",
      "epoch: 42, loss: 0.0015182093244109967\n",
      "epoch: 43, loss: 0.0018127273530869168\n",
      "epoch: 44, loss: 0.0017457401017324141\n",
      "epoch: 45, loss: 0.00185208025853047\n",
      "epoch: 46, loss: 0.0018237746725990403\n",
      "epoch: 47, loss: 0.0017326284570620448\n",
      "epoch: 48, loss: 0.0015464017143708548\n",
      "epoch: 49, loss: 0.001575288984014653\n",
      "epoch: 50, loss: 0.0013097482021886093\n",
      "epoch: 51, loss: 0.0015282470550953163\n",
      "epoch: 52, loss: 0.0011950791241190834\n",
      "epoch: 53, loss: 0.0014711195676390396\n",
      "epoch: 54, loss: 0.0014084046845649247\n",
      "epoch: 55, loss: 0.0012291769427851742\n",
      "epoch: 56, loss: 0.0011471927168737195\n",
      "epoch: 57, loss: 0.001160703125135433\n",
      "epoch: 58, loss: 0.0011183702914436188\n",
      "epoch: 59, loss: 0.0010777543048791647\n",
      "epoch: 60, loss: 0.0011211191007949825\n",
      "epoch: 61, loss: 0.0010958366991904377\n",
      "epoch: 62, loss: 0.0011496236471985272\n",
      "epoch: 63, loss: 0.0011797512517290865\n",
      "epoch: 64, loss: 0.0011441578118080167\n",
      "epoch: 65, loss: 0.0010925935839627204\n",
      "epoch: 66, loss: 0.00105055547658699\n",
      "epoch: 67, loss: 0.001057205679639739\n",
      "epoch: 68, loss: 0.0010875430193740953\n",
      "epoch: 69, loss: 0.0010539781618428051\n",
      "epoch: 70, loss: 0.001094190602498808\n",
      "epoch: 71, loss: 0.0010335867182178604\n",
      "epoch: 72, loss: 0.0011647953123719052\n",
      "epoch: 73, loss: 0.0010564166472640125\n",
      "epoch: 74, loss: 0.001080012792723172\n",
      "epoch: 75, loss: 0.0008578462326025889\n",
      "epoch: 76, loss: 0.000965220778271379\n",
      "epoch: 77, loss: 0.0010212326891815202\n",
      "epoch: 78, loss: 0.0009905618259880694\n",
      "epoch: 79, loss: 0.0008972718090157636\n",
      "epoch: 80, loss: 0.0009544483515357835\n",
      "epoch: 81, loss: 0.0010200053011734645\n",
      "epoch: 82, loss: 0.0010429502529996054\n",
      "epoch: 83, loss: 0.0010229064999266269\n",
      "epoch: 84, loss: 0.0010046531080190717\n",
      "epoch: 85, loss: 0.0009251095319281304\n",
      "epoch: 86, loss: 0.0009992284271547153\n",
      "epoch: 87, loss: 0.000923680321637653\n",
      "epoch: 88, loss: 0.0009588606885601691\n",
      "epoch: 89, loss: 0.000933193641722436\n",
      "epoch: 90, loss: 0.0009669181984995085\n",
      "epoch: 91, loss: 0.0009099209615962651\n",
      "epoch: 92, loss: 0.0010106777818274179\n",
      "epoch: 93, loss: 0.000955122701653693\n",
      "epoch: 94, loss: 0.00098168672649079\n",
      "epoch: 95, loss: 0.0009104576127157878\n",
      "epoch: 96, loss: 0.00104927666155917\n",
      "epoch: 97, loss: 0.000911034317552768\n",
      "epoch: 98, loss: 0.0009141536627925924\n",
      "epoch: 99, loss: 0.000823868383218538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f24e859eaf3461982107878ff405fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.10981365468400815\n",
      "epoch: 1, loss: 0.0713347635256225\n",
      "epoch: 2, loss: 0.049664292145489064\n",
      "epoch: 3, loss: 0.04109782907396142\n",
      "epoch: 4, loss: 0.03253042506757932\n",
      "epoch: 5, loss: 0.025942639889992604\n",
      "epoch: 6, loss: 0.016271182510933176\n",
      "epoch: 7, loss: 0.006511704730126654\n",
      "epoch: 8, loss: 0.0029969379375999676\n",
      "epoch: 9, loss: 0.006953356050461051\n",
      "epoch: 10, loss: 0.01052339550202905\n",
      "epoch: 11, loss: 0.010822424988053731\n",
      "epoch: 12, loss: 0.01026531491703031\n",
      "epoch: 13, loss: 0.009137776853567306\n",
      "epoch: 14, loss: 0.00692650925438545\n",
      "epoch: 15, loss: 0.0035047518880892002\n",
      "epoch: 16, loss: 0.002377005898644525\n",
      "epoch: 17, loss: 0.002876630429227176\n",
      "epoch: 18, loss: 0.0033836424607317076\n",
      "epoch: 19, loss: 0.0031778940143213524\n",
      "epoch: 20, loss: 0.0028655185404930484\n",
      "epoch: 21, loss: 0.002317753699807831\n",
      "epoch: 22, loss: 0.002248753561220693\n",
      "epoch: 23, loss: 0.0026157428547240996\n",
      "epoch: 24, loss: 0.0029402775708167496\n",
      "epoch: 25, loss: 0.002346541116049836\n",
      "epoch: 26, loss: 0.0016790196251008998\n",
      "epoch: 27, loss: 0.0012610178132931484\n",
      "epoch: 28, loss: 0.0015592497182288522\n",
      "epoch: 29, loss: 0.0017603092187815658\n",
      "epoch: 30, loss: 0.0017006601447669607\n",
      "epoch: 31, loss: 0.0012512535902364796\n",
      "epoch: 32, loss: 0.0009290772118211625\n",
      "epoch: 33, loss: 0.0010550067130918256\n",
      "epoch: 34, loss: 0.0013513814687888398\n",
      "epoch: 35, loss: 0.0012804763074116146\n",
      "epoch: 36, loss: 0.0010139958530398265\n",
      "epoch: 37, loss: 0.0007657172925684819\n",
      "epoch: 38, loss: 0.0007277385031970329\n",
      "epoch: 39, loss: 0.0009670696595063935\n",
      "epoch: 40, loss: 0.0009411213095123497\n",
      "epoch: 41, loss: 0.000760370953503749\n",
      "epoch: 42, loss: 0.0006472543835048737\n",
      "epoch: 43, loss: 0.0006998958592755269\n",
      "epoch: 44, loss: 0.0007236799044009335\n",
      "epoch: 45, loss: 0.0008009335205918562\n",
      "epoch: 46, loss: 0.0006672974374514354\n",
      "epoch: 47, loss: 0.0006197752109261502\n",
      "epoch: 48, loss: 0.0006152860093511385\n",
      "epoch: 49, loss: 0.0005605746754295794\n",
      "epoch: 50, loss: 0.0005802228657725371\n",
      "epoch: 51, loss: 0.0006406910514991245\n",
      "epoch: 52, loss: 0.0005340495717152851\n",
      "epoch: 53, loss: 0.0005645125117281886\n",
      "epoch: 54, loss: 0.0005317134802279538\n",
      "epoch: 55, loss: 0.00048266909409660304\n",
      "epoch: 56, loss: 0.0004902913300034472\n",
      "epoch: 57, loss: 0.0005006582656598626\n",
      "epoch: 58, loss: 0.0004306326831361705\n",
      "epoch: 59, loss: 0.00044954853392484046\n",
      "epoch: 60, loss: 0.0004482236974901948\n",
      "epoch: 61, loss: 0.0004456789227052859\n",
      "epoch: 62, loss: 0.00042566332293825275\n",
      "epoch: 63, loss: 0.0004412341055614194\n",
      "epoch: 64, loss: 0.00040081451041971\n",
      "epoch: 65, loss: 0.0004236608577990006\n",
      "epoch: 66, loss: 0.000394840361949265\n",
      "epoch: 67, loss: 0.00043719000158852077\n",
      "epoch: 68, loss: 0.000451057727069623\n",
      "epoch: 69, loss: 0.000432344441502366\n",
      "epoch: 70, loss: 0.000418778600701279\n",
      "epoch: 71, loss: 0.00038800535880523164\n",
      "epoch: 72, loss: 0.00039003149954927813\n",
      "epoch: 73, loss: 0.00040000135578609837\n",
      "epoch: 74, loss: 0.0003600316023187551\n",
      "epoch: 75, loss: 0.0003964918690303042\n",
      "epoch: 76, loss: 0.00040865323170415524\n",
      "epoch: 77, loss: 0.0004130484795001001\n",
      "epoch: 78, loss: 0.00041834190027864685\n",
      "epoch: 79, loss: 0.0003672736304074339\n",
      "epoch: 80, loss: 0.0003992173617250098\n",
      "epoch: 81, loss: 0.0003195851812034347\n",
      "epoch: 82, loss: 0.0003330949472834258\n",
      "epoch: 83, loss: 0.00035191991831437194\n",
      "epoch: 84, loss: 0.0003430177380869031\n",
      "epoch: 85, loss: 0.0003230863587178941\n",
      "epoch: 86, loss: 0.0003589000747236844\n",
      "epoch: 87, loss: 0.0004143456672103628\n",
      "epoch: 88, loss: 0.000358428466150317\n",
      "epoch: 89, loss: 0.00042442927633043936\n",
      "epoch: 90, loss: 0.0003554945762557974\n",
      "epoch: 91, loss: 0.00036757189304370845\n",
      "epoch: 92, loss: 0.0003461748528149051\n",
      "epoch: 93, loss: 0.00036340254125971465\n",
      "epoch: 94, loss: 0.0003980150005240085\n",
      "epoch: 95, loss: 0.00032265256700495236\n",
      "epoch: 96, loss: 0.00040108918109778665\n",
      "epoch: 97, loss: 0.00037272314134209725\n",
      "epoch: 98, loss: 0.00028653458221643025\n",
      "epoch: 99, loss: 0.000309434585303996\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [1, 4],\n",
    "                         dim = [1, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_1D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [1, 6, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_1D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 10\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.75]])\n",
    "var1 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean2 = np.array([[0.75, 0.25]])\n",
    "var2 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean3 = np.array([[0.25, 0.25]])\n",
    "var3 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "mean4 = np.array([[0.75, 0.75]])\n",
    "var4 = np.array([[0.02, 0], [0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var1) + gaussian(x, mean2, var2) - gaussian(x, mean3, var3) - gaussian(x, mean4, var4)\n",
    "\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe22a26b84eb4e90845e92f819394a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8043db81215e4f029f6ca2a0718ef2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07707288143267944\n",
      "epoch: 1, loss: 0.05347516993034528\n",
      "epoch: 2, loss: 0.04750162116162681\n",
      "epoch: 3, loss: 0.04758920707744973\n",
      "epoch: 4, loss: 0.04652063135188234\n",
      "epoch: 5, loss: 0.045590747364134925\n",
      "epoch: 6, loss: 0.04435377568369626\n",
      "epoch: 7, loss: 0.04396006188788742\n",
      "epoch: 8, loss: 0.04389305842707489\n",
      "epoch: 9, loss: 0.042398918363536924\n",
      "epoch: 10, loss: 0.04116334529666015\n",
      "epoch: 11, loss: 0.03980305350642224\n",
      "epoch: 12, loss: 0.039832020021053\n",
      "epoch: 13, loss: 0.040045732715591864\n",
      "epoch: 14, loss: 0.04003255086854828\n",
      "epoch: 15, loss: 0.039519044818994396\n",
      "epoch: 16, loss: 0.038237090256180155\n",
      "epoch: 17, loss: 0.03727242003053456\n",
      "epoch: 18, loss: 0.03627162353642508\n",
      "epoch: 19, loss: 0.0345521884267229\n",
      "epoch: 20, loss: 0.03349925869248559\n",
      "epoch: 21, loss: 0.032121058045531926\n",
      "epoch: 22, loss: 0.03155403742675501\n",
      "epoch: 23, loss: 0.0309681466868846\n",
      "epoch: 24, loss: 0.031216721532496515\n",
      "epoch: 25, loss: 0.030048144986861883\n",
      "epoch: 26, loss: 0.028929040818432103\n",
      "epoch: 27, loss: 0.02756621603076067\n",
      "epoch: 28, loss: 0.02618996554238763\n",
      "epoch: 29, loss: 0.02450898073003226\n",
      "epoch: 30, loss: 0.022495444345284287\n",
      "epoch: 31, loss: 0.021032768582058766\n",
      "epoch: 32, loss: 0.021355589295189655\n",
      "epoch: 33, loss: 0.022427712564644785\n",
      "epoch: 34, loss: 0.021364974236508533\n",
      "epoch: 35, loss: 0.018900186894946124\n",
      "epoch: 36, loss: 0.017295371928070374\n",
      "epoch: 37, loss: 0.017645226192749795\n",
      "epoch: 38, loss: 0.016575101282833918\n",
      "epoch: 39, loss: 0.015733437044473726\n",
      "epoch: 40, loss: 0.015209102201085544\n",
      "epoch: 41, loss: 0.014700944447389244\n",
      "epoch: 42, loss: 0.01569784682834567\n",
      "epoch: 43, loss: 0.015816988104193068\n",
      "epoch: 44, loss: 0.01617582800659219\n",
      "epoch: 45, loss: 0.016447342332455952\n",
      "epoch: 46, loss: 0.016394316702336768\n",
      "epoch: 47, loss: 0.016003459925442755\n",
      "epoch: 48, loss: 0.015465437161868883\n",
      "epoch: 49, loss: 0.015402928817467455\n",
      "epoch: 50, loss: 0.015302450673553585\n",
      "epoch: 51, loss: 0.014890584878078721\n",
      "epoch: 52, loss: 0.014291725461658711\n",
      "epoch: 53, loss: 0.013358180897869509\n",
      "epoch: 54, loss: 0.013406836523505708\n",
      "epoch: 55, loss: 0.013171465432185538\n",
      "epoch: 56, loss: 0.013103957459175556\n",
      "epoch: 57, loss: 0.012923727216735147\n",
      "epoch: 58, loss: 0.01248730467519351\n",
      "epoch: 59, loss: 0.01195814103534136\n",
      "epoch: 60, loss: 0.011793152831012128\n",
      "epoch: 61, loss: 0.01224595348829799\n",
      "epoch: 62, loss: 0.01216366925866285\n",
      "epoch: 63, loss: 0.012039696409645544\n",
      "epoch: 64, loss: 0.012133202894767851\n",
      "epoch: 65, loss: 0.012288344408532732\n",
      "epoch: 66, loss: 0.011988320195272956\n",
      "epoch: 67, loss: 0.011349250285668824\n",
      "epoch: 68, loss: 0.011555832111024945\n",
      "epoch: 69, loss: 0.01180920902771067\n",
      "epoch: 70, loss: 0.011815625141558287\n",
      "epoch: 71, loss: 0.011936755215681305\n",
      "epoch: 72, loss: 0.011554951574128313\n",
      "epoch: 73, loss: 0.011580617367143569\n",
      "epoch: 74, loss: 0.011848281474767302\n",
      "epoch: 75, loss: 0.011587446754623825\n",
      "epoch: 76, loss: 0.011652796211109817\n",
      "epoch: 77, loss: 0.011605520037646325\n",
      "epoch: 78, loss: 0.011520118914552594\n",
      "epoch: 79, loss: 0.011788228135808099\n",
      "epoch: 80, loss: 0.011752339028631686\n",
      "epoch: 81, loss: 0.011360584879840603\n",
      "epoch: 82, loss: 0.011721256486666602\n",
      "epoch: 83, loss: 0.011450054363487871\n",
      "epoch: 84, loss: 0.011632664355604116\n",
      "epoch: 85, loss: 0.01116908971184628\n",
      "epoch: 86, loss: 0.01150112483375562\n",
      "epoch: 87, loss: 0.011601716121803355\n",
      "epoch: 88, loss: 0.011695055887544508\n",
      "epoch: 89, loss: 0.01132234411064885\n",
      "epoch: 90, loss: 0.011623337233651877\n",
      "epoch: 91, loss: 0.011399779082998307\n",
      "epoch: 92, loss: 0.011691127548963356\n",
      "epoch: 93, loss: 0.01128153855673337\n",
      "epoch: 94, loss: 0.011319316198141941\n",
      "epoch: 95, loss: 0.011524424990480602\n",
      "epoch: 96, loss: 0.011016975752001685\n",
      "epoch: 97, loss: 0.01135732907257566\n",
      "epoch: 98, loss: 0.011081802801048792\n",
      "epoch: 99, loss: 0.011155466040388617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599a129fb25b49e1abe59bc6efd30512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09139942274357837\n",
      "epoch: 1, loss: 0.06739328986341259\n",
      "epoch: 2, loss: 0.054808855096896786\n",
      "epoch: 3, loss: 0.04859066630841978\n",
      "epoch: 4, loss: 0.04555004470664049\n",
      "epoch: 5, loss: 0.043783925120103505\n",
      "epoch: 6, loss: 0.043742559774875484\n",
      "epoch: 7, loss: 0.043534208591354116\n",
      "epoch: 8, loss: 0.04389242105363696\n",
      "epoch: 9, loss: 0.04317879662415173\n",
      "epoch: 10, loss: 0.04206608902200834\n",
      "epoch: 11, loss: 0.040443167506498616\n",
      "epoch: 12, loss: 0.03780917177699594\n",
      "epoch: 13, loss: 0.0357544417365621\n",
      "epoch: 14, loss: 0.034024433315659935\n",
      "epoch: 15, loss: 0.034321178593653315\n",
      "epoch: 16, loss: 0.03370172518101979\n",
      "epoch: 17, loss: 0.03198233404914391\n",
      "epoch: 18, loss: 0.030731872329880314\n",
      "epoch: 19, loss: 0.029279486488676728\n",
      "epoch: 20, loss: 0.028950173801420974\n",
      "epoch: 21, loss: 0.029163241035447974\n",
      "epoch: 22, loss: 0.028399976035197422\n",
      "epoch: 23, loss: 0.02755482672740641\n",
      "epoch: 24, loss: 0.026457816914202955\n",
      "epoch: 25, loss: 0.023954101724522076\n",
      "epoch: 26, loss: 0.02215866679591511\n",
      "epoch: 27, loss: 0.02088516598976647\n",
      "epoch: 28, loss: 0.02005454630554143\n",
      "epoch: 29, loss: 0.018381081534102367\n",
      "epoch: 30, loss: 0.016874795135626175\n",
      "epoch: 31, loss: 0.016789617664904173\n",
      "epoch: 32, loss: 0.01696912803138792\n",
      "epoch: 33, loss: 0.016500306550138677\n",
      "epoch: 34, loss: 0.0159631267687904\n",
      "epoch: 35, loss: 0.016375374994702204\n",
      "epoch: 36, loss: 0.016766711026535718\n",
      "epoch: 37, loss: 0.01628614098630904\n",
      "epoch: 38, loss: 0.015180913401465736\n",
      "epoch: 39, loss: 0.015923355882611772\n",
      "epoch: 40, loss: 0.01644068512900553\n",
      "epoch: 41, loss: 0.015432892899587143\n",
      "epoch: 42, loss: 0.01524481509204467\n",
      "epoch: 43, loss: 0.01507453337336177\n",
      "epoch: 44, loss: 0.015301012718239599\n",
      "epoch: 45, loss: 0.01522809427391001\n",
      "epoch: 46, loss: 0.01519867536634767\n",
      "epoch: 47, loss: 0.015143218643496572\n",
      "epoch: 48, loss: 0.015091442898257954\n",
      "epoch: 49, loss: 0.015175956023710066\n",
      "epoch: 50, loss: 0.015304503621782988\n",
      "epoch: 51, loss: 0.015335451025527767\n",
      "epoch: 52, loss: 0.015242549793719614\n",
      "epoch: 53, loss: 0.014833459531846265\n",
      "epoch: 54, loss: 0.014820915781037758\n",
      "epoch: 55, loss: 0.014345260580990584\n",
      "epoch: 56, loss: 0.01514382383597921\n",
      "epoch: 57, loss: 0.014706271173342828\n",
      "epoch: 58, loss: 0.01397740949332211\n",
      "epoch: 59, loss: 0.014594197126528914\n",
      "epoch: 60, loss: 0.01458320065458784\n",
      "epoch: 61, loss: 0.013690466910378334\n",
      "epoch: 62, loss: 0.014513597075683525\n",
      "epoch: 63, loss: 0.014236895348456867\n",
      "epoch: 64, loss: 0.014277422329460592\n",
      "epoch: 65, loss: 0.014287754190695452\n",
      "epoch: 66, loss: 0.014115010971381164\n",
      "epoch: 67, loss: 0.014613226758252726\n",
      "epoch: 68, loss: 0.014168536078624472\n",
      "epoch: 69, loss: 0.014155354098743488\n",
      "epoch: 70, loss: 0.01455967806561869\n",
      "epoch: 71, loss: 0.014099866724664865\n",
      "epoch: 72, loss: 0.014528541845323692\n",
      "epoch: 73, loss: 0.014587661495483167\n",
      "epoch: 74, loss: 0.014015443160177997\n",
      "epoch: 75, loss: 0.01405275579480823\n",
      "epoch: 76, loss: 0.013812550697711493\n",
      "epoch: 77, loss: 0.013797294136359186\n",
      "epoch: 78, loss: 0.01400077909912682\n",
      "epoch: 79, loss: 0.013758452963344348\n",
      "epoch: 80, loss: 0.013841144223487235\n",
      "epoch: 81, loss: 0.01359888605171468\n",
      "epoch: 82, loss: 0.013747179861842486\n",
      "epoch: 83, loss: 0.014377011395132745\n",
      "epoch: 84, loss: 0.013792708396860604\n",
      "epoch: 85, loss: 0.014274761208706816\n",
      "epoch: 86, loss: 0.014115800716587246\n",
      "epoch: 87, loss: 0.013623615183205269\n",
      "epoch: 88, loss: 0.014179269935254873\n",
      "epoch: 89, loss: 0.014010863035905222\n",
      "epoch: 90, loss: 0.014395363557232786\n",
      "epoch: 91, loss: 0.013880617514121635\n",
      "epoch: 92, loss: 0.014007434293606267\n",
      "epoch: 93, loss: 0.013726185584084647\n",
      "epoch: 94, loss: 0.013855217674974316\n",
      "epoch: 95, loss: 0.014040427613655173\n",
      "epoch: 96, loss: 0.013985981227053105\n",
      "epoch: 97, loss: 0.014107732121812756\n",
      "epoch: 98, loss: 0.014244201431315395\n",
      "epoch: 99, loss: 0.013782843713426392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4b63d5a4544ffe9d6442d35e92d462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0818436860610029\n",
      "epoch: 1, loss: 0.05227985658055681\n",
      "epoch: 2, loss: 0.04102115333654553\n",
      "epoch: 3, loss: 0.03661306875258623\n",
      "epoch: 4, loss: 0.030313943499181425\n",
      "epoch: 5, loss: 0.024850236444075496\n",
      "epoch: 6, loss: 0.0237595128468586\n",
      "epoch: 7, loss: 0.024052030419560354\n",
      "epoch: 8, loss: 0.023834564433776552\n",
      "epoch: 9, loss: 0.023798707302156035\n",
      "epoch: 10, loss: 0.022550895898093594\n",
      "epoch: 11, loss: 0.021100411795471955\n",
      "epoch: 12, loss: 0.018423164589283048\n",
      "epoch: 13, loss: 0.015574490888892926\n",
      "epoch: 14, loss: 0.014433969828806643\n",
      "epoch: 15, loss: 0.013182628970491803\n",
      "epoch: 16, loss: 0.013094004470033997\n",
      "epoch: 17, loss: 0.013565027332341493\n",
      "epoch: 18, loss: 0.014156237543667823\n",
      "epoch: 19, loss: 0.01464674991575607\n",
      "epoch: 20, loss: 0.014951066297110379\n",
      "epoch: 21, loss: 0.014643741288478911\n",
      "epoch: 22, loss: 0.014080782714927083\n",
      "epoch: 23, loss: 0.01300353851894042\n",
      "epoch: 24, loss: 0.012791103757036544\n",
      "epoch: 25, loss: 0.012203792874600487\n",
      "epoch: 26, loss: 0.011442212262150735\n",
      "epoch: 27, loss: 0.010923390466033429\n",
      "epoch: 28, loss: 0.011664558742155155\n",
      "epoch: 29, loss: 0.011551273898671464\n",
      "epoch: 30, loss: 0.011115267025525588\n",
      "epoch: 31, loss: 0.01146518510100209\n",
      "epoch: 32, loss: 0.01103906012705821\n",
      "epoch: 33, loss: 0.011106692143849637\n",
      "epoch: 34, loss: 0.010981854334633565\n",
      "epoch: 35, loss: 0.011043723595413637\n",
      "epoch: 36, loss: 0.011175997008631729\n",
      "epoch: 37, loss: 0.011675200923874491\n",
      "epoch: 38, loss: 0.011621654909108225\n",
      "epoch: 39, loss: 0.011412743586505078\n",
      "epoch: 40, loss: 0.011171187428282483\n",
      "epoch: 41, loss: 0.011335139276272499\n",
      "epoch: 42, loss: 0.010892402288067099\n",
      "epoch: 43, loss: 0.010741063810044606\n",
      "epoch: 44, loss: 0.010973948714651887\n",
      "epoch: 45, loss: 0.010804385939941619\n",
      "epoch: 46, loss: 0.010248318308897875\n",
      "epoch: 47, loss: 0.01094055121323255\n",
      "epoch: 48, loss: 0.01053383204395311\n",
      "epoch: 49, loss: 0.010740871354447285\n",
      "epoch: 50, loss: 0.010822134987149862\n",
      "epoch: 51, loss: 0.010695372143488036\n",
      "epoch: 52, loss: 0.011029058405047304\n",
      "epoch: 53, loss: 0.011021288744827702\n",
      "epoch: 54, loss: 0.011021699035137437\n",
      "epoch: 55, loss: 0.01056494404908711\n",
      "epoch: 56, loss: 0.010861454157637136\n",
      "epoch: 57, loss: 0.010719067645252133\n",
      "epoch: 58, loss: 0.010936764306891078\n",
      "epoch: 59, loss: 0.010538410083174625\n",
      "epoch: 60, loss: 0.010673216987072187\n",
      "epoch: 61, loss: 0.010741062047340696\n",
      "epoch: 62, loss: 0.011098421866804955\n",
      "epoch: 63, loss: 0.010967111111271521\n",
      "epoch: 64, loss: 0.010495285898956326\n",
      "epoch: 65, loss: 0.010530009553199491\n",
      "epoch: 66, loss: 0.010851613398987848\n",
      "epoch: 67, loss: 0.0105769696772212\n",
      "epoch: 68, loss: 0.01033997988593309\n",
      "epoch: 69, loss: 0.010418419875968645\n",
      "epoch: 70, loss: 0.010782235157189044\n",
      "epoch: 71, loss: 0.01041213655472262\n",
      "epoch: 72, loss: 0.010647791864166551\n",
      "epoch: 73, loss: 0.010358407235225655\n",
      "epoch: 74, loss: 0.010619274159840512\n",
      "epoch: 75, loss: 0.010921940030935525\n",
      "epoch: 76, loss: 0.010863460953145287\n",
      "epoch: 77, loss: 0.010456301713207224\n",
      "epoch: 78, loss: 0.010880305167225992\n",
      "epoch: 79, loss: 0.010692552370655015\n",
      "epoch: 80, loss: 0.010305480475933762\n",
      "epoch: 81, loss: 0.010908763740537682\n",
      "epoch: 82, loss: 0.010535006176009852\n",
      "epoch: 83, loss: 0.010820938931695017\n",
      "epoch: 84, loss: 0.010660040663387119\n",
      "epoch: 85, loss: 0.010528613031392133\n",
      "epoch: 86, loss: 0.010930148488425\n",
      "epoch: 87, loss: 0.010749614039070692\n",
      "epoch: 88, loss: 0.010547651094891601\n",
      "epoch: 89, loss: 0.010981517690641754\n",
      "epoch: 90, loss: 0.010872951093933918\n",
      "epoch: 91, loss: 0.010714703713727456\n",
      "epoch: 92, loss: 0.010942942037950798\n",
      "epoch: 93, loss: 0.010599159000602628\n",
      "epoch: 94, loss: 0.010578912148831223\n",
      "epoch: 95, loss: 0.010569555166919509\n",
      "epoch: 96, loss: 0.010887747586160006\n",
      "epoch: 97, loss: 0.0104965039993196\n",
      "epoch: 98, loss: 0.010719588809071296\n",
      "epoch: 99, loss: 0.010536691731194125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785efacee8b84c669ae7cc6cc725c7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07113454534259624\n",
      "epoch: 1, loss: 0.057049590528421824\n",
      "epoch: 2, loss: 0.049592168299606956\n",
      "epoch: 3, loss: 0.0461386753224717\n",
      "epoch: 4, loss: 0.04694719076786132\n",
      "epoch: 5, loss: 0.0478415996091126\n",
      "epoch: 6, loss: 0.04611519844965383\n",
      "epoch: 7, loss: 0.04235808686439681\n",
      "epoch: 8, loss: 0.03814701836569949\n",
      "epoch: 9, loss: 0.03501298486862672\n",
      "epoch: 10, loss: 0.033010422897775164\n",
      "epoch: 11, loss: 0.03147573539462386\n",
      "epoch: 12, loss: 0.029995702844883076\n",
      "epoch: 13, loss: 0.028770194035054413\n",
      "epoch: 14, loss: 0.025935741798315006\n",
      "epoch: 15, loss: 0.022578943025275605\n",
      "epoch: 16, loss: 0.01985688475355283\n",
      "epoch: 17, loss: 0.018105238045464843\n",
      "epoch: 18, loss: 0.018159165463205333\n",
      "epoch: 19, loss: 0.018370347386983003\n",
      "epoch: 20, loss: 0.018058455769749977\n",
      "epoch: 21, loss: 0.017473075163593282\n",
      "epoch: 22, loss: 0.01603939027524971\n",
      "epoch: 23, loss: 0.016629731343978615\n",
      "epoch: 24, loss: 0.016335377234348376\n",
      "epoch: 25, loss: 0.01657399625096061\n",
      "epoch: 26, loss: 0.016199774803068136\n",
      "epoch: 27, loss: 0.015887791655532415\n",
      "epoch: 28, loss: 0.016086941510988793\n",
      "epoch: 29, loss: 0.01602899590820415\n",
      "epoch: 30, loss: 0.01527454477057891\n",
      "epoch: 31, loss: 0.015061789050217928\n",
      "epoch: 32, loss: 0.0140906306651982\n",
      "epoch: 33, loss: 0.014291212741142385\n",
      "epoch: 34, loss: 0.013426646778278168\n",
      "epoch: 35, loss: 0.01346184676188244\n",
      "epoch: 36, loss: 0.013214698657084675\n",
      "epoch: 37, loss: 0.012793394474288702\n",
      "epoch: 38, loss: 0.012389016486685616\n",
      "epoch: 39, loss: 0.012203622277888753\n",
      "epoch: 40, loss: 0.011913280336790144\n",
      "epoch: 41, loss: 0.01256353906455875\n",
      "epoch: 42, loss: 0.01283556547800283\n",
      "epoch: 43, loss: 0.01259649607201752\n",
      "epoch: 44, loss: 0.012309059387826584\n",
      "epoch: 45, loss: 0.012429785128468884\n",
      "epoch: 46, loss: 0.011666899580910592\n",
      "epoch: 47, loss: 0.01228608714485068\n",
      "epoch: 48, loss: 0.011871948454123734\n",
      "epoch: 49, loss: 0.011926222294176815\n",
      "epoch: 50, loss: 0.012231232465606945\n",
      "epoch: 51, loss: 0.011995479286754054\n",
      "epoch: 52, loss: 0.011939136565156938\n",
      "epoch: 53, loss: 0.011713789122073217\n",
      "epoch: 54, loss: 0.011991343959669958\n",
      "epoch: 55, loss: 0.011849386116380735\n",
      "epoch: 56, loss: 0.011749753942125485\n",
      "epoch: 57, loss: 0.011529137484394247\n",
      "epoch: 58, loss: 0.011130447406970456\n",
      "epoch: 59, loss: 0.011468126616757925\n",
      "epoch: 60, loss: 0.01124490177001193\n",
      "epoch: 61, loss: 0.011075967008951842\n",
      "epoch: 62, loss: 0.010714292316241228\n",
      "epoch: 63, loss: 0.01082061060152119\n",
      "epoch: 64, loss: 0.010690730144087515\n",
      "epoch: 65, loss: 0.010874341649311397\n",
      "epoch: 66, loss: 0.010159765183165354\n",
      "epoch: 67, loss: 0.010390242189138164\n",
      "epoch: 68, loss: 0.010607092730652022\n",
      "epoch: 69, loss: 0.010066301858899786\n",
      "epoch: 70, loss: 0.01030143850671756\n",
      "epoch: 71, loss: 0.010472077799380024\n",
      "epoch: 72, loss: 0.010301641782160885\n",
      "epoch: 73, loss: 0.010074566955126634\n",
      "epoch: 74, loss: 0.010256371457963048\n",
      "epoch: 75, loss: 0.010593070236586004\n",
      "epoch: 76, loss: 0.01038644862692026\n",
      "epoch: 77, loss: 0.009864291574744334\n",
      "epoch: 78, loss: 0.010301491155896257\n",
      "epoch: 79, loss: 0.010211676555310252\n",
      "epoch: 80, loss: 0.010443250452554818\n",
      "epoch: 81, loss: 0.009986138330444507\n",
      "epoch: 82, loss: 0.009847933631480711\n",
      "epoch: 83, loss: 0.00994242109333275\n",
      "epoch: 84, loss: 0.009823073022271771\n",
      "epoch: 85, loss: 0.010167395682565448\n",
      "epoch: 86, loss: 0.010312327586905454\n",
      "epoch: 87, loss: 0.010017386995069188\n",
      "epoch: 88, loss: 0.009790965918994221\n",
      "epoch: 89, loss: 0.010132798452808692\n",
      "epoch: 90, loss: 0.009683730616415415\n",
      "epoch: 91, loss: 0.00983507828982614\n",
      "epoch: 92, loss: 0.00988531064641194\n",
      "epoch: 93, loss: 0.010097687571804969\n",
      "epoch: 94, loss: 0.010337655012371681\n",
      "epoch: 95, loss: 0.009947660356960299\n",
      "epoch: 96, loss: 0.01003169542263108\n",
      "epoch: 97, loss: 0.01006914808552939\n",
      "epoch: 98, loss: 0.01019137009097139\n",
      "epoch: 99, loss: 0.009952520443411206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46958802ae014b5d84b8e74c8c074024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.05777436789447741\n",
      "epoch: 1, loss: 0.046669687978172884\n",
      "epoch: 2, loss: 0.03972220341788151\n",
      "epoch: 3, loss: 0.033648442465327486\n",
      "epoch: 4, loss: 0.028877377541569646\n",
      "epoch: 5, loss: 0.02863055580422421\n",
      "epoch: 6, loss: 0.027645089944255758\n",
      "epoch: 7, loss: 0.02523548280106644\n",
      "epoch: 8, loss: 0.022491375843832442\n",
      "epoch: 9, loss: 0.022088123717709466\n",
      "epoch: 10, loss: 0.02279443995018955\n",
      "epoch: 11, loss: 0.02115757032663486\n",
      "epoch: 12, loss: 0.018336860897053442\n",
      "epoch: 13, loss: 0.016482962177508977\n",
      "epoch: 14, loss: 0.015503319102594355\n",
      "epoch: 15, loss: 0.015041786914364865\n",
      "epoch: 16, loss: 0.013779450861232384\n",
      "epoch: 17, loss: 0.014731245416494655\n",
      "epoch: 18, loss: 0.015390845296676448\n",
      "epoch: 19, loss: 0.015429647816355856\n",
      "epoch: 20, loss: 0.014906614055274752\n",
      "epoch: 21, loss: 0.0144047912181417\n",
      "epoch: 22, loss: 0.0140653632918351\n",
      "epoch: 23, loss: 0.013463965972732483\n",
      "epoch: 24, loss: 0.013184379224995895\n",
      "epoch: 25, loss: 0.014066342547756657\n",
      "epoch: 26, loss: 0.014379593875178634\n",
      "epoch: 27, loss: 0.013766610811326224\n",
      "epoch: 28, loss: 0.013330202568902227\n",
      "epoch: 29, loss: 0.01342054453558719\n",
      "epoch: 30, loss: 0.013247021210841212\n",
      "epoch: 31, loss: 0.013117120715101755\n",
      "epoch: 32, loss: 0.012775732171845916\n",
      "epoch: 33, loss: 0.012997458266850712\n",
      "epoch: 34, loss: 0.013022088257722433\n",
      "epoch: 35, loss: 0.012729091230017333\n",
      "epoch: 36, loss: 0.012701420838357213\n",
      "epoch: 37, loss: 0.013171636352622387\n",
      "epoch: 38, loss: 0.012874658615624545\n",
      "epoch: 39, loss: 0.012910051613528574\n",
      "epoch: 40, loss: 0.012614818990813934\n",
      "epoch: 41, loss: 0.01272758032368294\n",
      "epoch: 42, loss: 0.012477075756771419\n",
      "epoch: 43, loss: 0.0124068499689914\n",
      "epoch: 44, loss: 0.01241162043016482\n",
      "epoch: 45, loss: 0.012124202590392959\n",
      "epoch: 46, loss: 0.012468250297236512\n",
      "epoch: 47, loss: 0.012547552596531493\n",
      "epoch: 48, loss: 0.012600513567254477\n",
      "epoch: 49, loss: 0.012747351293160911\n",
      "epoch: 50, loss: 0.012597145887654829\n",
      "epoch: 51, loss: 0.011529798161084761\n",
      "epoch: 52, loss: 0.01223608640952479\n",
      "epoch: 53, loss: 0.012171914491570474\n",
      "epoch: 54, loss: 0.011981899690007103\n",
      "epoch: 55, loss: 0.012048289066319091\n",
      "epoch: 56, loss: 0.012502788300041588\n",
      "epoch: 57, loss: 0.012165143002962453\n",
      "epoch: 58, loss: 0.012444639258062322\n",
      "epoch: 59, loss: 0.012247169804288976\n",
      "epoch: 60, loss: 0.012203433340512729\n",
      "epoch: 61, loss: 0.012227796115300607\n",
      "epoch: 62, loss: 0.012332181964136442\n",
      "epoch: 63, loss: 0.012273897797026454\n",
      "epoch: 64, loss: 0.01257721951079404\n",
      "epoch: 65, loss: 0.012154571084082471\n",
      "epoch: 66, loss: 0.012214215566321032\n",
      "epoch: 67, loss: 0.012171628489453521\n",
      "epoch: 68, loss: 0.012125500351602822\n",
      "epoch: 69, loss: 0.012234189588698463\n",
      "epoch: 70, loss: 0.012041702632348111\n",
      "epoch: 71, loss: 0.012189690347279063\n",
      "epoch: 72, loss: 0.012223692727755565\n",
      "epoch: 73, loss: 0.012340785684156634\n",
      "epoch: 74, loss: 0.012293092008218291\n",
      "epoch: 75, loss: 0.012518034263234928\n",
      "epoch: 76, loss: 0.011958914752836619\n",
      "epoch: 77, loss: 0.012032490540130607\n",
      "epoch: 78, loss: 0.012381244547920119\n",
      "epoch: 79, loss: 0.012493844246129098\n",
      "epoch: 80, loss: 0.012127741137324812\n",
      "epoch: 81, loss: 0.01222782412138969\n",
      "epoch: 82, loss: 0.01170392115115886\n",
      "epoch: 83, loss: 0.011865512398291112\n",
      "epoch: 84, loss: 0.012258858078288798\n",
      "epoch: 85, loss: 0.01246363084513691\n",
      "epoch: 86, loss: 0.012277002330498728\n",
      "epoch: 87, loss: 0.012175956334300393\n",
      "epoch: 88, loss: 0.0123380965447154\n",
      "epoch: 89, loss: 0.01204128405235443\n",
      "epoch: 90, loss: 0.012194727540510299\n",
      "epoch: 91, loss: 0.012166776187205092\n",
      "epoch: 92, loss: 0.012195063161168336\n",
      "epoch: 93, loss: 0.012087079059811535\n",
      "epoch: 94, loss: 0.011992997085402517\n",
      "epoch: 95, loss: 0.0125749506170784\n",
      "epoch: 96, loss: 0.012255107622307915\n",
      "epoch: 97, loss: 0.012214185001663486\n",
      "epoch: 98, loss: 0.012091026570123835\n",
      "epoch: 99, loss: 0.011882644093205077\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadc03ac2c8042939bf0e4590216766d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a2837231c472c99952c686960505b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07937633355613255\n",
      "epoch: 1, loss: 0.03831498798896587\n",
      "epoch: 2, loss: 0.022181347978365563\n",
      "epoch: 3, loss: 0.022638149266011886\n",
      "epoch: 4, loss: 0.023110576645042676\n",
      "epoch: 5, loss: 0.01900290836112219\n",
      "epoch: 6, loss: 0.01619841082237681\n",
      "epoch: 7, loss: 0.017963479269358425\n",
      "epoch: 8, loss: 0.020679277500450875\n",
      "epoch: 9, loss: 0.021325617229711828\n",
      "epoch: 10, loss: 0.019190768738916314\n",
      "epoch: 11, loss: 0.01678060026156439\n",
      "epoch: 12, loss: 0.014724096251143135\n",
      "epoch: 13, loss: 0.014157229925133493\n",
      "epoch: 14, loss: 0.014338039761602837\n",
      "epoch: 15, loss: 0.0146181621302701\n",
      "epoch: 16, loss: 0.01357091357107778\n",
      "epoch: 17, loss: 0.012794505041668045\n",
      "epoch: 18, loss: 0.011540472281133222\n",
      "epoch: 19, loss: 0.010606045258833137\n",
      "epoch: 20, loss: 0.009685256845204819\n",
      "epoch: 21, loss: 0.009212734795578364\n",
      "epoch: 22, loss: 0.009110438123617426\n",
      "epoch: 23, loss: 0.008610076578288973\n",
      "epoch: 24, loss: 0.008443125709544983\n",
      "epoch: 25, loss: 0.008088401116752943\n",
      "epoch: 26, loss: 0.007831255978357671\n",
      "epoch: 27, loss: 0.007630450404353788\n",
      "epoch: 28, loss: 0.007364064722137801\n",
      "epoch: 29, loss: 0.0068784876095676\n",
      "epoch: 30, loss: 0.006805224229658161\n",
      "epoch: 31, loss: 0.007061253938087183\n",
      "epoch: 32, loss: 0.006759723381756753\n",
      "epoch: 33, loss: 0.006608543808112166\n",
      "epoch: 34, loss: 0.00629938909970967\n",
      "epoch: 35, loss: 0.005772233371183198\n",
      "epoch: 36, loss: 0.005613520087086007\n",
      "epoch: 37, loss: 0.005680693601332578\n",
      "epoch: 38, loss: 0.0052468144463600494\n",
      "epoch: 39, loss: 0.004915176830658312\n",
      "epoch: 40, loss: 0.005246186381685056\n",
      "epoch: 41, loss: 0.0047703637216583855\n",
      "epoch: 42, loss: 0.004779240652199978\n",
      "epoch: 43, loss: 0.0046423386763345086\n",
      "epoch: 44, loss: 0.004434570430177397\n",
      "epoch: 45, loss: 0.004503390906140269\n",
      "epoch: 46, loss: 0.004085782107805728\n",
      "epoch: 47, loss: 0.004063494527637911\n",
      "epoch: 48, loss: 0.003827667288935277\n",
      "epoch: 49, loss: 0.003723930190839745\n",
      "epoch: 50, loss: 0.0036144882146944136\n",
      "epoch: 51, loss: 0.003474768588009709\n",
      "epoch: 52, loss: 0.003582547197129356\n",
      "epoch: 53, loss: 0.0034015941977583015\n",
      "epoch: 54, loss: 0.0033414944647986496\n",
      "epoch: 55, loss: 0.0031360994800690017\n",
      "epoch: 56, loss: 0.002951393015470394\n",
      "epoch: 57, loss: 0.0032102373578829536\n",
      "epoch: 58, loss: 0.0029955395650548633\n",
      "epoch: 59, loss: 0.0031169373055150817\n",
      "epoch: 60, loss: 0.0031186877168107446\n",
      "epoch: 61, loss: 0.003024197086405235\n",
      "epoch: 62, loss: 0.003178519125128296\n",
      "epoch: 63, loss: 0.00312319047934411\n",
      "epoch: 64, loss: 0.0030374409650654175\n",
      "epoch: 65, loss: 0.0027279978484826247\n",
      "epoch: 66, loss: 0.002858526136644606\n",
      "epoch: 67, loss: 0.002862874134414428\n",
      "epoch: 68, loss: 0.002859390375357066\n",
      "epoch: 69, loss: 0.002760100061941564\n",
      "epoch: 70, loss: 0.002812732875142094\n",
      "epoch: 71, loss: 0.0027590961952986765\n",
      "epoch: 72, loss: 0.0027275706158457606\n",
      "epoch: 73, loss: 0.0028769232136951362\n",
      "epoch: 74, loss: 0.002688112876898854\n",
      "epoch: 75, loss: 0.002787356634836399\n",
      "epoch: 76, loss: 0.002654586136806308\n",
      "epoch: 77, loss: 0.002682328407938207\n",
      "epoch: 78, loss: 0.002581935499636687\n",
      "epoch: 79, loss: 0.0027786651347983865\n",
      "epoch: 80, loss: 0.0024827294285119633\n",
      "epoch: 81, loss: 0.0024675858233339973\n",
      "epoch: 82, loss: 0.002592621768968224\n",
      "epoch: 83, loss: 0.0025925768820436223\n",
      "epoch: 84, loss: 0.0026550264678070474\n",
      "epoch: 85, loss: 0.0025260381369017243\n",
      "epoch: 86, loss: 0.0025205489424689276\n",
      "epoch: 87, loss: 0.0025772356954332355\n",
      "epoch: 88, loss: 0.002399760273531276\n",
      "epoch: 89, loss: 0.002439446711534635\n",
      "epoch: 90, loss: 0.002400417633206184\n",
      "epoch: 91, loss: 0.002423022906853138\n",
      "epoch: 92, loss: 0.0023209303229551168\n",
      "epoch: 93, loss: 0.002417936156434835\n",
      "epoch: 94, loss: 0.00227878187813232\n",
      "epoch: 95, loss: 0.0022733247056353476\n",
      "epoch: 96, loss: 0.0022875149542855916\n",
      "epoch: 97, loss: 0.0023098864197499303\n",
      "epoch: 98, loss: 0.0022864766477343835\n",
      "epoch: 99, loss: 0.0022977971096075807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513ae133437d458d9973a5a0a21abf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.045130877534361666\n",
      "epoch: 1, loss: 0.04078285563424625\n",
      "epoch: 2, loss: 0.035327185583971195\n",
      "epoch: 3, loss: 0.030214227651706605\n",
      "epoch: 4, loss: 0.026523138821400876\n",
      "epoch: 5, loss: 0.025505841068935133\n",
      "epoch: 6, loss: 0.023902789410926273\n",
      "epoch: 7, loss: 0.021795474183066817\n",
      "epoch: 8, loss: 0.019073834581141834\n",
      "epoch: 9, loss: 0.017768475047005575\n",
      "epoch: 10, loss: 0.017176467625918304\n",
      "epoch: 11, loss: 0.017686899981357255\n",
      "epoch: 12, loss: 0.017910946193565345\n",
      "epoch: 13, loss: 0.017282482085746887\n",
      "epoch: 14, loss: 0.016610400611914225\n",
      "epoch: 15, loss: 0.01670650814662048\n",
      "epoch: 16, loss: 0.01646466363321694\n",
      "epoch: 17, loss: 0.01626753122117886\n",
      "epoch: 18, loss: 0.015428422923771046\n",
      "epoch: 19, loss: 0.01553362177255313\n",
      "epoch: 20, loss: 0.01498128493155369\n",
      "epoch: 21, loss: 0.014034031043599172\n",
      "epoch: 22, loss: 0.013510351750876996\n",
      "epoch: 23, loss: 0.01254256175608165\n",
      "epoch: 24, loss: 0.01213125826909472\n",
      "epoch: 25, loss: 0.011186356944513713\n",
      "epoch: 26, loss: 0.010756741383713938\n",
      "epoch: 27, loss: 0.009388238086328096\n",
      "epoch: 28, loss: 0.008622439022009041\n",
      "epoch: 29, loss: 0.009174871260580258\n",
      "epoch: 30, loss: 0.008531209604837317\n",
      "epoch: 31, loss: 0.007866916003605496\n",
      "epoch: 32, loss: 0.007782192151424613\n",
      "epoch: 33, loss: 0.007412413425450924\n",
      "epoch: 34, loss: 0.0072968676409181765\n",
      "epoch: 35, loss: 0.006894405923502538\n",
      "epoch: 36, loss: 0.006865710798626598\n",
      "epoch: 37, loss: 0.007126596548254138\n",
      "epoch: 38, loss: 0.006950160781150444\n",
      "epoch: 39, loss: 0.006473920142959061\n",
      "epoch: 40, loss: 0.006444113019027185\n",
      "epoch: 41, loss: 0.005938662562618602\n",
      "epoch: 42, loss: 0.0055316897749187145\n",
      "epoch: 43, loss: 0.005451587063683507\n",
      "epoch: 44, loss: 0.005057125706246025\n",
      "epoch: 45, loss: 0.005017364178015013\n",
      "epoch: 46, loss: 0.004773402899624476\n",
      "epoch: 47, loss: 0.004516594833008204\n",
      "epoch: 48, loss: 0.00460353500539869\n",
      "epoch: 49, loss: 0.0044120010553150455\n",
      "epoch: 50, loss: 0.004117647612643572\n",
      "epoch: 51, loss: 0.003972808768011634\n",
      "epoch: 52, loss: 0.003423489983468797\n",
      "epoch: 53, loss: 0.0033122901268741066\n",
      "epoch: 54, loss: 0.0031859283089601662\n",
      "epoch: 55, loss: 0.003086279085894686\n",
      "epoch: 56, loss: 0.0030676109999285654\n",
      "epoch: 57, loss: 0.00274727934109935\n",
      "epoch: 58, loss: 0.0028002749505526016\n",
      "epoch: 59, loss: 0.0027252447675461737\n",
      "epoch: 60, loss: 0.002780129113019822\n",
      "epoch: 61, loss: 0.002793547595839707\n",
      "epoch: 62, loss: 0.0025765980459081282\n",
      "epoch: 63, loss: 0.002509776918679592\n",
      "epoch: 64, loss: 0.002396935515453723\n",
      "epoch: 65, loss: 0.0024708862290039055\n",
      "epoch: 66, loss: 0.002542410386507494\n",
      "epoch: 67, loss: 0.0025712599665237905\n",
      "epoch: 68, loss: 0.0025795651769740403\n",
      "epoch: 69, loss: 0.0024852238820366355\n",
      "epoch: 70, loss: 0.0024369606555061842\n",
      "epoch: 71, loss: 0.00229726986137661\n",
      "epoch: 72, loss: 0.002420960053122003\n",
      "epoch: 73, loss: 0.0023216009432444635\n",
      "epoch: 74, loss: 0.002266410090179915\n",
      "epoch: 75, loss: 0.0024611486617284843\n",
      "epoch: 76, loss: 0.0023302578505428728\n",
      "epoch: 77, loss: 0.0023736453643731607\n",
      "epoch: 78, loss: 0.0022331431275643063\n",
      "epoch: 79, loss: 0.00218141539889757\n",
      "epoch: 80, loss: 0.0022652977924706675\n",
      "epoch: 81, loss: 0.002182373800662197\n",
      "epoch: 82, loss: 0.0021245932194235684\n",
      "epoch: 83, loss: 0.0022111739524149376\n",
      "epoch: 84, loss: 0.002186201226925934\n",
      "epoch: 85, loss: 0.002228620045300355\n",
      "epoch: 86, loss: 0.002192638897842046\n",
      "epoch: 87, loss: 0.002118030923492434\n",
      "epoch: 88, loss: 0.002038431810843185\n",
      "epoch: 89, loss: 0.0022874809581280332\n",
      "epoch: 90, loss: 0.002196815037573839\n",
      "epoch: 91, loss: 0.0020644812575654047\n",
      "epoch: 92, loss: 0.0021030083683691407\n",
      "epoch: 93, loss: 0.0020694344910933467\n",
      "epoch: 94, loss: 0.002139780746031517\n",
      "epoch: 95, loss: 0.0021522638206556994\n",
      "epoch: 96, loss: 0.0019285274000430423\n",
      "epoch: 97, loss: 0.0020889367153457184\n",
      "epoch: 98, loss: 0.00195751649254223\n",
      "epoch: 99, loss: 0.00197993962487085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d326244dd4ba9aedd24660e028608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.11789436944906327\n",
      "epoch: 1, loss: 0.0742881764016224\n",
      "epoch: 2, loss: 0.05249914884598337\n",
      "epoch: 3, loss: 0.04100468334745435\n",
      "epoch: 4, loss: 0.03610031254903457\n",
      "epoch: 5, loss: 0.03538791948336799\n",
      "epoch: 6, loss: 0.03679953170333918\n",
      "epoch: 7, loss: 0.03390065641908169\n",
      "epoch: 8, loss: 0.028974342203911124\n",
      "epoch: 9, loss: 0.025399484022835938\n",
      "epoch: 10, loss: 0.022932609180386594\n",
      "epoch: 11, loss: 0.02037875377916149\n",
      "epoch: 12, loss: 0.018423114253061163\n",
      "epoch: 13, loss: 0.018828174893483374\n",
      "epoch: 14, loss: 0.01919151851900834\n",
      "epoch: 15, loss: 0.019626548150269346\n",
      "epoch: 16, loss: 0.018803566194972143\n",
      "epoch: 17, loss: 0.018393596938075338\n",
      "epoch: 18, loss: 0.018279029832357664\n",
      "epoch: 19, loss: 0.016819745398555\n",
      "epoch: 20, loss: 0.016021620707307786\n",
      "epoch: 21, loss: 0.015409655682212248\n",
      "epoch: 22, loss: 0.01513699841972686\n",
      "epoch: 23, loss: 0.014330410548354098\n",
      "epoch: 24, loss: 0.013881287959243104\n",
      "epoch: 25, loss: 0.013137042020910148\n",
      "epoch: 26, loss: 0.012589923919628161\n",
      "epoch: 27, loss: 0.011979391672797017\n",
      "epoch: 28, loss: 0.011568051656679847\n",
      "epoch: 29, loss: 0.011613914838893324\n",
      "epoch: 30, loss: 0.011191446318565753\n",
      "epoch: 31, loss: 0.010976690064501632\n",
      "epoch: 32, loss: 0.011242517016858697\n",
      "epoch: 33, loss: 0.011138021405163463\n",
      "epoch: 34, loss: 0.011068904627607325\n",
      "epoch: 35, loss: 0.010937048960755695\n",
      "epoch: 36, loss: 0.010843681348839037\n",
      "epoch: 37, loss: 0.010642001422998789\n",
      "epoch: 38, loss: 0.010411581594141522\n",
      "epoch: 39, loss: 0.010180512879268746\n",
      "epoch: 40, loss: 0.010136542338841672\n",
      "epoch: 41, loss: 0.010004396990438662\n",
      "epoch: 42, loss: 0.010097827741281174\n",
      "epoch: 43, loss: 0.00991610208284776\n",
      "epoch: 44, loss: 0.01020024767474883\n",
      "epoch: 45, loss: 0.010067128912110262\n",
      "epoch: 46, loss: 0.01000202412677361\n",
      "epoch: 47, loss: 0.009899116418948762\n",
      "epoch: 48, loss: 0.009750090089214783\n",
      "epoch: 49, loss: 0.009665342183968835\n",
      "epoch: 50, loss: 0.009573359171854444\n",
      "epoch: 51, loss: 0.009478274962914236\n",
      "epoch: 52, loss: 0.009541701347714116\n",
      "epoch: 53, loss: 0.009091246754129128\n",
      "epoch: 54, loss: 0.009127469577935147\n",
      "epoch: 55, loss: 0.009090123576910122\n",
      "epoch: 56, loss: 0.008926684601506451\n",
      "epoch: 57, loss: 0.0090966028784891\n",
      "epoch: 58, loss: 0.009143817867168688\n",
      "epoch: 59, loss: 0.008710043287993461\n",
      "epoch: 60, loss: 0.009054337694675934\n",
      "epoch: 61, loss: 0.008873718565107068\n",
      "epoch: 62, loss: 0.008783919357403566\n",
      "epoch: 63, loss: 0.008660679455179236\n",
      "epoch: 64, loss: 0.008983520900331467\n",
      "epoch: 65, loss: 0.00875514765990541\n",
      "epoch: 66, loss: 0.008741280119713231\n",
      "epoch: 67, loss: 0.008767724484841685\n",
      "epoch: 68, loss: 0.008555740393376773\n",
      "epoch: 69, loss: 0.008423713776226369\n",
      "epoch: 70, loss: 0.00866578408071424\n",
      "epoch: 71, loss: 0.008370620436026491\n",
      "epoch: 72, loss: 0.008497726862110927\n",
      "epoch: 73, loss: 0.00842899683289442\n",
      "epoch: 74, loss: 0.0084525089747648\n",
      "epoch: 75, loss: 0.00865231644816592\n",
      "epoch: 76, loss: 0.008165923676508495\n",
      "epoch: 77, loss: 0.008292996387119814\n",
      "epoch: 78, loss: 0.008144995062881202\n",
      "epoch: 79, loss: 0.008310533788492362\n",
      "epoch: 80, loss: 0.008262917721167537\n",
      "epoch: 81, loss: 0.008326674126658388\n",
      "epoch: 82, loss: 0.00823910108391553\n",
      "epoch: 83, loss: 0.008052324902770363\n",
      "epoch: 84, loss: 0.008322267512428854\n",
      "epoch: 85, loss: 0.008160624371469073\n",
      "epoch: 86, loss: 0.008045516705219692\n",
      "epoch: 87, loss: 0.008309487115720491\n",
      "epoch: 88, loss: 0.008365383394542139\n",
      "epoch: 89, loss: 0.008252955030051485\n",
      "epoch: 90, loss: 0.00815607593483294\n",
      "epoch: 91, loss: 0.008184909811217438\n",
      "epoch: 92, loss: 0.008507146801983896\n",
      "epoch: 93, loss: 0.008090419913259435\n",
      "epoch: 94, loss: 0.00804181078315574\n",
      "epoch: 95, loss: 0.008169163075087629\n",
      "epoch: 96, loss: 0.007861750282596402\n",
      "epoch: 97, loss: 0.00796844794524733\n",
      "epoch: 98, loss: 0.008119707694832612\n",
      "epoch: 99, loss: 0.008135389841484963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd6aeecff6c47dca3fbbb1831bab9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.05114791188061397\n",
      "epoch: 1, loss: 0.04104214211221406\n",
      "epoch: 2, loss: 0.035705847398019755\n",
      "epoch: 3, loss: 0.028444536239311594\n",
      "epoch: 4, loss: 0.02429593041596409\n",
      "epoch: 5, loss: 0.023053987967716676\n",
      "epoch: 6, loss: 0.02109300876422965\n",
      "epoch: 7, loss: 0.01907533054539317\n",
      "epoch: 8, loss: 0.01592855504544096\n",
      "epoch: 9, loss: 0.013774576152485032\n",
      "epoch: 10, loss: 0.011796139531352329\n",
      "epoch: 11, loss: 0.011613729098660234\n",
      "epoch: 12, loss: 0.012608827614803631\n",
      "epoch: 13, loss: 0.011525611862898006\n",
      "epoch: 14, loss: 0.009615719486887126\n",
      "epoch: 15, loss: 0.008646069420437812\n",
      "epoch: 16, loss: 0.008642649273263654\n",
      "epoch: 17, loss: 0.00849098449623631\n",
      "epoch: 18, loss: 0.0084149190350387\n",
      "epoch: 19, loss: 0.007872313406754986\n",
      "epoch: 20, loss: 0.008025451410247662\n",
      "epoch: 21, loss: 0.008491198804811062\n",
      "epoch: 22, loss: 0.008092520639824385\n",
      "epoch: 23, loss: 0.007520947116020293\n",
      "epoch: 24, loss: 0.0072686241903459825\n",
      "epoch: 25, loss: 0.007255617372966094\n",
      "epoch: 26, loss: 0.007104703329551635\n",
      "epoch: 27, loss: 0.006779579582108468\n",
      "epoch: 28, loss: 0.0066012835264775485\n",
      "epoch: 29, loss: 0.006396125329359935\n",
      "epoch: 30, loss: 0.006196857152318528\n",
      "epoch: 31, loss: 0.006382801172793257\n",
      "epoch: 32, loss: 0.0062469396184305906\n",
      "epoch: 33, loss: 0.006047540759294633\n",
      "epoch: 34, loss: 0.005934491900290908\n",
      "epoch: 35, loss: 0.005833195326408069\n",
      "epoch: 36, loss: 0.005888692143700615\n",
      "epoch: 37, loss: 0.005664848901923878\n",
      "epoch: 38, loss: 0.005299011481718821\n",
      "epoch: 39, loss: 0.005484056745171868\n",
      "epoch: 40, loss: 0.005588285333072293\n",
      "epoch: 41, loss: 0.005464247294290749\n",
      "epoch: 42, loss: 0.005500645332042685\n",
      "epoch: 43, loss: 0.005270439153700502\n",
      "epoch: 44, loss: 0.004928053027136996\n",
      "epoch: 45, loss: 0.005176235752476689\n",
      "epoch: 46, loss: 0.004992248514501524\n",
      "epoch: 47, loss: 0.0049358586327508365\n",
      "epoch: 48, loss: 0.005097456619256624\n",
      "epoch: 49, loss: 0.004800198513848318\n",
      "epoch: 50, loss: 0.004733617307113008\n",
      "epoch: 51, loss: 0.0050013584340824905\n",
      "epoch: 52, loss: 0.004816854077205269\n",
      "epoch: 53, loss: 0.004827023154986552\n",
      "epoch: 54, loss: 0.0046543844662271655\n",
      "epoch: 55, loss: 0.00473093744562475\n",
      "epoch: 56, loss: 0.004714821964540662\n",
      "epoch: 57, loss: 0.004744674765179552\n",
      "epoch: 58, loss: 0.0046813215285964865\n",
      "epoch: 59, loss: 0.004495376272803336\n",
      "epoch: 60, loss: 0.004605037347066478\n",
      "epoch: 61, loss: 0.004431198189752586\n",
      "epoch: 62, loss: 0.004317146646501208\n",
      "epoch: 63, loss: 0.004291663885266009\n",
      "epoch: 64, loss: 0.004210698349311636\n",
      "epoch: 65, loss: 0.004174297819689806\n",
      "epoch: 66, loss: 0.004173613269442187\n",
      "epoch: 67, loss: 0.004038671212571824\n",
      "epoch: 68, loss: 0.003906557588337803\n",
      "epoch: 69, loss: 0.003968789860689769\n",
      "epoch: 70, loss: 0.0039029371795404905\n",
      "epoch: 71, loss: 0.003869873210232754\n",
      "epoch: 72, loss: 0.003720921947769904\n",
      "epoch: 73, loss: 0.0036309846671042064\n",
      "epoch: 74, loss: 0.003427240025643753\n",
      "epoch: 75, loss: 0.0033730970413647548\n",
      "epoch: 76, loss: 0.0031598723704895624\n",
      "epoch: 77, loss: 0.0031474149894130943\n",
      "epoch: 78, loss: 0.002975234576385706\n",
      "epoch: 79, loss: 0.0029896189663586974\n",
      "epoch: 80, loss: 0.0030706147719989666\n",
      "epoch: 81, loss: 0.002925149516762853\n",
      "epoch: 82, loss: 0.0029118299692268163\n",
      "epoch: 83, loss: 0.002877695929166772\n",
      "epoch: 84, loss: 0.0028453692990398926\n",
      "epoch: 85, loss: 0.002878468784007523\n",
      "epoch: 86, loss: 0.002764842630665968\n",
      "epoch: 87, loss: 0.0026767948410375258\n",
      "epoch: 88, loss: 0.002663114132285235\n",
      "epoch: 89, loss: 0.0026610724581288053\n",
      "epoch: 90, loss: 0.002626651578986473\n",
      "epoch: 91, loss: 0.0025875823840914793\n",
      "epoch: 92, loss: 0.002441007874520684\n",
      "epoch: 93, loss: 0.0026408768577024\n",
      "epoch: 94, loss: 0.002468056336894873\n",
      "epoch: 95, loss: 0.00243945185992161\n",
      "epoch: 96, loss: 0.0024102827455520933\n",
      "epoch: 97, loss: 0.0024343668515807737\n",
      "epoch: 98, loss: 0.0024156495655809095\n",
      "epoch: 99, loss: 0.0023525682816609023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b68e7c606941398b6307b69db90770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06276859183560227\n",
      "epoch: 1, loss: 0.0466465029227157\n",
      "epoch: 2, loss: 0.03956829697186236\n",
      "epoch: 3, loss: 0.0345501490310497\n",
      "epoch: 4, loss: 0.03290597863875298\n",
      "epoch: 5, loss: 0.03081730381331249\n",
      "epoch: 6, loss: 0.027388127825309905\n",
      "epoch: 7, loss: 0.022565122000280886\n",
      "epoch: 8, loss: 0.01742287216648025\n",
      "epoch: 9, loss: 0.015369933627191735\n",
      "epoch: 10, loss: 0.014546891178920768\n",
      "epoch: 11, loss: 0.01359177925219618\n",
      "epoch: 12, loss: 0.013014051231711494\n",
      "epoch: 13, loss: 0.012791024741424547\n",
      "epoch: 14, loss: 0.011502623492700542\n",
      "epoch: 15, loss: 0.011018704104254954\n",
      "epoch: 16, loss: 0.011099848205605256\n",
      "epoch: 17, loss: 0.01141252944831028\n",
      "epoch: 18, loss: 0.011258814624319179\n",
      "epoch: 19, loss: 0.010813424799016045\n",
      "epoch: 20, loss: 0.009967922628004106\n",
      "epoch: 21, loss: 0.01016548218086957\n",
      "epoch: 22, loss: 0.010154242479049084\n",
      "epoch: 23, loss: 0.009728654398210947\n",
      "epoch: 24, loss: 0.00896061484020562\n",
      "epoch: 25, loss: 0.008953314335836451\n",
      "epoch: 26, loss: 0.008784376451473615\n",
      "epoch: 27, loss: 0.008806002349620473\n",
      "epoch: 28, loss: 0.008713495167607433\n",
      "epoch: 29, loss: 0.008616441178487762\n",
      "epoch: 30, loss: 0.008450456588396778\n",
      "epoch: 31, loss: 0.008214650720683028\n",
      "epoch: 32, loss: 0.007798868939291211\n",
      "epoch: 33, loss: 0.00789280747478784\n",
      "epoch: 34, loss: 0.007789652326986818\n",
      "epoch: 35, loss: 0.007986253127229868\n",
      "epoch: 36, loss: 0.007921704566102363\n",
      "epoch: 37, loss: 0.0074667330125331955\n",
      "epoch: 38, loss: 0.0073542010501227014\n",
      "epoch: 39, loss: 0.007423727851376618\n",
      "epoch: 40, loss: 0.007359800315994281\n",
      "epoch: 41, loss: 0.007243049078172152\n",
      "epoch: 42, loss: 0.007166714819437361\n",
      "epoch: 43, loss: 0.006990953775788801\n",
      "epoch: 44, loss: 0.00724822874651134\n",
      "epoch: 45, loss: 0.007063130903145521\n",
      "epoch: 46, loss: 0.006740160639790191\n",
      "epoch: 47, loss: 0.006809071418416513\n",
      "epoch: 48, loss: 0.006664540379909708\n",
      "epoch: 49, loss: 0.006891901958898014\n",
      "epoch: 50, loss: 0.006591969136679607\n",
      "epoch: 51, loss: 0.006490052492019724\n",
      "epoch: 52, loss: 0.006585415282043946\n",
      "epoch: 53, loss: 0.006361507359693518\n",
      "epoch: 54, loss: 0.006590386143333185\n",
      "epoch: 55, loss: 0.0063421482790876225\n",
      "epoch: 56, loss: 0.006602274217007785\n",
      "epoch: 57, loss: 0.00601569298795994\n",
      "epoch: 58, loss: 0.006365519074791116\n",
      "epoch: 59, loss: 0.006103981866683074\n",
      "epoch: 60, loss: 0.00619622422929866\n",
      "epoch: 61, loss: 0.006228965514097167\n",
      "epoch: 62, loss: 0.006178142822545976\n",
      "epoch: 63, loss: 0.005831742383833136\n",
      "epoch: 64, loss: 0.005878257291462232\n",
      "epoch: 65, loss: 0.006015349735409428\n",
      "epoch: 66, loss: 0.005676971231287924\n",
      "epoch: 67, loss: 0.005511889851247116\n",
      "epoch: 68, loss: 0.005580151685114058\n",
      "epoch: 69, loss: 0.005246850246405346\n",
      "epoch: 70, loss: 0.0054904542003924575\n",
      "epoch: 71, loss: 0.005230110269585197\n",
      "epoch: 72, loss: 0.005023567026957884\n",
      "epoch: 73, loss: 0.004880505374967286\n",
      "epoch: 74, loss: 0.0049643882111384445\n",
      "epoch: 75, loss: 0.004656712305461239\n",
      "epoch: 76, loss: 0.004336411033767384\n",
      "epoch: 77, loss: 0.0042591247358520455\n",
      "epoch: 78, loss: 0.004099817855331911\n",
      "epoch: 79, loss: 0.0038555601515252442\n",
      "epoch: 80, loss: 0.003870856483862524\n",
      "epoch: 81, loss: 0.003596407590051034\n",
      "epoch: 82, loss: 0.003428378308076754\n",
      "epoch: 83, loss: 0.003288351983276078\n",
      "epoch: 84, loss: 0.0031981324393294225\n",
      "epoch: 85, loss: 0.00314999018679746\n",
      "epoch: 86, loss: 0.003049684660926318\n",
      "epoch: 87, loss: 0.0027438453868634956\n",
      "epoch: 88, loss: 0.0027941379170370044\n",
      "epoch: 89, loss: 0.0026826897914639506\n",
      "epoch: 90, loss: 0.002638694895831574\n",
      "epoch: 91, loss: 0.002423530441946173\n",
      "epoch: 92, loss: 0.0025421035479778102\n",
      "epoch: 93, loss: 0.002465108176984351\n",
      "epoch: 94, loss: 0.002365581038864242\n",
      "epoch: 95, loss: 0.002220598538954636\n",
      "epoch: 96, loss: 0.0022624313405504946\n",
      "epoch: 97, loss: 0.0022625636406805485\n",
      "epoch: 98, loss: 0.0021991234600365808\n",
      "epoch: 99, loss: 0.0020909744486728348\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [2, 4],\n",
    "                         dim = [2, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend=backend,\n",
    "                         shots=10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_2D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [2, 5, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=1000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_2D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x_qnn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ce59dab2f049f49bee35b576c53120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c180fe0da064460e93aaf2334f7139f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.04828635228112989\n",
      "epoch: 1, loss: 0.039003136881437035\n",
      "epoch: 2, loss: 0.029496646531267354\n",
      "epoch: 3, loss: 0.024680934660924508\n",
      "epoch: 4, loss: 0.02370861397163775\n",
      "epoch: 5, loss: 0.021681424622808657\n",
      "epoch: 6, loss: 0.020237098923040864\n",
      "epoch: 7, loss: 0.020047640040227795\n",
      "epoch: 8, loss: 0.02109565307689766\n",
      "epoch: 9, loss: 0.021641388993428844\n",
      "epoch: 10, loss: 0.021250484936363438\n",
      "epoch: 11, loss: 0.02022185119244297\n",
      "epoch: 12, loss: 0.01984687965024265\n",
      "epoch: 13, loss: 0.01925961254614399\n",
      "epoch: 14, loss: 0.01884384157852344\n",
      "epoch: 15, loss: 0.018153699699137684\n",
      "epoch: 16, loss: 0.01751385148260749\n",
      "epoch: 17, loss: 0.016975974283520264\n",
      "epoch: 18, loss: 0.016286735169088035\n",
      "epoch: 19, loss: 0.01582295646482196\n",
      "epoch: 20, loss: 0.015213847285576489\n",
      "epoch: 21, loss: 0.014812497507999116\n",
      "epoch: 22, loss: 0.014439676248223697\n",
      "epoch: 23, loss: 0.014300637526061736\n",
      "epoch: 24, loss: 0.014297931271826452\n",
      "epoch: 25, loss: 0.014144155043437167\n",
      "epoch: 26, loss: 0.014080351371422705\n",
      "epoch: 27, loss: 0.014238276955067894\n",
      "epoch: 28, loss: 0.014109204638175957\n",
      "epoch: 29, loss: 0.014237753280418331\n",
      "epoch: 30, loss: 0.013743899321837817\n",
      "epoch: 31, loss: 0.01356319144480557\n",
      "epoch: 32, loss: 0.01314985529712868\n",
      "epoch: 33, loss: 0.01339621683437592\n",
      "epoch: 34, loss: 0.013569781534284762\n",
      "epoch: 35, loss: 0.01304052770691504\n",
      "epoch: 36, loss: 0.012807326216224891\n",
      "epoch: 37, loss: 0.012609525151480373\n",
      "epoch: 38, loss: 0.012600559849198703\n",
      "epoch: 39, loss: 0.013013656185272726\n",
      "epoch: 40, loss: 0.012576697852229911\n",
      "epoch: 41, loss: 0.012495786387767424\n",
      "epoch: 42, loss: 0.01237944331301569\n",
      "epoch: 43, loss: 0.012299241000937336\n",
      "epoch: 44, loss: 0.012391857479818648\n",
      "epoch: 45, loss: 0.012511725612846167\n",
      "epoch: 46, loss: 0.012129007655479866\n",
      "epoch: 47, loss: 0.012297523465458362\n",
      "epoch: 48, loss: 0.012282523873401612\n",
      "epoch: 49, loss: 0.012309338307975959\n",
      "epoch: 50, loss: 0.012265246797462026\n",
      "epoch: 51, loss: 0.012276032696882353\n",
      "epoch: 52, loss: 0.012094296432625187\n",
      "epoch: 53, loss: 0.012107007961819051\n",
      "epoch: 54, loss: 0.011857191216536302\n",
      "epoch: 55, loss: 0.011912536248290395\n",
      "epoch: 56, loss: 0.01200947877751907\n",
      "epoch: 57, loss: 0.011821183793914416\n",
      "epoch: 58, loss: 0.01182333189640885\n",
      "epoch: 59, loss: 0.011809688323004334\n",
      "epoch: 60, loss: 0.011789020117153113\n",
      "epoch: 61, loss: 0.01161485036487154\n",
      "epoch: 62, loss: 0.011763610206970025\n",
      "epoch: 63, loss: 0.011404052515813622\n",
      "epoch: 64, loss: 0.011501829768277417\n",
      "epoch: 65, loss: 0.011667675624026584\n",
      "epoch: 66, loss: 0.011760090262996173\n",
      "epoch: 67, loss: 0.01153014709468032\n",
      "epoch: 68, loss: 0.011625819535416056\n",
      "epoch: 69, loss: 0.011508704107441668\n",
      "epoch: 70, loss: 0.011694525371845808\n",
      "epoch: 71, loss: 0.01139728233842859\n",
      "epoch: 72, loss: 0.011765002818606518\n",
      "epoch: 73, loss: 0.011482440262965351\n",
      "epoch: 74, loss: 0.011576360519000116\n",
      "epoch: 75, loss: 0.0114506715768831\n",
      "epoch: 76, loss: 0.011724036612244708\n",
      "epoch: 77, loss: 0.011766650121482725\n",
      "epoch: 78, loss: 0.011623594386888255\n",
      "epoch: 79, loss: 0.011643203080442895\n",
      "epoch: 80, loss: 0.011513046933476125\n",
      "epoch: 81, loss: 0.01140718070909066\n",
      "epoch: 82, loss: 0.01168751986353126\n",
      "epoch: 83, loss: 0.011522906701860396\n",
      "epoch: 84, loss: 0.011815237604785468\n",
      "epoch: 85, loss: 0.011594684891244525\n",
      "epoch: 86, loss: 0.011734258859975845\n",
      "epoch: 87, loss: 0.011710026910103597\n",
      "epoch: 88, loss: 0.01145577136683328\n",
      "epoch: 89, loss: 0.011623347298787952\n",
      "epoch: 90, loss: 0.011551943814482834\n",
      "epoch: 91, loss: 0.011776049111897328\n",
      "epoch: 92, loss: 0.011706675041561906\n",
      "epoch: 93, loss: 0.011764186082014995\n",
      "epoch: 94, loss: 0.011658737102734065\n",
      "epoch: 95, loss: 0.011591870068531532\n",
      "epoch: 96, loss: 0.011554329041823606\n",
      "epoch: 97, loss: 0.011838959630406046\n",
      "epoch: 98, loss: 0.011739712018587268\n",
      "epoch: 99, loss: 0.011341239835372266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d936c5dc54489e826f8c61d9271705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.027373987026830318\n",
      "epoch: 1, loss: 0.023865530387788523\n",
      "epoch: 2, loss: 0.02396405943182518\n",
      "epoch: 3, loss: 0.024003636032097538\n",
      "epoch: 4, loss: 0.023723092578236266\n",
      "epoch: 5, loss: 0.023464809717404545\n",
      "epoch: 6, loss: 0.0230281851408667\n",
      "epoch: 7, loss: 0.022883442540642514\n",
      "epoch: 8, loss: 0.02309465553671925\n",
      "epoch: 9, loss: 0.0226736932461601\n",
      "epoch: 10, loss: 0.022454284324954724\n",
      "epoch: 11, loss: 0.022454002516402675\n",
      "epoch: 12, loss: 0.02209171733328152\n",
      "epoch: 13, loss: 0.022049226929994715\n",
      "epoch: 14, loss: 0.02209204668360812\n",
      "epoch: 15, loss: 0.02186375758649878\n",
      "epoch: 16, loss: 0.02170960611924149\n",
      "epoch: 17, loss: 0.021588606800001225\n",
      "epoch: 18, loss: 0.021480286213591\n",
      "epoch: 19, loss: 0.021105385508172207\n",
      "epoch: 20, loss: 0.020867124768861926\n",
      "epoch: 21, loss: 0.020495383729718823\n",
      "epoch: 22, loss: 0.020073461267137802\n",
      "epoch: 23, loss: 0.019683552280592768\n",
      "epoch: 24, loss: 0.0193033880419981\n",
      "epoch: 25, loss: 0.018755735296747164\n",
      "epoch: 26, loss: 0.018665010745659263\n",
      "epoch: 27, loss: 0.018386305444428777\n",
      "epoch: 28, loss: 0.018110121249575742\n",
      "epoch: 29, loss: 0.0178295948479432\n",
      "epoch: 30, loss: 0.01745789154408753\n",
      "epoch: 31, loss: 0.017401579043191624\n",
      "epoch: 32, loss: 0.017129688626824506\n",
      "epoch: 33, loss: 0.016763879976812616\n",
      "epoch: 34, loss: 0.01639222676475351\n",
      "epoch: 35, loss: 0.015714737387325133\n",
      "epoch: 36, loss: 0.015450508879075911\n",
      "epoch: 37, loss: 0.015017621196038422\n",
      "epoch: 38, loss: 0.015146962311683134\n",
      "epoch: 39, loss: 0.015179589866928144\n",
      "epoch: 40, loss: 0.014751234720518976\n",
      "epoch: 41, loss: 0.014594299348196109\n",
      "epoch: 42, loss: 0.014289663010889363\n",
      "epoch: 43, loss: 0.014222104028849743\n",
      "epoch: 44, loss: 0.013891294379483653\n",
      "epoch: 45, loss: 0.013578057704377833\n",
      "epoch: 46, loss: 0.013756622254714922\n",
      "epoch: 47, loss: 0.01362934310170662\n",
      "epoch: 48, loss: 0.013494333337141772\n",
      "epoch: 49, loss: 0.013210617675216954\n",
      "epoch: 50, loss: 0.013199457957780922\n",
      "epoch: 51, loss: 0.013310716752068415\n",
      "epoch: 52, loss: 0.01310964176905148\n",
      "epoch: 53, loss: 0.01321264965098524\n",
      "epoch: 54, loss: 0.01312662690111018\n",
      "epoch: 55, loss: 0.013430833475084126\n",
      "epoch: 56, loss: 0.013074440997436347\n",
      "epoch: 57, loss: 0.013077650554192204\n",
      "epoch: 58, loss: 0.01306565605984732\n",
      "epoch: 59, loss: 0.012946470713433375\n",
      "epoch: 60, loss: 0.01310756458984058\n",
      "epoch: 61, loss: 0.013081515746682334\n",
      "epoch: 62, loss: 0.013021123343025286\n",
      "epoch: 63, loss: 0.01320022845016168\n",
      "epoch: 64, loss: 0.013092435189793186\n",
      "epoch: 65, loss: 0.012808200929853485\n",
      "epoch: 66, loss: 0.013003870813089855\n",
      "epoch: 67, loss: 0.0129805033505904\n",
      "epoch: 68, loss: 0.013090817462993346\n",
      "epoch: 69, loss: 0.01308645065374545\n",
      "epoch: 70, loss: 0.013061546955543648\n",
      "epoch: 71, loss: 0.01292035470355007\n",
      "epoch: 72, loss: 0.012932201864771762\n",
      "epoch: 73, loss: 0.012995976613976981\n",
      "epoch: 74, loss: 0.012967045834137849\n",
      "epoch: 75, loss: 0.012991976149073865\n",
      "epoch: 76, loss: 0.012781687145986202\n",
      "epoch: 77, loss: 0.012899966133328267\n",
      "epoch: 78, loss: 0.012825226849332853\n",
      "epoch: 79, loss: 0.012999922523038939\n",
      "epoch: 80, loss: 0.012553298614046637\n",
      "epoch: 81, loss: 0.012987745474653282\n",
      "epoch: 82, loss: 0.012974063660960766\n",
      "epoch: 83, loss: 0.012883109779860295\n",
      "epoch: 84, loss: 0.013015114513112934\n",
      "epoch: 85, loss: 0.01290948510894532\n",
      "epoch: 86, loss: 0.012967181217581579\n",
      "epoch: 87, loss: 0.012984911335484546\n",
      "epoch: 88, loss: 0.01297886385155599\n",
      "epoch: 89, loss: 0.012879002601265765\n",
      "epoch: 90, loss: 0.012971639672198157\n",
      "epoch: 91, loss: 0.013000152889162528\n",
      "epoch: 92, loss: 0.012838782974762012\n",
      "epoch: 93, loss: 0.012926487816639713\n",
      "epoch: 94, loss: 0.012995172861756192\n",
      "epoch: 95, loss: 0.01305280353803151\n",
      "epoch: 96, loss: 0.012867217656558742\n",
      "epoch: 97, loss: 0.013068035520069587\n",
      "epoch: 98, loss: 0.012654896712576786\n",
      "epoch: 99, loss: 0.01284131917803908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa42e95aaa140479660a37f6e9ce86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0601551539779326\n",
      "epoch: 1, loss: 0.03563271557047814\n",
      "epoch: 2, loss: 0.02516591846432895\n",
      "epoch: 3, loss: 0.025611484030789493\n",
      "epoch: 4, loss: 0.027455318490158896\n",
      "epoch: 5, loss: 0.027192751698490388\n",
      "epoch: 6, loss: 0.025569882424917503\n",
      "epoch: 7, loss: 0.023631691766179883\n",
      "epoch: 8, loss: 0.02327740780562598\n",
      "epoch: 9, loss: 0.02350182075680447\n",
      "epoch: 10, loss: 0.022449447577875422\n",
      "epoch: 11, loss: 0.022250191232044428\n",
      "epoch: 12, loss: 0.02212504126852792\n",
      "epoch: 13, loss: 0.022286398469261164\n",
      "epoch: 14, loss: 0.022394376891036984\n",
      "epoch: 15, loss: 0.022491718670563907\n",
      "epoch: 16, loss: 0.022028847788739352\n",
      "epoch: 17, loss: 0.021681446161133138\n",
      "epoch: 18, loss: 0.021445553020734972\n",
      "epoch: 19, loss: 0.02131791312395241\n",
      "epoch: 20, loss: 0.021143003080116393\n",
      "epoch: 21, loss: 0.020839339660628696\n",
      "epoch: 22, loss: 0.020666053445082547\n",
      "epoch: 23, loss: 0.020557875994479\n",
      "epoch: 24, loss: 0.020421687454394523\n",
      "epoch: 25, loss: 0.020302989154800888\n",
      "epoch: 26, loss: 0.02031054650060038\n",
      "epoch: 27, loss: 0.02025191239714614\n",
      "epoch: 28, loss: 0.019921957642539387\n",
      "epoch: 29, loss: 0.01985863525380466\n",
      "epoch: 30, loss: 0.01954941046424121\n",
      "epoch: 31, loss: 0.01959503204816532\n",
      "epoch: 32, loss: 0.01967149997681114\n",
      "epoch: 33, loss: 0.01947551239999108\n",
      "epoch: 34, loss: 0.019200666862904617\n",
      "epoch: 35, loss: 0.018934989441244915\n",
      "epoch: 36, loss: 0.019218149934358136\n",
      "epoch: 37, loss: 0.019096251096700018\n",
      "epoch: 38, loss: 0.01904659434122751\n",
      "epoch: 39, loss: 0.01890663561380113\n",
      "epoch: 40, loss: 0.018946979696632418\n",
      "epoch: 41, loss: 0.018795194871791442\n",
      "epoch: 42, loss: 0.018736309515680703\n",
      "epoch: 43, loss: 0.0186068751074644\n",
      "epoch: 44, loss: 0.01865439195771587\n",
      "epoch: 45, loss: 0.01850631086750499\n",
      "epoch: 46, loss: 0.018544686941011117\n",
      "epoch: 47, loss: 0.018311721515736838\n",
      "epoch: 48, loss: 0.018554703794610754\n",
      "epoch: 49, loss: 0.018481172393367246\n",
      "epoch: 50, loss: 0.018571910476640432\n",
      "epoch: 51, loss: 0.01838049309130414\n",
      "epoch: 52, loss: 0.01841659600178515\n",
      "epoch: 53, loss: 0.018427049413628133\n",
      "epoch: 54, loss: 0.018302607051067295\n",
      "epoch: 55, loss: 0.018265344335923254\n",
      "epoch: 56, loss: 0.01809818890203266\n",
      "epoch: 57, loss: 0.017917586240293316\n",
      "epoch: 58, loss: 0.01830173975935266\n",
      "epoch: 59, loss: 0.018028415874280522\n",
      "epoch: 60, loss: 0.018340764478398216\n",
      "epoch: 61, loss: 0.018077959540832408\n",
      "epoch: 62, loss: 0.018244048470113114\n",
      "epoch: 63, loss: 0.0178145534517876\n",
      "epoch: 64, loss: 0.017871566591691288\n",
      "epoch: 65, loss: 0.01775902812166906\n",
      "epoch: 66, loss: 0.017990292922694932\n",
      "epoch: 67, loss: 0.017746449411297008\n",
      "epoch: 68, loss: 0.01771201233976585\n",
      "epoch: 69, loss: 0.017649971006237823\n",
      "epoch: 70, loss: 0.017561209715680086\n",
      "epoch: 71, loss: 0.01753196720226298\n",
      "epoch: 72, loss: 0.017814259855971196\n",
      "epoch: 73, loss: 0.01766856886108916\n",
      "epoch: 74, loss: 0.017428091713147784\n",
      "epoch: 75, loss: 0.017384871185058173\n",
      "epoch: 76, loss: 0.017367865847192736\n",
      "epoch: 77, loss: 0.016933317719829366\n",
      "epoch: 78, loss: 0.017090938939686464\n",
      "epoch: 79, loss: 0.01691459574596399\n",
      "epoch: 80, loss: 0.016864055517476273\n",
      "epoch: 81, loss: 0.016563530230487716\n",
      "epoch: 82, loss: 0.01633501249795788\n",
      "epoch: 83, loss: 0.015812749635559856\n",
      "epoch: 84, loss: 0.015449546932465887\n",
      "epoch: 85, loss: 0.014914774467576098\n",
      "epoch: 86, loss: 0.014288708827799748\n",
      "epoch: 87, loss: 0.013812661293326664\n",
      "epoch: 88, loss: 0.012991664834664867\n",
      "epoch: 89, loss: 0.01273719665764409\n",
      "epoch: 90, loss: 0.012090961531549729\n",
      "epoch: 91, loss: 0.011539073591459092\n",
      "epoch: 92, loss: 0.010703153822177904\n",
      "epoch: 93, loss: 0.010598995645747247\n",
      "epoch: 94, loss: 0.010331754265964246\n",
      "epoch: 95, loss: 0.009628907092208356\n",
      "epoch: 96, loss: 0.008626011566227968\n",
      "epoch: 97, loss: 0.008601639234632618\n",
      "epoch: 98, loss: 0.008410148467811388\n",
      "epoch: 99, loss: 0.008020577944232082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7315d54d89438ca01b4bb5979e1f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.03095132612025051\n",
      "epoch: 1, loss: 0.024798424670677523\n",
      "epoch: 2, loss: 0.022164461854125902\n",
      "epoch: 3, loss: 0.021374884271252222\n",
      "epoch: 4, loss: 0.02101816834544885\n",
      "epoch: 5, loss: 0.02130689551339093\n",
      "epoch: 6, loss: 0.021211236013415197\n",
      "epoch: 7, loss: 0.021290070188415205\n",
      "epoch: 8, loss: 0.02109942695657668\n",
      "epoch: 9, loss: 0.02077832948395953\n",
      "epoch: 10, loss: 0.02049639651704642\n",
      "epoch: 11, loss: 0.02022592713633587\n",
      "epoch: 12, loss: 0.019848139513299987\n",
      "epoch: 13, loss: 0.019210471269134693\n",
      "epoch: 14, loss: 0.018719439112090848\n",
      "epoch: 15, loss: 0.01771200459672875\n",
      "epoch: 16, loss: 0.017284774092966606\n",
      "epoch: 17, loss: 0.016550558934557487\n",
      "epoch: 18, loss: 0.015927316255155012\n",
      "epoch: 19, loss: 0.015056146116962255\n",
      "epoch: 20, loss: 0.01408154941696406\n",
      "epoch: 21, loss: 0.013343285399896484\n",
      "epoch: 22, loss: 0.012047296174685621\n",
      "epoch: 23, loss: 0.011553118586613826\n",
      "epoch: 24, loss: 0.011672013282149498\n",
      "epoch: 25, loss: 0.011964185019031915\n",
      "epoch: 26, loss: 0.012104819680846111\n",
      "epoch: 27, loss: 0.011905442940099674\n",
      "epoch: 28, loss: 0.011523260999704623\n",
      "epoch: 29, loss: 0.010951851071039782\n",
      "epoch: 30, loss: 0.01041881195452712\n",
      "epoch: 31, loss: 0.01038025667203555\n",
      "epoch: 32, loss: 0.010295562263365653\n",
      "epoch: 33, loss: 0.009878081679180941\n",
      "epoch: 34, loss: 0.009795166550379803\n",
      "epoch: 35, loss: 0.009620592041156447\n",
      "epoch: 36, loss: 0.009678584593202496\n",
      "epoch: 37, loss: 0.009736493621893575\n",
      "epoch: 38, loss: 0.01001971906342587\n",
      "epoch: 39, loss: 0.009524451800888045\n",
      "epoch: 40, loss: 0.009444067489846476\n",
      "epoch: 41, loss: 0.009503413392362329\n",
      "epoch: 42, loss: 0.00929620504714921\n",
      "epoch: 43, loss: 0.009380041179481937\n",
      "epoch: 44, loss: 0.009430544634253038\n",
      "epoch: 45, loss: 0.00939409273846799\n",
      "epoch: 46, loss: 0.009251515277764046\n",
      "epoch: 47, loss: 0.009296283892825176\n",
      "epoch: 48, loss: 0.0092242895901835\n",
      "epoch: 49, loss: 0.009178353858651908\n",
      "epoch: 50, loss: 0.009169487594374415\n",
      "epoch: 51, loss: 0.009328066694991996\n",
      "epoch: 52, loss: 0.009135434901834917\n",
      "epoch: 53, loss: 0.009321983802750258\n",
      "epoch: 54, loss: 0.009241129527961753\n",
      "epoch: 55, loss: 0.009281671091601721\n",
      "epoch: 56, loss: 0.009223465237931916\n",
      "epoch: 57, loss: 0.008944117751961758\n",
      "epoch: 58, loss: 0.009227448908571679\n",
      "epoch: 59, loss: 0.009262066041367098\n",
      "epoch: 60, loss: 0.00904264196519395\n",
      "epoch: 61, loss: 0.008962918731341717\n",
      "epoch: 62, loss: 0.009113587873090698\n",
      "epoch: 63, loss: 0.00903429873610093\n",
      "epoch: 64, loss: 0.009168358029961661\n",
      "epoch: 65, loss: 0.009125246805291146\n",
      "epoch: 66, loss: 0.009034992256268881\n",
      "epoch: 67, loss: 0.009277537942713443\n",
      "epoch: 68, loss: 0.009108593237879138\n",
      "epoch: 69, loss: 0.008729572698386652\n",
      "epoch: 70, loss: 0.008910541197431263\n",
      "epoch: 71, loss: 0.008932583096564962\n",
      "epoch: 72, loss: 0.008862172342065966\n",
      "epoch: 73, loss: 0.009001972152570654\n",
      "epoch: 74, loss: 0.00894938912105748\n",
      "epoch: 75, loss: 0.009053166356035428\n",
      "epoch: 76, loss: 0.008994993079790715\n",
      "epoch: 77, loss: 0.009130217746752137\n",
      "epoch: 78, loss: 0.008956257370811483\n",
      "epoch: 79, loss: 0.009023236142235656\n",
      "epoch: 80, loss: 0.009113284949954083\n",
      "epoch: 81, loss: 0.009058592902384153\n",
      "epoch: 82, loss: 0.008669454821598222\n",
      "epoch: 83, loss: 0.008896497474409201\n",
      "epoch: 84, loss: 0.009030219723713585\n",
      "epoch: 85, loss: 0.00891758117134902\n",
      "epoch: 86, loss: 0.008953968593629704\n",
      "epoch: 87, loss: 0.009259792355988401\n",
      "epoch: 88, loss: 0.008919965479159029\n",
      "epoch: 89, loss: 0.008922474864091555\n",
      "epoch: 90, loss: 0.008728592275353879\n",
      "epoch: 91, loss: 0.008936414482934961\n",
      "epoch: 92, loss: 0.008964106831732712\n",
      "epoch: 93, loss: 0.008968130897001056\n",
      "epoch: 94, loss: 0.008902248944096948\n",
      "epoch: 95, loss: 0.009104435036752389\n",
      "epoch: 96, loss: 0.009029997910101317\n",
      "epoch: 97, loss: 0.009046054263590262\n",
      "epoch: 98, loss: 0.009107461616402445\n",
      "epoch: 99, loss: 0.008782688241660024\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "qnn_list = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 1),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a4715154b4daabd6d30a36e3e6892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf17de30285429f8e5926cdd1c04c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0559680857593431\n",
      "epoch: 1, loss: 0.03697720125268838\n",
      "epoch: 2, loss: 0.029110854947261317\n",
      "epoch: 3, loss: 0.027519070960164635\n",
      "epoch: 4, loss: 0.027437093398845497\n",
      "epoch: 5, loss: 0.025969801228769265\n",
      "epoch: 6, loss: 0.02478083753547045\n",
      "epoch: 7, loss: 0.02353019392434656\n",
      "epoch: 8, loss: 0.023454347143725844\n",
      "epoch: 9, loss: 0.022957857992989356\n",
      "epoch: 10, loss: 0.02260987467836544\n",
      "epoch: 11, loss: 0.022117242386954145\n",
      "epoch: 12, loss: 0.021344281178992532\n",
      "epoch: 13, loss: 0.020591478490406635\n",
      "epoch: 14, loss: 0.019391180399898\n",
      "epoch: 15, loss: 0.018354423895041265\n",
      "epoch: 16, loss: 0.01734337884774248\n",
      "epoch: 17, loss: 0.016856865749051826\n",
      "epoch: 18, loss: 0.016287188163814297\n",
      "epoch: 19, loss: 0.015607808223159488\n",
      "epoch: 20, loss: 0.01469759367411915\n",
      "epoch: 21, loss: 0.014160169933169752\n",
      "epoch: 22, loss: 0.013600173031524445\n",
      "epoch: 23, loss: 0.013574026157707277\n",
      "epoch: 24, loss: 0.013600638392159983\n",
      "epoch: 25, loss: 0.0133917226040653\n",
      "epoch: 26, loss: 0.013341211074535675\n",
      "epoch: 27, loss: 0.013423695814742427\n",
      "epoch: 28, loss: 0.01285347029373745\n",
      "epoch: 29, loss: 0.0122555168332646\n",
      "epoch: 30, loss: 0.012298340685295454\n",
      "epoch: 31, loss: 0.011933651507633447\n",
      "epoch: 32, loss: 0.011789098993671952\n",
      "epoch: 33, loss: 0.011622615998546124\n",
      "epoch: 34, loss: 0.011429988006484723\n",
      "epoch: 35, loss: 0.011358468917777816\n",
      "epoch: 36, loss: 0.011235169057462187\n",
      "epoch: 37, loss: 0.010954439972621485\n",
      "epoch: 38, loss: 0.010831900983369532\n",
      "epoch: 39, loss: 0.01066129937773063\n",
      "epoch: 40, loss: 0.010611765205247389\n",
      "epoch: 41, loss: 0.01057722263800676\n",
      "epoch: 42, loss: 0.010443940853859333\n",
      "epoch: 43, loss: 0.010198078262526045\n",
      "epoch: 44, loss: 0.010155291732862719\n",
      "epoch: 45, loss: 0.010046256638446755\n",
      "epoch: 46, loss: 0.009880724425855698\n",
      "epoch: 47, loss: 0.010093617421641131\n",
      "epoch: 48, loss: 0.009886309081649326\n",
      "epoch: 49, loss: 0.009925606089019027\n",
      "epoch: 50, loss: 0.009772434375172266\n",
      "epoch: 51, loss: 0.009622648252983828\n",
      "epoch: 52, loss: 0.009680802399830588\n",
      "epoch: 53, loss: 0.009617040621219045\n",
      "epoch: 54, loss: 0.009818143202941306\n",
      "epoch: 55, loss: 0.009707021687931925\n",
      "epoch: 56, loss: 0.009538019128503331\n",
      "epoch: 57, loss: 0.009393349064297719\n",
      "epoch: 58, loss: 0.009478755558304077\n",
      "epoch: 59, loss: 0.009402382972864332\n",
      "epoch: 60, loss: 0.009320003818635021\n",
      "epoch: 61, loss: 0.009387700914613372\n",
      "epoch: 62, loss: 0.009454476386231927\n",
      "epoch: 63, loss: 0.009459022333359999\n",
      "epoch: 64, loss: 0.009465142973816456\n",
      "epoch: 65, loss: 0.009378253711038299\n",
      "epoch: 66, loss: 0.009505684471288857\n",
      "epoch: 67, loss: 0.009379778166259078\n",
      "epoch: 68, loss: 0.009198113240146534\n",
      "epoch: 69, loss: 0.009501978069927662\n",
      "epoch: 70, loss: 0.009315131939689955\n",
      "epoch: 71, loss: 0.00910812061009827\n",
      "epoch: 72, loss: 0.009365664168666838\n",
      "epoch: 73, loss: 0.00933485080692596\n",
      "epoch: 74, loss: 0.009273621297417134\n",
      "epoch: 75, loss: 0.009309078762212082\n",
      "epoch: 76, loss: 0.00926259366443616\n",
      "epoch: 77, loss: 0.00925049154192437\n",
      "epoch: 78, loss: 0.009164654694407156\n",
      "epoch: 79, loss: 0.009512081610872568\n",
      "epoch: 80, loss: 0.009233768859350726\n",
      "epoch: 81, loss: 0.009056295374198534\n",
      "epoch: 82, loss: 0.009326880754960468\n",
      "epoch: 83, loss: 0.00902346583241006\n",
      "epoch: 84, loss: 0.009238449450297479\n",
      "epoch: 85, loss: 0.00906504272478137\n",
      "epoch: 86, loss: 0.009099594369548426\n",
      "epoch: 87, loss: 0.009169413542733665\n",
      "epoch: 88, loss: 0.009418586667475136\n",
      "epoch: 89, loss: 0.009018976734880014\n",
      "epoch: 90, loss: 0.00899465461173972\n",
      "epoch: 91, loss: 0.009148645235853464\n",
      "epoch: 92, loss: 0.009124477171828361\n",
      "epoch: 93, loss: 0.009107081246341728\n",
      "epoch: 94, loss: 0.00908503811081373\n",
      "epoch: 95, loss: 0.008937249988979558\n",
      "epoch: 96, loss: 0.00894775910394915\n",
      "epoch: 97, loss: 0.009114072315347922\n",
      "epoch: 98, loss: 0.008981955289193187\n",
      "epoch: 99, loss: 0.009210655287997185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6134c2f508ec49bbbe0dfd2f8e39f752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.06755964287836769\n",
      "epoch: 1, loss: 0.04490714015210805\n",
      "epoch: 2, loss: 0.03489692667031402\n",
      "epoch: 3, loss: 0.028188616669126212\n",
      "epoch: 4, loss: 0.025141431643925252\n",
      "epoch: 5, loss: 0.02385159823807526\n",
      "epoch: 6, loss: 0.02207210016643366\n",
      "epoch: 7, loss: 0.01970727203855447\n",
      "epoch: 8, loss: 0.018026648974424356\n",
      "epoch: 9, loss: 0.017911756400759943\n",
      "epoch: 10, loss: 0.01796783305775529\n",
      "epoch: 11, loss: 0.018105623200982305\n",
      "epoch: 12, loss: 0.018039550172714133\n",
      "epoch: 13, loss: 0.017873318213373384\n",
      "epoch: 14, loss: 0.01752620650303811\n",
      "epoch: 15, loss: 0.017596440355907215\n",
      "epoch: 16, loss: 0.01713711216301002\n",
      "epoch: 17, loss: 0.01637159399666957\n",
      "epoch: 18, loss: 0.01609051379570409\n",
      "epoch: 19, loss: 0.015422499066593036\n",
      "epoch: 20, loss: 0.015269309580684589\n",
      "epoch: 21, loss: 0.01481720082477519\n",
      "epoch: 22, loss: 0.014914003539802457\n",
      "epoch: 23, loss: 0.014589961160458634\n",
      "epoch: 24, loss: 0.014292691257619509\n",
      "epoch: 25, loss: 0.013973661217809786\n",
      "epoch: 26, loss: 0.014049337400287906\n",
      "epoch: 27, loss: 0.013661991665770423\n",
      "epoch: 28, loss: 0.013848776467662739\n",
      "epoch: 29, loss: 0.013204733466555976\n",
      "epoch: 30, loss: 0.013337589917264567\n",
      "epoch: 31, loss: 0.01305780351823241\n",
      "epoch: 32, loss: 0.0128812112377407\n",
      "epoch: 33, loss: 0.012650650764376391\n",
      "epoch: 34, loss: 0.012779338684310993\n",
      "epoch: 35, loss: 0.01270678324801736\n",
      "epoch: 36, loss: 0.01265172204422157\n",
      "epoch: 37, loss: 0.012181854392969862\n",
      "epoch: 38, loss: 0.012543382956652459\n",
      "epoch: 39, loss: 0.012336682845046432\n",
      "epoch: 40, loss: 0.012286598353069198\n",
      "epoch: 41, loss: 0.01192690751134302\n",
      "epoch: 42, loss: 0.01210710647572217\n",
      "epoch: 43, loss: 0.01204973651743663\n",
      "epoch: 44, loss: 0.012008273008646295\n",
      "epoch: 45, loss: 0.012082713603555539\n",
      "epoch: 46, loss: 0.011949101067519848\n",
      "epoch: 47, loss: 0.011814349921180307\n",
      "epoch: 48, loss: 0.011697779686759753\n",
      "epoch: 49, loss: 0.011715945497583698\n",
      "epoch: 50, loss: 0.011851940317518286\n",
      "epoch: 51, loss: 0.011725153905758243\n",
      "epoch: 52, loss: 0.01179115485234713\n",
      "epoch: 53, loss: 0.011622932780822662\n",
      "epoch: 54, loss: 0.011583855099549893\n",
      "epoch: 55, loss: 0.011459822718652526\n",
      "epoch: 56, loss: 0.011649227104873592\n",
      "epoch: 57, loss: 0.011487873326471188\n",
      "epoch: 58, loss: 0.011539272429731788\n",
      "epoch: 59, loss: 0.011692997067651234\n",
      "epoch: 60, loss: 0.011629516218588485\n",
      "epoch: 61, loss: 0.011588688286155838\n",
      "epoch: 62, loss: 0.0116293845773462\n",
      "epoch: 63, loss: 0.011653139791924883\n",
      "epoch: 64, loss: 0.011587438822559736\n",
      "epoch: 65, loss: 0.011647017196730126\n",
      "epoch: 66, loss: 0.011119065422482088\n",
      "epoch: 67, loss: 0.011499668034628877\n",
      "epoch: 68, loss: 0.01138533154057703\n",
      "epoch: 69, loss: 0.011420626452110495\n",
      "epoch: 70, loss: 0.011290644816420557\n",
      "epoch: 71, loss: 0.011374152621861474\n",
      "epoch: 72, loss: 0.01127246706426122\n",
      "epoch: 73, loss: 0.011578439318029833\n",
      "epoch: 74, loss: 0.01156183271645277\n",
      "epoch: 75, loss: 0.011422895083590212\n",
      "epoch: 76, loss: 0.01123671996009973\n",
      "epoch: 77, loss: 0.011211763208228188\n",
      "epoch: 78, loss: 0.011230400739229903\n",
      "epoch: 79, loss: 0.011299842712418726\n",
      "epoch: 80, loss: 0.011050708543633861\n",
      "epoch: 81, loss: 0.011166127636715007\n",
      "epoch: 82, loss: 0.011182868336094649\n",
      "epoch: 83, loss: 0.011095244421646047\n",
      "epoch: 84, loss: 0.011087868593476756\n",
      "epoch: 85, loss: 0.010857831850901875\n",
      "epoch: 86, loss: 0.010881497554976303\n",
      "epoch: 87, loss: 0.010837504675375722\n",
      "epoch: 88, loss: 0.010855183263815113\n",
      "epoch: 89, loss: 0.010701014282462729\n",
      "epoch: 90, loss: 0.010709885249237367\n",
      "epoch: 91, loss: 0.01067165056231983\n",
      "epoch: 92, loss: 0.010568842836061151\n",
      "epoch: 93, loss: 0.010568014784866728\n",
      "epoch: 94, loss: 0.010522199674530818\n",
      "epoch: 95, loss: 0.010368434922868265\n",
      "epoch: 96, loss: 0.010381296613204367\n",
      "epoch: 97, loss: 0.010537293434268288\n",
      "epoch: 98, loss: 0.009983832697885655\n",
      "epoch: 99, loss: 0.010228138765384405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48f5abdd3544b37bde89f44fd4464e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.04059234572849378\n",
      "epoch: 1, loss: 0.0316712356615862\n",
      "epoch: 2, loss: 0.026685221287336948\n",
      "epoch: 3, loss: 0.024133895811833603\n",
      "epoch: 4, loss: 0.022974461630389447\n",
      "epoch: 5, loss: 0.022737244903550773\n",
      "epoch: 6, loss: 0.02263900396234762\n",
      "epoch: 7, loss: 0.022462883376004213\n",
      "epoch: 8, loss: 0.02284533738809455\n",
      "epoch: 9, loss: 0.022490887898493832\n",
      "epoch: 10, loss: 0.021785127814804725\n",
      "epoch: 11, loss: 0.02064050723997925\n",
      "epoch: 12, loss: 0.01994356688278618\n",
      "epoch: 13, loss: 0.018958335934190204\n",
      "epoch: 14, loss: 0.017602603720367378\n",
      "epoch: 15, loss: 0.01650989633411805\n",
      "epoch: 16, loss: 0.016668249877396783\n",
      "epoch: 17, loss: 0.01758529854558406\n",
      "epoch: 18, loss: 0.017034636136536656\n",
      "epoch: 19, loss: 0.01639211081969097\n",
      "epoch: 20, loss: 0.01614792455316816\n",
      "epoch: 21, loss: 0.016278111311023694\n",
      "epoch: 22, loss: 0.015977938186388627\n",
      "epoch: 23, loss: 0.01549540079633537\n",
      "epoch: 24, loss: 0.015364310265633594\n",
      "epoch: 25, loss: 0.015070098715183614\n",
      "epoch: 26, loss: 0.01481633353195486\n",
      "epoch: 27, loss: 0.01483884186065696\n",
      "epoch: 28, loss: 0.014614444631046766\n",
      "epoch: 29, loss: 0.014156573474672666\n",
      "epoch: 30, loss: 0.014067755706814684\n",
      "epoch: 31, loss: 0.013962190631243078\n",
      "epoch: 32, loss: 0.013741451068079554\n",
      "epoch: 33, loss: 0.013949758423150905\n",
      "epoch: 34, loss: 0.013731938050269892\n",
      "epoch: 35, loss: 0.013762158023979933\n",
      "epoch: 36, loss: 0.013395436170823526\n",
      "epoch: 37, loss: 0.013236563067460681\n",
      "epoch: 38, loss: 0.013246306655234406\n",
      "epoch: 39, loss: 0.013110777271173154\n",
      "epoch: 40, loss: 0.01296563799646262\n",
      "epoch: 41, loss: 0.012677952637594718\n",
      "epoch: 42, loss: 0.012507337630679995\n",
      "epoch: 43, loss: 0.012760872565706629\n",
      "epoch: 44, loss: 0.01255265025935118\n",
      "epoch: 45, loss: 0.012396990865316057\n",
      "epoch: 46, loss: 0.012312506943436264\n",
      "epoch: 47, loss: 0.012498248773857034\n",
      "epoch: 48, loss: 0.012337009084033736\n",
      "epoch: 49, loss: 0.011958727332608363\n",
      "epoch: 50, loss: 0.011947810576286137\n",
      "epoch: 51, loss: 0.011654811614698127\n",
      "epoch: 52, loss: 0.011532221597624582\n",
      "epoch: 53, loss: 0.011493214470908567\n",
      "epoch: 54, loss: 0.011422649921756952\n",
      "epoch: 55, loss: 0.011105367411979572\n",
      "epoch: 56, loss: 0.011127827630830863\n",
      "epoch: 57, loss: 0.010914787488882438\n",
      "epoch: 58, loss: 0.010745031034632747\n",
      "epoch: 59, loss: 0.010774987668586509\n",
      "epoch: 60, loss: 0.010587221687412699\n",
      "epoch: 61, loss: 0.010695492297010788\n",
      "epoch: 62, loss: 0.010526832336840718\n",
      "epoch: 63, loss: 0.01043542048095759\n",
      "epoch: 64, loss: 0.010212246292765901\n",
      "epoch: 65, loss: 0.010079526167953098\n",
      "epoch: 66, loss: 0.009746641560718892\n",
      "epoch: 67, loss: 0.009759085901297966\n",
      "epoch: 68, loss: 0.009428216758785048\n",
      "epoch: 69, loss: 0.00926238983842317\n",
      "epoch: 70, loss: 0.009195758118263246\n",
      "epoch: 71, loss: 0.008960789799686782\n",
      "epoch: 72, loss: 0.009026165692311988\n",
      "epoch: 73, loss: 0.008979259453666679\n",
      "epoch: 74, loss: 0.009061126216228085\n",
      "epoch: 75, loss: 0.00904176930808867\n",
      "epoch: 76, loss: 0.008734649931939658\n",
      "epoch: 77, loss: 0.00880149687312544\n",
      "epoch: 78, loss: 0.008717842535647072\n",
      "epoch: 79, loss: 0.0085276711264282\n",
      "epoch: 80, loss: 0.00830756181448961\n",
      "epoch: 81, loss: 0.008402618725089165\n",
      "epoch: 82, loss: 0.008282452945395253\n",
      "epoch: 83, loss: 0.008611660432762212\n",
      "epoch: 84, loss: 0.008602267076610107\n",
      "epoch: 85, loss: 0.008448033570741687\n",
      "epoch: 86, loss: 0.008349170399194035\n",
      "epoch: 87, loss: 0.008219947465843959\n",
      "epoch: 88, loss: 0.008306643518644468\n",
      "epoch: 89, loss: 0.008171589926318834\n",
      "epoch: 90, loss: 0.008082325741605528\n",
      "epoch: 91, loss: 0.00811448090724288\n",
      "epoch: 92, loss: 0.008255595326510825\n",
      "epoch: 93, loss: 0.008302589268317952\n",
      "epoch: 94, loss: 0.008219160009426607\n",
      "epoch: 95, loss: 0.00811126286388807\n",
      "epoch: 96, loss: 0.008118679799892139\n",
      "epoch: 97, loss: 0.008086209626909743\n",
      "epoch: 98, loss: 0.007970362462426328\n",
      "epoch: 99, loss: 0.008303123756191308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94982eb976624b43bcb81456fc191f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.1044222303092689\n",
      "epoch: 1, loss: 0.07334983570893071\n",
      "epoch: 2, loss: 0.051172396756700156\n",
      "epoch: 3, loss: 0.03859759813331862\n",
      "epoch: 4, loss: 0.03398470419966841\n",
      "epoch: 5, loss: 0.030625043959947234\n",
      "epoch: 6, loss: 0.02878904248960087\n",
      "epoch: 7, loss: 0.025853389596946837\n",
      "epoch: 8, loss: 0.024232147013521214\n",
      "epoch: 9, loss: 0.023719398592013125\n",
      "epoch: 10, loss: 0.024258527261899497\n",
      "epoch: 11, loss: 0.025295094086788126\n",
      "epoch: 12, loss: 0.025252957302986937\n",
      "epoch: 13, loss: 0.024142237958233875\n",
      "epoch: 14, loss: 0.022745504034572355\n",
      "epoch: 15, loss: 0.021462703751256494\n",
      "epoch: 16, loss: 0.021251909735481803\n",
      "epoch: 17, loss: 0.021136466251326744\n",
      "epoch: 18, loss: 0.020582168283697685\n",
      "epoch: 19, loss: 0.020271553783186326\n",
      "epoch: 20, loss: 0.01990497206002511\n",
      "epoch: 21, loss: 0.019265554775448928\n",
      "epoch: 22, loss: 0.018994003992204696\n",
      "epoch: 23, loss: 0.018995421524535174\n",
      "epoch: 24, loss: 0.01883492430207659\n",
      "epoch: 25, loss: 0.01873105978650828\n",
      "epoch: 26, loss: 0.01838284285507408\n",
      "epoch: 27, loss: 0.017654098117170686\n",
      "epoch: 28, loss: 0.017196538994548597\n",
      "epoch: 29, loss: 0.016922870574969547\n",
      "epoch: 30, loss: 0.01685130523838934\n",
      "epoch: 31, loss: 0.01663593555995118\n",
      "epoch: 32, loss: 0.01661593840065462\n",
      "epoch: 33, loss: 0.016102622405857865\n",
      "epoch: 34, loss: 0.015778872614865404\n",
      "epoch: 35, loss: 0.015540363062131258\n",
      "epoch: 36, loss: 0.015567788851471968\n",
      "epoch: 37, loss: 0.015512689033233305\n",
      "epoch: 38, loss: 0.01545631376379291\n",
      "epoch: 39, loss: 0.015249689066196219\n",
      "epoch: 40, loss: 0.015132438724791298\n",
      "epoch: 41, loss: 0.014599827427333952\n",
      "epoch: 42, loss: 0.014401022736500948\n",
      "epoch: 43, loss: 0.013835157406706435\n",
      "epoch: 44, loss: 0.014139934216974495\n",
      "epoch: 45, loss: 0.013802836283487134\n",
      "epoch: 46, loss: 0.013464231960279982\n",
      "epoch: 47, loss: 0.01328358688479657\n",
      "epoch: 48, loss: 0.013086111521672284\n",
      "epoch: 49, loss: 0.01295254065128829\n",
      "epoch: 50, loss: 0.013050246164534493\n",
      "epoch: 51, loss: 0.01283492203926646\n",
      "epoch: 52, loss: 0.012661898294238028\n",
      "epoch: 53, loss: 0.012392458148072103\n",
      "epoch: 54, loss: 0.012032287332027023\n",
      "epoch: 55, loss: 0.011827132234818627\n",
      "epoch: 56, loss: 0.011453514112964407\n",
      "epoch: 57, loss: 0.011337779099437232\n",
      "epoch: 58, loss: 0.01080763064987823\n",
      "epoch: 59, loss: 0.010705705036006112\n",
      "epoch: 60, loss: 0.010437284963480942\n",
      "epoch: 61, loss: 0.010203820258601785\n",
      "epoch: 62, loss: 0.009957717776497543\n",
      "epoch: 63, loss: 0.00973467987824078\n",
      "epoch: 64, loss: 0.009638105544603278\n",
      "epoch: 65, loss: 0.009275050879384093\n",
      "epoch: 66, loss: 0.009277638871619644\n",
      "epoch: 67, loss: 0.00879458467415929\n",
      "epoch: 68, loss: 0.008770226344143968\n",
      "epoch: 69, loss: 0.00857352394053983\n",
      "epoch: 70, loss: 0.00855161319560934\n",
      "epoch: 71, loss: 0.008299964675549262\n",
      "epoch: 72, loss: 0.008136180282048223\n",
      "epoch: 73, loss: 0.008084282481477302\n",
      "epoch: 74, loss: 0.007931591352638163\n",
      "epoch: 75, loss: 0.007831298376985407\n",
      "epoch: 76, loss: 0.007906747532181405\n",
      "epoch: 77, loss: 0.007826130837549908\n",
      "epoch: 78, loss: 0.007636287517263445\n",
      "epoch: 79, loss: 0.007510673929570061\n",
      "epoch: 80, loss: 0.007337156296103488\n",
      "epoch: 81, loss: 0.0075967422351825905\n",
      "epoch: 82, loss: 0.007549719934875259\n",
      "epoch: 83, loss: 0.007444882777536071\n",
      "epoch: 84, loss: 0.0072916552962844\n",
      "epoch: 85, loss: 0.007278933175663472\n",
      "epoch: 86, loss: 0.006957183704567883\n",
      "epoch: 87, loss: 0.0071650940020264306\n",
      "epoch: 88, loss: 0.00721316376443639\n",
      "epoch: 89, loss: 0.007072216562271621\n",
      "epoch: 90, loss: 0.007118918715434825\n",
      "epoch: 91, loss: 0.007150962103171232\n",
      "epoch: 92, loss: 0.007108857122385061\n",
      "epoch: 93, loss: 0.007104655235264056\n",
      "epoch: 94, loss: 0.006954561082226289\n",
      "epoch: 95, loss: 0.007049045696412505\n",
      "epoch: 96, loss: 0.007024173232180054\n",
      "epoch: 97, loss: 0.007025743830291915\n",
      "epoch: 98, loss: 0.007054575454800414\n",
      "epoch: 99, loss: 0.007124911642023202\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#qnn_list = []\n",
    "qnn_list = loader(data_path(\"trainability_qnn_3D_reps_2\"))\n",
    "qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "for i in tqdm(range(1, 5)):\n",
    "    qnn = sequential_qnn(n_qubits = [3, 4],\n",
    "                         dim = [3, 4, 1],\n",
    "                         encoder= Encoder(),\n",
    "                         ansatz = Ansatz(blocks = [\"entangle\", \"ry\"], reps = 2),\n",
    "                         sampler = Parity(),\n",
    "                         cost = MSE(),\n",
    "                         optimizer = Adam(lr=0.1),\n",
    "                         backend = backend,\n",
    "                         shots = 10000)\n",
    "    qnn.train(x_qnn, y, epochs=100, verbose=True)\n",
    "    qnn_list.append(qnn)\n",
    "\n",
    "saver(qnn_list, data_path(\"trainability_qnn_3D_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dnn_list = []\n",
    "for i in range(5):\n",
    "    dnn = sequential_dnn(dim = [3, 6, 1],\n",
    "                         optimizer = Adam(lr=0.1))\n",
    "    \n",
    "    dnn.train(x_dnn, y, epochs=10000)\n",
    "    dnn_list.append(dnn)\n",
    "\n",
    "saver(dnn_list, data_path(\"trainability_dnn_3D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep QKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(n,n,n)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "qnn = sequential_qnn(q_bits = [3, 4, 4],\n",
    "                     dim = [3, 4, 4, 1],\n",
    "                     reps = 2,\n",
    "                     backend=backend,\n",
    "                     shots=10000,\n",
    "                     lr = 0.1)\n",
    "\n",
    "qnn.train(x, y, epochs=200, verbose=True)\n",
    "    \n",
    "saver(qnn, data_path(\"trainability_qnn_3D_deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scaler(x, mode=\"standard\")\n",
    "\n",
    "dnn = sequential_dnn(dim = [3, 6, 5, 1], lr = 0.1)\n",
    "\n",
    "dnn.train(x, y, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qnn.loss)\n",
    "plt.plot(dnn.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 6\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x, x, x])\n",
    "\n",
    "mean1 = np.array([[0.25, 0.25, 0.25]])\n",
    "mean2 = np.array([[0.25, 0.25, 0.75]])\n",
    "mean3 = np.array([[0.25, 0.75, 0.75]])\n",
    "mean4 = np.array([[0.25, 0.75, 0.25]])\n",
    "\n",
    "mean5 = np.array([[0.75, 0.25, 0.25]])\n",
    "mean6 = np.array([[0.75, 0.25, 0.75]])\n",
    "mean7 = np.array([[0.75, 0.75, 0.75]])\n",
    "mean8 = np.array([[0.75, 0.75, 0.25]])\n",
    "\n",
    "var = np.array([[0.02, 0, 0], [0, 0.02, 0], [0, 0, 0.02]])\n",
    "\n",
    "y = gaussian(x, mean1, var) - gaussian(x, mean2, var) + gaussian(x, mean3, var) - gaussian(x, mean4, var) - gaussian(x, mean5, var) + gaussian(x, mean6, var) - gaussian(x, mean7, var) + gaussian(x, mean8, var)\n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=-2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "layer1 = QLayer(n_qubits=3, n_features=3, n_targets=3, encoder=Encoder(), ansatz=Ansatz(), sampler=Parity(), reps=2, scale=1, backend=backend, shots=10000)\n",
    "layer2 = Dense(n_features=3, n_targets=1, activation=Identity())\n",
    "layers = [layer1, layer2]\n",
    "network = NeuralNetwork(layers=layers, optimizer = Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x, y, epochs=100, verbose=True)\n",
    "saver(network, data_path(\"trainability_hybrid_2_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(0, 1, n).reshape(-1,1)\n",
    "y = gaussian(x, 0.3, 0.02) - gaussian(x, 0.7, 0.02) \n",
    "\n",
    "x = scaler(x, a=0, b=np.pi)\n",
    "y = scaler(y, a=0.1, b=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = sequential_qnn(q_bits = [3],\n",
    "                         dim = [3, 1],\n",
    "                         reps = 3,\n",
    "                         backend=backend,\n",
    "                         shots=10000,\n",
    "                         lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.train(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
