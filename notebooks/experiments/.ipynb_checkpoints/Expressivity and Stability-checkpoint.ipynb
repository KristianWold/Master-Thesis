{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressivity of QKN vs Classic NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(0, 2*np.pi, 1000)\n",
    "theta = np.append(theta, theta[0:1]).reshape(-1,1)\n",
    "\n",
    "x1 = np.pi*(np.cos(theta)+1)/2\n",
    "x2 = np.pi*(np.sin(theta)+1)/2\n",
    "x_qcn = np.hstack([x1, x2])\n",
    "x_dnn = scaler(x_qcn, mode=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_qcn[:,0], x_qcn[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "network = sequential_qnn(n_qubits = 8*[4],\n",
    "                         dim = [2] + 7*[4],\n",
    "                         scale = 8*[[-np.pi, np.pi]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_width_4_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "network = sequential_qnn(n_qubits = 8*[5],\n",
    "                         dim = [2] + 7*[5],\n",
    "                         scale = 8*[[-np.pi, np.pi]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_width_5_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "network = sequential_qnn(n_qubits = 8*[6],\n",
    "                         dim = [2] + 7*[6],\n",
    "                         scale = 8*[[-np.pi, np.pi]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_width_6_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "network = sequential_qnn(n_qubits = 8*[7],\n",
    "                         dim = [2] + 7*[7],\n",
    "                         scale = 8*[[-np.pi, np.pi]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_width_7_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "network = sequential_qnn(n_qubits = 8*[8],\n",
    "                         dim = [2] + 7*[8],\n",
    "                         scale = 8*[[-np.pi, np.pi]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_width_8_reps_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170f1b8679614bf49ed14a9f3310c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = sequential_dnn(dim=[2] + 7*[10])\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "\n",
    "network.predict(x_dnn, verbose=True)\n",
    "saver(tl, data_path(\"tl_expressivity_dnn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 12\n",
    "x = np.linspace(0, 1, n)\n",
    "x = generate_meshgrid([x,x])\n",
    "\n",
    "mean1 = np.array([[0.2, 0.8]])\n",
    "var1 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean2 = np.array([[0.5, 0.8]])\n",
    "var2 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean3 = np.array([[0.8, 0.8]])\n",
    "var3 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean4 = np.array([[0.2, 0.5]])\n",
    "var4 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean5 = np.array([[0.5, 0.5]])\n",
    "var5 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean6 = np.array([[0.8, 0.5]])\n",
    "var6 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean7 = np.array([[0.2, 0.2]])\n",
    "var7 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean8 = np.array([[0.5, 0.2]])\n",
    "var8 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "mean9 = np.array([[0.8, 0.2]])\n",
    "var9 = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "\n",
    "y = gaussian(x, mean1, var1) - gaussian(x, mean2, var2) + gaussian(x, mean3, var3) - gaussian(x, mean4, var4) +\\\n",
    "gaussian(x, mean5, var5) - gaussian(x, mean6, var6) + gaussian(x, mean7, var7) - gaussian(x, mean8, var8) +\\\n",
    "gaussian(x, mean9, var9)\n",
    "\n",
    "\n",
    "x_train_qcn = scaler(x, a=-np.pi/2, b=np.pi/2)\n",
    "x_train_dnn = scaler(x, mode=\"standard\")\n",
    "y = scaler(y, a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.reshape(n,n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "network = sequential_qnn(n_qubits = [5, 5, 5, 5],\n",
    "                         dim = [2, 5, 5, 5, 1] ,\n",
    "                         scale = 3*[[-np.pi, np.pi]] + [[0,1]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0,\n",
    "                         optimizer=Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_5_epochs_0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92eac2344284c38a35b7fab661be09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.10753750855339678\n",
      "epoch: 1, loss: 0.05469373487535791\n",
      "epoch: 2, loss: 0.06194648418692619\n",
      "epoch: 3, loss: 0.05358460329241951\n",
      "epoch: 4, loss: 0.05378873103479949\n",
      "epoch: 5, loss: 0.05388694821995833\n",
      "epoch: 6, loss: 0.05364019949536528\n",
      "epoch: 7, loss: 0.05283920363719211\n",
      "epoch: 8, loss: 0.052077703096661675\n",
      "epoch: 9, loss: 0.05157267528178281\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_5_epochs_10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4395903dafcf4fe594abff1abc7fd546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.05060743389569197\n",
      "epoch: 1, loss: 0.0499388478173374\n",
      "epoch: 2, loss: 0.04799817186332259\n",
      "epoch: 3, loss: 0.045554830316214844\n",
      "epoch: 4, loss: 0.04091426128932548\n",
      "epoch: 5, loss: 0.03847438209158517\n",
      "epoch: 6, loss: 0.03564557133516782\n",
      "epoch: 7, loss: 0.03814066640740307\n",
      "epoch: 8, loss: 0.028661446243289885\n",
      "epoch: 9, loss: 0.02914614204075895\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_5_epochs_20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939162905f74402caa02364b67c996ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.03817022812221903\n",
      "epoch: 1, loss: 0.02626776343782878\n",
      "epoch: 2, loss: 0.030543742422384595\n",
      "epoch: 3, loss: 0.0248334215038493\n",
      "epoch: 4, loss: 0.030885032494114475\n",
      "epoch: 5, loss: 0.024074534095583706\n",
      "epoch: 6, loss: 0.025654146420673207\n",
      "epoch: 7, loss: 0.021622691826322572\n",
      "epoch: 8, loss: 0.023409098374442296\n",
      "epoch: 9, loss: 0.019499462888387675\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_5_epochs_30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9256e38d0e144136af517c75ed39ea9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.019259825996106778\n",
      "epoch: 1, loss: 0.01545549785570197\n",
      "epoch: 2, loss: 0.015872797803214194\n",
      "epoch: 3, loss: 0.012221755331321011\n",
      "epoch: 4, loss: 0.012623849971247873\n",
      "epoch: 5, loss: 0.010095911990899495\n",
      "epoch: 6, loss: 0.010732578006679353\n",
      "epoch: 7, loss: 0.009253446980001745\n",
      "epoch: 8, loss: 0.008777549505998353\n",
      "epoch: 9, loss: 0.008229580223710042\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_5_epochs_40\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "network = sequential_qnn(n_qubits = [6, 6, 6, 6],\n",
    "                         dim = [2, 6, 6, 6, 1] ,\n",
    "                         scale = 3*[[-np.pi, np.pi]] + [[0,1]],\n",
    "                         encoder = Encoder(mode=\"x\"),\n",
    "                         ansatz = Ansatz(blocks=[\"entangle\", \"ry\"], reps=2),\n",
    "                         sampler = Parity(),\n",
    "                         shots = 0,\n",
    "                         optimizer=Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356e1b1ee4104339b4823906618a8922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08630832338085878\n",
      "epoch: 1, loss: 0.05709119811722255\n",
      "epoch: 2, loss: 0.05400108420612014\n",
      "epoch: 3, loss: 0.053064641330191814\n",
      "epoch: 4, loss: 0.05272043035916593\n",
      "epoch: 5, loss: 0.05242343407293984\n",
      "epoch: 6, loss: 0.05192663486242562\n",
      "epoch: 7, loss: 0.05143327711198853\n",
      "epoch: 8, loss: 0.05056431601586434\n",
      "epoch: 9, loss: 0.04848666015700506\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a329077a07c44b68e4feabd1e7b30ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.046085691928983596\n",
      "epoch: 1, loss: 0.04216105602885562\n",
      "epoch: 2, loss: 0.03933682122375527\n",
      "epoch: 3, loss: 0.033096270329572755\n",
      "epoch: 4, loss: 0.029905018560120948\n",
      "epoch: 5, loss: 0.025008015753748412\n",
      "epoch: 6, loss: 0.023873648413097626\n",
      "epoch: 7, loss: 0.01912223392860887\n",
      "epoch: 8, loss: 0.018102093622113735\n",
      "epoch: 9, loss: 0.016987040080188906\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa33fec16ec4a95a34578769026cf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.014431676708826693\n",
      "epoch: 1, loss: 0.015146095178957958\n",
      "epoch: 2, loss: 0.01958366631535198\n",
      "epoch: 3, loss: 0.023683495859406364\n",
      "epoch: 4, loss: 0.013991335944048496\n",
      "epoch: 5, loss: 0.023104201418871942\n",
      "epoch: 6, loss: 0.01735866745447305\n",
      "epoch: 7, loss: 0.015321837058061338\n",
      "epoch: 8, loss: 0.015234180419374506\n",
      "epoch: 9, loss: 0.016139233828695164\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "l = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dff041a65754e83bb6eaf6a57a9c7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.010671422827442977\n",
      "epoch: 1, loss: 0.015154134450343828\n",
      "epoch: 2, loss: 0.0105531579237618\n",
      "epoch: 3, loss: 0.011381035644690236\n",
      "epoch: 4, loss: 0.009408233004157955\n",
      "epoch: 5, loss: 0.010199279856411392\n",
      "epoch: 6, loss: 0.007658636808282448\n",
      "epoch: 7, loss: 0.00994797983224541\n",
      "epoch: 8, loss: 0.0064721261233679035\n",
      "epoch: 9, loss: 0.008191681451436804\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_40\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40264534226436ab3b47486a1cc1a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.006783865885317551\n",
      "epoch: 1, loss: 0.006475650362592383\n",
      "epoch: 2, loss: 0.0054668203653854865\n",
      "epoch: 3, loss: 0.00683297434565143\n",
      "epoch: 4, loss: 0.004526863409948825\n",
      "epoch: 5, loss: 0.005537339670382462\n",
      "epoch: 6, loss: 0.00436582598421782\n",
      "epoch: 7, loss: 0.004155034803544259\n",
      "epoch: 8, loss: 0.00417155589913147\n",
      "epoch: 9, loss: 0.003284353725829367\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cde3d7334ea4cb195a77d538d9aa6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0038453478504707752\n",
      "epoch: 1, loss: 0.00296567900801277\n",
      "epoch: 2, loss: 0.003108908157427461\n",
      "epoch: 3, loss: 0.0030866851477173735\n",
      "epoch: 4, loss: 0.0023864519975515803\n",
      "epoch: 5, loss: 0.002524318273437375\n",
      "epoch: 6, loss: 0.002691676825104915\n",
      "epoch: 7, loss: 0.002311849065869853\n",
      "epoch: 8, loss: 0.0017002807361710154\n",
      "epoch: 9, loss: 0.002058991681969347\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_60\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231297acac144757b6ba4b8c18c2fdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.002356001331843218\n",
      "epoch: 1, loss: 0.0022743652006216486\n",
      "epoch: 2, loss: 0.002186556486717349\n",
      "epoch: 3, loss: 0.0019316922780747278\n",
      "epoch: 4, loss: 0.0015763981769627204\n",
      "epoch: 5, loss: 0.0015174827470473451\n",
      "epoch: 6, loss: 0.0013429803637499756\n",
      "epoch: 7, loss: 0.0013554341176107917\n",
      "epoch: 8, loss: 0.0013081970214949249\n",
      "epoch: 9, loss: 0.001724320138235086\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_70\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2244e8d1fe4344aa2c9fb0eaa30a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0034581825898862164\n",
      "epoch: 1, loss: 0.009492497513711472\n",
      "epoch: 2, loss: 0.009415565467820372\n",
      "epoch: 3, loss: 0.0018026710178850008\n",
      "epoch: 4, loss: 0.005899212312531957\n",
      "epoch: 5, loss: 0.0025240704981994012\n",
      "epoch: 6, loss: 0.00393041844128431\n",
      "epoch: 7, loss: 0.0022490506657733347\n",
      "epoch: 8, loss: 0.0033260044921717778\n",
      "epoch: 9, loss: 0.0023106501794766966\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_qcn, y, epochs = 10, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_qcn)\n",
    "saver(tl, data_path(\"tl_expressivity_qubit_6_epochs_80\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "network = sequential_dnn(dim=[2, 8, 8, 8, 1],\n",
    "                         optimizer=Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_0_dnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x_train_dnn, y, epochs=66)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_66_dnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x_train_dnn, y, epochs=46)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_112_dnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x_train_dnn, y, epochs=45)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_157_dnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(x_train_dnn, y, epochs=376)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_533_dnn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.04848666015700506\n",
    "0.016139233828695164\n",
    "0.003284353725829367\n",
    "0.002058991681969347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "network = sequential_dnn(dim=[2, 9, 9, 9, 1],\n",
    "                         optimizer=Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_0_dnn_9_nodes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d34171e4394c79b079c69b4b92a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.05993721742985571\n",
      "epoch: 1, loss: 0.06501206684502787\n",
      "epoch: 2, loss: 0.057736839905299174\n",
      "epoch: 3, loss: 0.05362294283117229\n",
      "epoch: 4, loss: 0.056239575025647665\n",
      "epoch: 5, loss: 0.05762808103905383\n",
      "epoch: 6, loss: 0.05604302062088107\n",
      "epoch: 7, loss: 0.05409676083594415\n",
      "epoch: 8, loss: 0.05365388151967762\n",
      "epoch: 9, loss: 0.054486433333276046\n",
      "epoch: 10, loss: 0.05527965839276099\n",
      "epoch: 11, loss: 0.0552164336945919\n",
      "epoch: 12, loss: 0.054513135817509856\n",
      "epoch: 13, loss: 0.05383487949070592\n",
      "epoch: 14, loss: 0.05360239664651471\n",
      "epoch: 15, loss: 0.05378686632939146\n",
      "epoch: 16, loss: 0.05410655802433405\n",
      "epoch: 17, loss: 0.05429661325692822\n",
      "epoch: 18, loss: 0.05425610105449975\n",
      "epoch: 19, loss: 0.054046311856170294\n",
      "epoch: 20, loss: 0.053801986402693944\n",
      "epoch: 21, loss: 0.053637975343443015\n",
      "epoch: 22, loss: 0.0536002029182141\n",
      "epoch: 23, loss: 0.053664555171920314\n",
      "epoch: 24, loss: 0.053766052793128835\n",
      "epoch: 25, loss: 0.05383847568383087\n",
      "epoch: 26, loss: 0.05384580966512607\n",
      "epoch: 27, loss: 0.05379212076251074\n",
      "epoch: 28, loss: 0.05370937526939057\n",
      "epoch: 29, loss: 0.05363522665758791\n",
      "epoch: 30, loss: 0.05359477470619342\n",
      "epoch: 31, loss: 0.053593277922722976\n",
      "epoch: 32, loss: 0.0536191065142609\n",
      "epoch: 33, loss: 0.05365246246554251\n",
      "epoch: 34, loss: 0.053674994397200054\n",
      "epoch: 35, loss: 0.05367662188690865\n",
      "epoch: 36, loss: 0.05365773939591914\n",
      "epoch: 37, loss: 0.05362697444346692\n",
      "epoch: 38, loss: 0.05359621827722177\n",
      "epoch: 39, loss: 0.05357528185224056\n",
      "epoch: 40, loss: 0.05356823994534579\n",
      "epoch: 41, loss: 0.053572608975434304\n",
      "epoch: 42, loss: 0.05358132292833002\n",
      "epoch: 43, loss: 0.053586380229412034\n",
      "epoch: 44, loss: 0.053582412719008495\n",
      "epoch: 45, loss: 0.053568588596202255\n",
      "epoch: 46, loss: 0.053548177297260385\n",
      "epoch: 47, loss: 0.05352628780133898\n",
      "epoch: 48, loss: 0.053507084712345804\n",
      "epoch: 49, loss: 0.053491820394174\n",
      "epoch: 50, loss: 0.05347842152111727\n",
      "epoch: 51, loss: 0.053462558290059685\n",
      "epoch: 52, loss: 0.053439500488732944\n",
      "epoch: 53, loss: 0.05340583650488231\n",
      "epoch: 54, loss: 0.053360320100553534\n",
      "epoch: 55, loss: 0.053303547993643854\n",
      "epoch: 56, loss: 0.05323660067600747\n",
      "epoch: 57, loss: 0.05315911812711116\n",
      "epoch: 58, loss: 0.05306763199730487\n",
      "epoch: 59, loss: 0.05295537638104035\n",
      "epoch: 60, loss: 0.05281402671276335\n",
      "epoch: 61, loss: 0.052636462079915916\n",
      "epoch: 62, loss: 0.05241981070414978\n",
      "epoch: 63, loss: 0.052165896811415785\n",
      "epoch: 64, loss: 0.051878963868211725\n",
      "epoch: 65, loss: 0.05156260697193877\n",
      "epoch: 66, loss: 0.05121427002089591\n",
      "epoch: 67, loss: 0.05082771123407584\n",
      "epoch: 68, loss: 0.05040391232718059\n",
      "epoch: 69, loss: 0.04995353603763746\n",
      "epoch: 70, loss: 0.049477035190635524\n",
      "epoch: 71, loss: 0.04895282158933012\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_dnn, y, epochs=72, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_72_dnn_9_nodes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9888b158a59f438099b1cfa7eda98501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.04835065558742931\n",
      "epoch: 1, loss: 0.047648065482059784\n",
      "epoch: 2, loss: 0.046838722461521175\n",
      "epoch: 3, loss: 0.045915404311156184\n",
      "epoch: 4, loss: 0.0448122834228092\n",
      "epoch: 5, loss: 0.04360102042017292\n",
      "epoch: 6, loss: 0.0424460061563434\n",
      "epoch: 7, loss: 0.04145604233582537\n",
      "epoch: 8, loss: 0.040690470989853046\n",
      "epoch: 9, loss: 0.04012133576847207\n",
      "epoch: 10, loss: 0.03967223588187924\n",
      "epoch: 11, loss: 0.039362194985538154\n",
      "epoch: 12, loss: 0.039164157462300744\n",
      "epoch: 13, loss: 0.03905209539349275\n",
      "epoch: 14, loss: 0.03893987082603199\n",
      "epoch: 15, loss: 0.0387869904048537\n",
      "epoch: 16, loss: 0.03863957436801924\n",
      "epoch: 17, loss: 0.03851674162596329\n",
      "epoch: 18, loss: 0.03837429766268913\n",
      "epoch: 19, loss: 0.03816450378490951\n",
      "epoch: 20, loss: 0.03788748813144293\n",
      "epoch: 21, loss: 0.037702479599868825\n",
      "epoch: 22, loss: 0.03758215115795988\n",
      "epoch: 23, loss: 0.03747242288565068\n",
      "epoch: 24, loss: 0.03753865856183555\n",
      "epoch: 25, loss: 0.03760966109744615\n",
      "epoch: 26, loss: 0.03713428109155902\n",
      "epoch: 27, loss: 0.037092174783136214\n",
      "epoch: 28, loss: 0.037293827430692206\n",
      "epoch: 29, loss: 0.03694400536052426\n",
      "epoch: 30, loss: 0.03710787482875186\n",
      "epoch: 31, loss: 0.036944817785926655\n",
      "epoch: 32, loss: 0.03687485462786706\n",
      "epoch: 33, loss: 0.03693971780238256\n",
      "epoch: 34, loss: 0.036739781833008266\n",
      "epoch: 35, loss: 0.03684262561958302\n",
      "epoch: 36, loss: 0.03665720901591144\n",
      "epoch: 37, loss: 0.03667386829569185\n",
      "epoch: 38, loss: 0.036577722780813275\n",
      "epoch: 39, loss: 0.03651704888037011\n",
      "epoch: 40, loss: 0.036503570357413376\n",
      "epoch: 41, loss: 0.03639154963685265\n",
      "epoch: 42, loss: 0.036376610951846716\n",
      "epoch: 43, loss: 0.03625027617435258\n",
      "epoch: 44, loss: 0.036235925151377225\n",
      "epoch: 45, loss: 0.03612526433809357\n",
      "epoch: 46, loss: 0.03609231556005559\n",
      "epoch: 47, loss: 0.036016636531159114\n",
      "epoch: 48, loss: 0.03594610618395317\n",
      "epoch: 49, loss: 0.03589083882718083\n",
      "epoch: 50, loss: 0.03579293766246532\n",
      "epoch: 51, loss: 0.035718684971098785\n",
      "epoch: 52, loss: 0.03562279000306065\n",
      "epoch: 53, loss: 0.035528444262496954\n",
      "epoch: 54, loss: 0.0354342744283013\n",
      "epoch: 55, loss: 0.03532325191654858\n",
      "epoch: 56, loss: 0.03521305160183244\n",
      "epoch: 57, loss: 0.035106500837567625\n",
      "epoch: 58, loss: 0.03498096148571203\n",
      "epoch: 59, loss: 0.03486933655060571\n",
      "epoch: 60, loss: 0.03472124925892037\n",
      "epoch: 61, loss: 0.03456781324397305\n",
      "epoch: 62, loss: 0.034397166761855745\n",
      "epoch: 63, loss: 0.03420273289996601\n",
      "epoch: 64, loss: 0.034020634808623794\n",
      "epoch: 65, loss: 0.03382370374158509\n",
      "epoch: 66, loss: 0.03360239834726683\n",
      "epoch: 67, loss: 0.033344951454406004\n",
      "epoch: 68, loss: 0.03304761411842532\n",
      "epoch: 69, loss: 0.03269822814238757\n",
      "epoch: 70, loss: 0.032283669308657344\n",
      "epoch: 71, loss: 0.03179210023108113\n",
      "epoch: 72, loss: 0.0313292023387765\n",
      "epoch: 73, loss: 0.0315393764662395\n",
      "epoch: 74, loss: 0.03233821599202219\n",
      "epoch: 75, loss: 0.03142318082799199\n",
      "epoch: 76, loss: 0.030090130825951942\n",
      "epoch: 77, loss: 0.03100643649448799\n",
      "epoch: 78, loss: 0.029234258447110736\n",
      "epoch: 79, loss: 0.0293386853956143\n",
      "epoch: 80, loss: 0.029650127247983785\n",
      "epoch: 81, loss: 0.028102841777091398\n",
      "epoch: 82, loss: 0.028628078156955825\n",
      "epoch: 83, loss: 0.02791241028317553\n",
      "epoch: 84, loss: 0.027308073098880243\n",
      "epoch: 85, loss: 0.027466041900585745\n",
      "epoch: 86, loss: 0.026322787097238485\n",
      "epoch: 87, loss: 0.026234312640680515\n",
      "epoch: 88, loss: 0.025253368399412373\n",
      "epoch: 89, loss: 0.02445846200968073\n",
      "epoch: 90, loss: 0.023926338945578187\n",
      "epoch: 91, loss: 0.0225901814989701\n",
      "epoch: 92, loss: 0.021864340946310387\n",
      "epoch: 93, loss: 0.021159633482212396\n",
      "epoch: 94, loss: 0.020277321964821314\n",
      "epoch: 95, loss: 0.019898334027709307\n",
      "epoch: 96, loss: 0.01964387794530645\n",
      "epoch: 97, loss: 0.0193274197448956\n",
      "epoch: 98, loss: 0.01927364744854285\n",
      "epoch: 99, loss: 0.019364585262468703\n",
      "epoch: 100, loss: 0.019061775277848962\n",
      "epoch: 101, loss: 0.01874857591429038\n",
      "epoch: 102, loss: 0.018644720208700016\n",
      "epoch: 103, loss: 0.01841139329540097\n",
      "epoch: 104, loss: 0.01821425332283924\n",
      "epoch: 105, loss: 0.01818750035507814\n",
      "epoch: 106, loss: 0.018047712026136076\n",
      "epoch: 107, loss: 0.017874526143531827\n",
      "epoch: 108, loss: 0.017789093468955738\n",
      "epoch: 109, loss: 0.01766231791674705\n",
      "epoch: 110, loss: 0.017579512283835235\n",
      "epoch: 111, loss: 0.01750388523874164\n",
      "epoch: 112, loss: 0.01737253962944043\n",
      "epoch: 113, loss: 0.017269412736480774\n",
      "epoch: 114, loss: 0.01711103805982726\n",
      "epoch: 115, loss: 0.016971508869442112\n",
      "epoch: 116, loss: 0.016852185424304614\n",
      "epoch: 117, loss: 0.01671009276343708\n",
      "epoch: 118, loss: 0.01659975034729182\n",
      "epoch: 119, loss: 0.016477779616904054\n",
      "epoch: 120, loss: 0.016360155515457355\n",
      "epoch: 121, loss: 0.016258201841426456\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_dnn, y, epochs=122, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_194_dnn_9_nodes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786afb5901ba4201a728eb86fc5ca6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.01613345151848344\n",
      "epoch: 1, loss: 0.016015165338110857\n",
      "epoch: 2, loss: 0.01592050171967663\n",
      "epoch: 3, loss: 0.015824066311189083\n",
      "epoch: 4, loss: 0.015745392335414235\n",
      "epoch: 5, loss: 0.01567390063337755\n",
      "epoch: 6, loss: 0.015580518210839575\n",
      "epoch: 7, loss: 0.015494992230975453\n",
      "epoch: 8, loss: 0.015411305804976398\n",
      "epoch: 9, loss: 0.0153178624510629\n",
      "epoch: 10, loss: 0.015227624792476444\n",
      "epoch: 11, loss: 0.015135465866034167\n",
      "epoch: 12, loss: 0.015044348446712381\n",
      "epoch: 13, loss: 0.014961815589698176\n",
      "epoch: 14, loss: 0.014888525138438278\n",
      "epoch: 15, loss: 0.014812475767282483\n",
      "epoch: 16, loss: 0.01473491787411738\n",
      "epoch: 17, loss: 0.014655931889841649\n",
      "epoch: 18, loss: 0.01457187332685357\n",
      "epoch: 19, loss: 0.014488504058871813\n",
      "epoch: 20, loss: 0.014406922898464378\n",
      "epoch: 21, loss: 0.014326799034152701\n",
      "epoch: 22, loss: 0.01424658936602123\n",
      "epoch: 23, loss: 0.014169941676793918\n",
      "epoch: 24, loss: 0.014095889992566779\n",
      "epoch: 25, loss: 0.014022477051928963\n",
      "epoch: 26, loss: 0.01394902681657491\n",
      "epoch: 27, loss: 0.013878063030696528\n",
      "epoch: 28, loss: 0.013810519780358296\n",
      "epoch: 29, loss: 0.01374445701979319\n",
      "epoch: 30, loss: 0.013679648682990396\n",
      "epoch: 31, loss: 0.013616803000450916\n",
      "epoch: 32, loss: 0.01355679795285961\n",
      "epoch: 33, loss: 0.013499646774828344\n",
      "epoch: 34, loss: 0.013445199499443512\n",
      "epoch: 35, loss: 0.013391378789238972\n",
      "epoch: 36, loss: 0.013338248510380858\n",
      "epoch: 37, loss: 0.013286759367162325\n",
      "epoch: 38, loss: 0.013237489737986224\n",
      "epoch: 39, loss: 0.013190786942792469\n",
      "epoch: 40, loss: 0.013145837401383954\n",
      "epoch: 41, loss: 0.013101792584311232\n",
      "epoch: 42, loss: 0.013058155428627694\n",
      "epoch: 43, loss: 0.013015144669611432\n",
      "epoch: 44, loss: 0.012972816288236398\n",
      "epoch: 45, loss: 0.01293369543525397\n",
      "epoch: 46, loss: 0.012902544410524667\n",
      "epoch: 47, loss: 0.01289531619820812\n",
      "epoch: 48, loss: 0.012956283924468327\n",
      "epoch: 49, loss: 0.013207900267839183\n",
      "epoch: 50, loss: 0.013849759427089478\n",
      "epoch: 51, loss: 0.014700302088750708\n",
      "epoch: 52, loss: 0.014292019986924448\n",
      "epoch: 53, loss: 0.012804284125587602\n",
      "epoch: 54, loss: 0.012882072610144863\n",
      "epoch: 55, loss: 0.013786255434317527\n",
      "epoch: 56, loss: 0.013065286321503408\n",
      "epoch: 57, loss: 0.012475340315757802\n",
      "epoch: 58, loss: 0.013183120904477806\n",
      "epoch: 59, loss: 0.012910087259449353\n",
      "epoch: 60, loss: 0.012317492055187323\n",
      "epoch: 61, loss: 0.012800865587019308\n",
      "epoch: 62, loss: 0.012670565352019322\n",
      "epoch: 63, loss: 0.01218778746600358\n",
      "epoch: 64, loss: 0.012542487517282694\n",
      "epoch: 65, loss: 0.0124308737034379\n",
      "epoch: 66, loss: 0.012040255154531915\n",
      "epoch: 67, loss: 0.012289778112953917\n",
      "epoch: 68, loss: 0.012211029315197235\n",
      "epoch: 69, loss: 0.011885718638379953\n",
      "epoch: 70, loss: 0.012045047931823467\n",
      "epoch: 71, loss: 0.01202632014494653\n",
      "epoch: 72, loss: 0.011735755273374178\n",
      "epoch: 73, loss: 0.01178494109576194\n",
      "epoch: 74, loss: 0.011834543450297916\n",
      "epoch: 75, loss: 0.011607565913958832\n",
      "epoch: 76, loss: 0.011534438032180298\n",
      "epoch: 77, loss: 0.011615711234148283\n",
      "epoch: 78, loss: 0.011507710778062421\n",
      "epoch: 79, loss: 0.011346120225161365\n",
      "epoch: 80, loss: 0.011351833060917364\n",
      "epoch: 81, loss: 0.011373338403902934\n",
      "epoch: 82, loss: 0.011269183404133095\n",
      "epoch: 83, loss: 0.011144852025930147\n",
      "epoch: 84, loss: 0.01112454108459341\n",
      "epoch: 85, loss: 0.011144267101500377\n",
      "epoch: 86, loss: 0.011093901898560674\n",
      "epoch: 87, loss: 0.010992879264546772\n",
      "epoch: 88, loss: 0.010911652936511716\n",
      "epoch: 89, loss: 0.01088903887530747\n",
      "epoch: 90, loss: 0.010893200624243368\n",
      "epoch: 91, loss: 0.010878990366093515\n",
      "epoch: 92, loss: 0.010835844825682277\n",
      "epoch: 93, loss: 0.010766393213578652\n",
      "epoch: 94, loss: 0.010698910656301703\n",
      "epoch: 95, loss: 0.010641478728720776\n",
      "epoch: 96, loss: 0.01059882130882717\n",
      "epoch: 97, loss: 0.01056658885474978\n",
      "epoch: 98, loss: 0.01054396454558946\n",
      "epoch: 99, loss: 0.010533958016725945\n",
      "epoch: 100, loss: 0.010547882762650904\n",
      "epoch: 101, loss: 0.010621199645827833\n",
      "epoch: 102, loss: 0.01080696386328382\n",
      "epoch: 103, loss: 0.011251228934910694\n",
      "epoch: 104, loss: 0.011822717265160423\n",
      "epoch: 105, loss: 0.012168819030325915\n",
      "epoch: 106, loss: 0.011277558282347121\n",
      "epoch: 107, loss: 0.010335902316484435\n",
      "epoch: 108, loss: 0.010535152328329911\n",
      "epoch: 109, loss: 0.011189048621899721\n",
      "epoch: 110, loss: 0.011050269220335307\n",
      "epoch: 111, loss: 0.010279297801264685\n",
      "epoch: 112, loss: 0.010365158646369702\n",
      "epoch: 113, loss: 0.010904201455561773\n",
      "epoch: 114, loss: 0.010640655531724014\n",
      "epoch: 115, loss: 0.010121485926154753\n",
      "epoch: 116, loss: 0.010258452420651341\n",
      "epoch: 117, loss: 0.010553445541207296\n",
      "epoch: 118, loss: 0.010341432693319642\n",
      "epoch: 119, loss: 0.009998299179609114\n",
      "epoch: 120, loss: 0.01015378653490133\n",
      "epoch: 121, loss: 0.010328845692537504\n",
      "epoch: 122, loss: 0.01009981178551561\n",
      "epoch: 123, loss: 0.009912628891788308\n",
      "epoch: 124, loss: 0.010042275373737412\n",
      "epoch: 125, loss: 0.01012388915520689\n",
      "epoch: 126, loss: 0.009963119803404955\n",
      "epoch: 127, loss: 0.00982101329961135\n",
      "epoch: 128, loss: 0.009901053735370762\n",
      "epoch: 129, loss: 0.009980128001789117\n",
      "epoch: 130, loss: 0.009892204567671084\n",
      "epoch: 131, loss: 0.009750774855181237\n",
      "epoch: 132, loss: 0.009737772168263758\n",
      "epoch: 133, loss: 0.00980789314129845\n",
      "epoch: 134, loss: 0.009830516894932573\n",
      "epoch: 135, loss: 0.009746481286581114\n",
      "epoch: 136, loss: 0.009649776530271105\n",
      "epoch: 137, loss: 0.009615571231917905\n",
      "epoch: 138, loss: 0.009647100997276581\n",
      "epoch: 139, loss: 0.009677234630428416\n",
      "epoch: 140, loss: 0.009671103368977403\n",
      "epoch: 141, loss: 0.009619058791383044\n",
      "epoch: 142, loss: 0.009556140341669232\n",
      "epoch: 143, loss: 0.009501978314522017\n",
      "epoch: 144, loss: 0.009473353663833717\n",
      "epoch: 145, loss: 0.009465230399999266\n",
      "epoch: 146, loss: 0.009470245540719997\n",
      "epoch: 147, loss: 0.00948941149800065\n",
      "epoch: 148, loss: 0.009524567977385413\n",
      "epoch: 149, loss: 0.009589260326321002\n",
      "epoch: 150, loss: 0.009704793925378777\n",
      "epoch: 151, loss: 0.009887809693446107\n",
      "epoch: 152, loss: 0.010162226480852937\n",
      "epoch: 153, loss: 0.01037572233747717\n",
      "epoch: 154, loss: 0.010372861146910748\n",
      "epoch: 155, loss: 0.009963277889185402\n",
      "epoch: 156, loss: 0.009480035895255761\n",
      "epoch: 157, loss: 0.009265826456310916\n",
      "epoch: 158, loss: 0.009412718554036073\n",
      "epoch: 159, loss: 0.009703530171649497\n",
      "epoch: 160, loss: 0.009812255995049606\n",
      "epoch: 161, loss: 0.009642445925269761\n",
      "epoch: 162, loss: 0.009339450809229407\n",
      "epoch: 163, loss: 0.009184344549997978\n",
      "epoch: 164, loss: 0.00924776467712021\n",
      "epoch: 165, loss: 0.009413329797841853\n",
      "epoch: 166, loss: 0.009521817235405869\n",
      "epoch: 167, loss: 0.009459120725800187\n",
      "epoch: 168, loss: 0.009293105911402403\n",
      "epoch: 169, loss: 0.00914277956317945\n",
      "epoch: 170, loss: 0.009096937830669758\n",
      "epoch: 171, loss: 0.009149439972992952\n",
      "epoch: 172, loss: 0.00923736016327767\n",
      "epoch: 173, loss: 0.009299437260477085\n",
      "epoch: 174, loss: 0.009293761937554759\n",
      "epoch: 175, loss: 0.009235645787997175\n",
      "epoch: 176, loss: 0.0091451246920819\n",
      "epoch: 177, loss: 0.009063410817752637\n",
      "epoch: 178, loss: 0.009008750272536413\n",
      "epoch: 179, loss: 0.008986699365267541\n",
      "epoch: 180, loss: 0.008990530131873611\n",
      "epoch: 181, loss: 0.00901324166951444\n",
      "epoch: 182, loss: 0.009053463813034335\n",
      "epoch: 183, loss: 0.00911252394437563\n",
      "epoch: 184, loss: 0.009207444106571781\n",
      "epoch: 185, loss: 0.009338560822386432\n",
      "epoch: 186, loss: 0.009533524308005106\n",
      "epoch: 187, loss: 0.00971759935291476\n",
      "epoch: 188, loss: 0.009855182439012908\n",
      "epoch: 189, loss: 0.009736224135113027\n",
      "epoch: 190, loss: 0.009439186125136423\n",
      "epoch: 191, loss: 0.009064729152764546\n",
      "epoch: 192, loss: 0.00886552364019967\n",
      "epoch: 193, loss: 0.008898886875002423\n",
      "epoch: 194, loss: 0.009080043398305414\n",
      "epoch: 195, loss: 0.009274826522486866\n",
      "epoch: 196, loss: 0.009325123944204317\n",
      "epoch: 197, loss: 0.00922836308657463\n",
      "epoch: 198, loss: 0.009018392934020193\n",
      "epoch: 199, loss: 0.008844635837785783\n",
      "epoch: 200, loss: 0.008778954870564558\n",
      "epoch: 201, loss: 0.008823636061344833\n",
      "epoch: 202, loss: 0.008927019381482576\n",
      "epoch: 203, loss: 0.009019076335270838\n",
      "epoch: 204, loss: 0.009064522483155778\n",
      "epoch: 205, loss: 0.00902686572343621\n",
      "epoch: 206, loss: 0.008941222094826042\n",
      "epoch: 207, loss: 0.008830736072769059\n",
      "epoch: 208, loss: 0.008743734879631933\n",
      "epoch: 209, loss: 0.008696768674369243\n",
      "epoch: 210, loss: 0.00868997476545761\n",
      "epoch: 211, loss: 0.008713034523266968\n",
      "epoch: 212, loss: 0.008755367470571977\n",
      "epoch: 213, loss: 0.008814803373650166\n",
      "epoch: 214, loss: 0.008885738099375219\n",
      "epoch: 215, loss: 0.008981057191473298\n",
      "epoch: 216, loss: 0.009079059917421625\n",
      "epoch: 217, loss: 0.009193089366600233\n",
      "epoch: 218, loss: 0.009243810509864657\n",
      "epoch: 219, loss: 0.009241960639490355\n",
      "epoch: 220, loss: 0.00909951764490028\n",
      "epoch: 221, loss: 0.008908320327703645\n",
      "epoch: 222, loss: 0.008705866976942986\n",
      "epoch: 223, loss: 0.00857978746641325\n",
      "epoch: 224, loss: 0.008545983510459744\n",
      "epoch: 225, loss: 0.008588431223306502\n",
      "epoch: 226, loss: 0.008681544229081482\n",
      "epoch: 227, loss: 0.008799412242070296\n",
      "epoch: 228, loss: 0.008941179600285197\n",
      "epoch: 229, loss: 0.009062158629427471\n",
      "epoch: 230, loss: 0.00916023943039232\n",
      "epoch: 231, loss: 0.009130790341294166\n",
      "epoch: 232, loss: 0.009007246836041953\n",
      "epoch: 233, loss: 0.008772887016006516\n",
      "epoch: 234, loss: 0.00855979198383319\n",
      "epoch: 235, loss: 0.008429268541544792\n",
      "epoch: 236, loss: 0.008408572395371256\n",
      "epoch: 237, loss: 0.008472719910978317\n",
      "epoch: 238, loss: 0.008581397476639933\n",
      "epoch: 239, loss: 0.008711818985661653\n",
      "epoch: 240, loss: 0.00882464920223826\n",
      "epoch: 241, loss: 0.0089183027360883\n",
      "epoch: 242, loss: 0.008925798840061264\n",
      "epoch: 243, loss: 0.00886393300843215\n",
      "epoch: 244, loss: 0.008696149549506626\n",
      "epoch: 245, loss: 0.008507500183767787\n",
      "epoch: 246, loss: 0.008338796394235829\n",
      "epoch: 247, loss: 0.008239309763151759\n",
      "epoch: 248, loss: 0.008211121683467899\n",
      "epoch: 249, loss: 0.008239009130078797\n",
      "epoch: 250, loss: 0.008309938941824179\n",
      "epoch: 251, loss: 0.008423396139289142\n",
      "epoch: 252, loss: 0.008605092230128456\n",
      "epoch: 253, loss: 0.008869857717424144\n",
      "epoch: 254, loss: 0.009239998565930944\n",
      "epoch: 255, loss: 0.009543428431470888\n",
      "epoch: 256, loss: 0.009591017615656077\n",
      "epoch: 257, loss: 0.009081729437566423\n",
      "epoch: 258, loss: 0.00841794574638581\n",
      "epoch: 259, loss: 0.008041431006416747\n",
      "epoch: 260, loss: 0.008166675094895725\n",
      "epoch: 261, loss: 0.008562546976926329\n",
      "epoch: 262, loss: 0.008802128018472237\n",
      "epoch: 263, loss: 0.008700853711419342\n",
      "epoch: 264, loss: 0.008312972083624328\n",
      "epoch: 265, loss: 0.007988260189937126\n",
      "epoch: 266, loss: 0.007929146170597347\n",
      "epoch: 267, loss: 0.008099606493483208\n",
      "epoch: 268, loss: 0.008331460884269866\n",
      "epoch: 269, loss: 0.008443382474119285\n",
      "epoch: 270, loss: 0.008369455897218138\n",
      "epoch: 271, loss: 0.008150459954010666\n",
      "epoch: 272, loss: 0.00792481398485924\n",
      "epoch: 273, loss: 0.00780598342552504\n",
      "epoch: 274, loss: 0.007821596731925479\n",
      "epoch: 275, loss: 0.007926976106157765\n",
      "epoch: 276, loss: 0.008059343260239719\n",
      "epoch: 277, loss: 0.008182373715585778\n",
      "epoch: 278, loss: 0.008261396983182527\n",
      "epoch: 279, loss: 0.008263331337849835\n",
      "epoch: 280, loss: 0.00817828723949564\n",
      "epoch: 281, loss: 0.008025661338216658\n",
      "epoch: 282, loss: 0.007853738364534466\n",
      "epoch: 283, loss: 0.007717371452214715\n",
      "epoch: 284, loss: 0.0076440510271864786\n",
      "epoch: 285, loss: 0.007630809280922829\n",
      "epoch: 286, loss: 0.007663139640593964\n",
      "epoch: 287, loss: 0.007733960369566494\n",
      "epoch: 288, loss: 0.007850831164427483\n",
      "epoch: 289, loss: 0.008045044014732779\n",
      "epoch: 290, loss: 0.008335388336583002\n",
      "epoch: 291, loss: 0.008739842038997254\n",
      "epoch: 292, loss: 0.009039448886268096\n",
      "epoch: 293, loss: 0.00897724212726507\n",
      "epoch: 294, loss: 0.008397288307280273\n",
      "epoch: 295, loss: 0.007728142461313089\n",
      "epoch: 296, loss: 0.007504830135297018\n",
      "epoch: 297, loss: 0.00778098622050923\n",
      "epoch: 298, loss: 0.008177077264209583\n",
      "epoch: 299, loss: 0.008285925839968936\n",
      "epoch: 300, loss: 0.008012348270491125\n",
      "epoch: 301, loss: 0.007601574200224402\n",
      "epoch: 302, loss: 0.007409271465020167\n",
      "epoch: 303, loss: 0.007509219789416326\n",
      "epoch: 304, loss: 0.007742293391276217\n",
      "epoch: 305, loss: 0.00789687243297357\n",
      "epoch: 306, loss: 0.0078311324232087\n",
      "epoch: 307, loss: 0.00761514954593472\n",
      "epoch: 308, loss: 0.007394884255985061\n",
      "epoch: 309, loss: 0.0073058577922394275\n",
      "epoch: 310, loss: 0.007358063921345591\n",
      "epoch: 311, loss: 0.007479427369557794\n",
      "epoch: 312, loss: 0.007589506798635792\n",
      "epoch: 313, loss: 0.007633057462133986\n",
      "epoch: 314, loss: 0.007616143671002424\n",
      "epoch: 315, loss: 0.007526432620409216\n",
      "epoch: 316, loss: 0.0074165824949700854\n",
      "epoch: 317, loss: 0.007304333664778255\n",
      "epoch: 318, loss: 0.007216452434882206\n",
      "epoch: 319, loss: 0.007158165740660439\n",
      "epoch: 320, loss: 0.007126508185434198\n",
      "epoch: 321, loss: 0.007115327489149531\n",
      "epoch: 322, loss: 0.007120817588935182\n",
      "epoch: 323, loss: 0.007151084002914931\n",
      "epoch: 324, loss: 0.007230033135366383\n",
      "epoch: 325, loss: 0.007426901565560066\n",
      "epoch: 326, loss: 0.007846490291690468\n",
      "epoch: 327, loss: 0.008734858128675027\n",
      "epoch: 328, loss: 0.009737419000440015\n",
      "epoch: 329, loss: 0.010311315959760687\n",
      "epoch: 330, loss: 0.008929080305607372\n",
      "epoch: 331, loss: 0.00726812932348781\n",
      "epoch: 332, loss: 0.0073087730866414746\n",
      "epoch: 333, loss: 0.008480251564208409\n",
      "epoch: 334, loss: 0.008621827260649688\n",
      "epoch: 335, loss: 0.007510940118580183\n",
      "epoch: 336, loss: 0.00692821802352589\n",
      "epoch: 337, loss: 0.007526125645381618\n",
      "epoch: 338, loss: 0.008073359393956654\n",
      "epoch: 339, loss: 0.007680256653678492\n",
      "epoch: 340, loss: 0.0069418249045938435\n",
      "epoch: 341, loss: 0.006993749600727945\n",
      "epoch: 342, loss: 0.007498228952972468\n",
      "epoch: 343, loss: 0.007454614427925405\n",
      "epoch: 344, loss: 0.006945205037196939\n",
      "epoch: 345, loss: 0.006773296897812173\n",
      "epoch: 346, loss: 0.007062665117898185\n",
      "epoch: 347, loss: 0.007212014792207839\n",
      "epoch: 348, loss: 0.006924511111162179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 349, loss: 0.006674742308595967\n",
      "epoch: 350, loss: 0.006744017628981592\n",
      "epoch: 351, loss: 0.0069282873577084046\n",
      "epoch: 352, loss: 0.006914649844293089\n",
      "epoch: 353, loss: 0.006703366133473359\n",
      "epoch: 354, loss: 0.006551020624246807\n",
      "epoch: 355, loss: 0.006571166915588021\n",
      "epoch: 356, loss: 0.006676625785189926\n",
      "epoch: 357, loss: 0.006732619537523917\n",
      "epoch: 358, loss: 0.0066557853271038\n",
      "epoch: 359, loss: 0.006521347756465383\n",
      "epoch: 360, loss: 0.00641386556116411\n",
      "epoch: 361, loss: 0.006374501819516438\n",
      "epoch: 362, loss: 0.006394035764876825\n",
      "epoch: 363, loss: 0.006442353701354623\n",
      "epoch: 364, loss: 0.006504841571408092\n",
      "epoch: 365, loss: 0.00655449054807826\n",
      "epoch: 366, loss: 0.0066279894358565565\n",
      "epoch: 367, loss: 0.0066937530930713755\n",
      "epoch: 368, loss: 0.006795749400247814\n",
      "epoch: 369, loss: 0.006858965753036874\n",
      "epoch: 370, loss: 0.006947621902033269\n",
      "epoch: 371, loss: 0.00694232283611829\n",
      "epoch: 372, loss: 0.006925257511651594\n",
      "epoch: 373, loss: 0.006794953287213312\n",
      "epoch: 374, loss: 0.0066499615628581015\n",
      "epoch: 375, loss: 0.0064513760558247224\n",
      "epoch: 376, loss: 0.006287973639814194\n",
      "epoch: 377, loss: 0.006149843184960278\n",
      "epoch: 378, loss: 0.006054005355273138\n",
      "epoch: 379, loss: 0.005987588514818323\n",
      "epoch: 380, loss: 0.005942535601432155\n",
      "epoch: 381, loss: 0.005909066683473712\n",
      "epoch: 382, loss: 0.005881780950022183\n",
      "epoch: 383, loss: 0.005857734015326653\n",
      "epoch: 384, loss: 0.005835316121738716\n",
      "epoch: 385, loss: 0.005815307971281612\n",
      "epoch: 386, loss: 0.005800825097067332\n",
      "epoch: 387, loss: 0.005801426213193405\n",
      "epoch: 388, loss: 0.005847230746770201\n",
      "epoch: 389, loss: 0.0060213368338406\n",
      "epoch: 390, loss: 0.0066064554476134765\n",
      "epoch: 391, loss: 0.008124388923948678\n",
      "epoch: 392, loss: 0.011718082938815141\n",
      "epoch: 393, loss: 0.012957085476334226\n",
      "epoch: 394, loss: 0.009957184178466746\n",
      "epoch: 395, loss: 0.006063176544627775\n",
      "epoch: 396, loss: 0.008124583284805361\n",
      "epoch: 397, loss: 0.009766153971338232\n",
      "epoch: 398, loss: 0.006559111117394138\n",
      "epoch: 399, loss: 0.0064847241160495015\n",
      "epoch: 400, loss: 0.008353537445620392\n",
      "epoch: 401, loss: 0.006707950145560386\n",
      "epoch: 402, loss: 0.005946929684747314\n",
      "epoch: 403, loss: 0.00709059310272576\n",
      "epoch: 404, loss: 0.006756990703057795\n",
      "epoch: 405, loss: 0.00572415040059443\n",
      "epoch: 406, loss: 0.006308848997650329\n",
      "epoch: 407, loss: 0.006302667618205971\n",
      "epoch: 408, loss: 0.0056466988162492005\n",
      "epoch: 409, loss: 0.006022228430895604\n",
      "epoch: 410, loss: 0.006001482844245801\n",
      "epoch: 411, loss: 0.005632761089304752\n",
      "epoch: 412, loss: 0.005742510076598245\n",
      "epoch: 413, loss: 0.005895444834060769\n",
      "epoch: 414, loss: 0.005524034294696657\n",
      "epoch: 415, loss: 0.005630304564234186\n",
      "epoch: 416, loss: 0.005731396900608362\n",
      "epoch: 417, loss: 0.005534768279104838\n",
      "epoch: 418, loss: 0.005468432086048481\n",
      "epoch: 419, loss: 0.005578516992012265\n",
      "epoch: 420, loss: 0.005514698803862825\n",
      "epoch: 421, loss: 0.005348712623932439\n",
      "epoch: 422, loss: 0.005480360354036207\n",
      "epoch: 423, loss: 0.005463344350161847\n",
      "epoch: 424, loss: 0.005335460672415655\n",
      "epoch: 425, loss: 0.0053284593739624\n",
      "epoch: 426, loss: 0.005374673603287584\n",
      "epoch: 427, loss: 0.0053370520818651936\n",
      "epoch: 428, loss: 0.005218216077652072\n",
      "epoch: 429, loss: 0.005251748408823403\n",
      "epoch: 430, loss: 0.005286997808153396\n",
      "epoch: 431, loss: 0.005213237182458738\n",
      "epoch: 432, loss: 0.005158765103737115\n",
      "epoch: 433, loss: 0.005147692876539227\n",
      "epoch: 434, loss: 0.005171198949632121\n",
      "epoch: 435, loss: 0.005148286210196501\n",
      "epoch: 436, loss: 0.005080958635769895\n",
      "epoch: 437, loss: 0.005060791626620356\n",
      "epoch: 438, loss: 0.0050701098520829\n",
      "epoch: 439, loss: 0.005061517250205918\n",
      "epoch: 440, loss: 0.005038554508482764\n",
      "epoch: 441, loss: 0.004998074913322979\n",
      "epoch: 442, loss: 0.004964753916531069\n",
      "epoch: 443, loss: 0.004961743876920587\n",
      "epoch: 444, loss: 0.004960976784510288\n",
      "epoch: 445, loss: 0.0049454825108107\n",
      "epoch: 446, loss: 0.004924979591058121\n",
      "epoch: 447, loss: 0.004896690604780223\n",
      "epoch: 448, loss: 0.004866717045847454\n",
      "epoch: 449, loss: 0.0048476983586606065\n",
      "epoch: 450, loss: 0.004837806058093089\n",
      "epoch: 451, loss: 0.004827534471308926\n",
      "epoch: 452, loss: 0.004815921783032629\n",
      "epoch: 453, loss: 0.0048065088924524315\n",
      "epoch: 454, loss: 0.004794669898027265\n",
      "epoch: 455, loss: 0.004779761566188707\n",
      "epoch: 456, loss: 0.004766949303478803\n",
      "epoch: 457, loss: 0.004760329282473886\n",
      "epoch: 458, loss: 0.004757319755760299\n",
      "epoch: 459, loss: 0.00476521474674125\n",
      "epoch: 460, loss: 0.0047904785646289425\n",
      "epoch: 461, loss: 0.004856973331979742\n",
      "epoch: 462, loss: 0.004987631724884326\n",
      "epoch: 463, loss: 0.005257246375493395\n",
      "epoch: 464, loss: 0.005723648766077085\n",
      "epoch: 465, loss: 0.0065688653629648345\n",
      "epoch: 466, loss: 0.007367362928697643\n",
      "epoch: 467, loss: 0.00784502085073888\n",
      "epoch: 468, loss: 0.006690961470073493\n",
      "epoch: 469, loss: 0.005141419221991156\n",
      "epoch: 470, loss: 0.004690631662848252\n",
      "epoch: 471, loss: 0.005489380043811889\n",
      "epoch: 472, loss: 0.006178757434518783\n",
      "epoch: 473, loss: 0.005641563074572208\n",
      "epoch: 474, loss: 0.004747792924257416\n",
      "epoch: 475, loss: 0.004667424043849725\n",
      "epoch: 476, loss: 0.005154060402315058\n",
      "epoch: 477, loss: 0.0053913221732121664\n",
      "epoch: 478, loss: 0.0050917244128678\n",
      "epoch: 479, loss: 0.004682924647075701\n",
      "epoch: 480, loss: 0.004530472687130662\n",
      "epoch: 481, loss: 0.0048654288698805085\n",
      "epoch: 482, loss: 0.00505057021860077\n",
      "epoch: 483, loss: 0.004651092867899422\n",
      "epoch: 484, loss: 0.004445037529497197\n",
      "epoch: 485, loss: 0.00457234018462814\n",
      "epoch: 486, loss: 0.004647700607952883\n",
      "epoch: 487, loss: 0.004650087715414437\n",
      "epoch: 488, loss: 0.004594107733430485\n",
      "epoch: 489, loss: 0.004446225242627031\n",
      "epoch: 490, loss: 0.004329459530642103\n",
      "epoch: 491, loss: 0.004385873528024063\n",
      "epoch: 492, loss: 0.004508584782628967\n",
      "epoch: 493, loss: 0.004492260260426116\n",
      "epoch: 494, loss: 0.004374773498890016\n",
      "epoch: 495, loss: 0.00429400284848802\n",
      "epoch: 496, loss: 0.004270984229267967\n",
      "epoch: 497, loss: 0.004264125882168725\n",
      "epoch: 498, loss: 0.004286988212910818\n",
      "epoch: 499, loss: 0.00432668399872154\n",
      "epoch: 500, loss: 0.004323539960344397\n",
      "epoch: 501, loss: 0.00426628919380599\n",
      "epoch: 502, loss: 0.0041945912334012975\n",
      "epoch: 503, loss: 0.004158295514634098\n",
      "epoch: 504, loss: 0.004144096680262112\n",
      "epoch: 505, loss: 0.004126129624025333\n",
      "epoch: 506, loss: 0.004112254274806302\n",
      "epoch: 507, loss: 0.00411769612381498\n",
      "epoch: 508, loss: 0.0041432728660949015\n",
      "epoch: 509, loss: 0.0041756417610855565\n",
      "epoch: 510, loss: 0.004213636385556869\n",
      "epoch: 511, loss: 0.004269427702760861\n",
      "epoch: 512, loss: 0.004375805721822246\n",
      "epoch: 513, loss: 0.004548042402565961\n",
      "epoch: 514, loss: 0.004831579859970292\n",
      "epoch: 515, loss: 0.005170227111267311\n",
      "epoch: 516, loss: 0.0055667643157973675\n",
      "epoch: 517, loss: 0.005686836047906935\n",
      "epoch: 518, loss: 0.00544768900583937\n",
      "epoch: 519, loss: 0.004768554760900936\n",
      "epoch: 520, loss: 0.00414490410086738\n",
      "epoch: 521, loss: 0.003935131463706737\n",
      "epoch: 522, loss: 0.004158363862356709\n",
      "epoch: 523, loss: 0.004534423388818798\n",
      "epoch: 524, loss: 0.004722950293775359\n",
      "epoch: 525, loss: 0.0046304728482186265\n",
      "epoch: 526, loss: 0.0042921023613879125\n",
      "epoch: 527, loss: 0.003980912217157093\n",
      "epoch: 528, loss: 0.003845118520737389\n",
      "epoch: 529, loss: 0.0039351794605193635\n",
      "epoch: 530, loss: 0.0041427642985717205\n",
      "epoch: 531, loss: 0.004292440951351788\n",
      "epoch: 532, loss: 0.004321785768654827\n",
      "epoch: 533, loss: 0.004205509591222028\n",
      "epoch: 534, loss: 0.004051381370348731\n",
      "epoch: 535, loss: 0.003914472323142629\n",
      "epoch: 536, loss: 0.0038247406130986236\n",
      "epoch: 537, loss: 0.003773647730406114\n",
      "epoch: 538, loss: 0.0037442224645182634\n",
      "epoch: 539, loss: 0.00374711656638082\n",
      "epoch: 540, loss: 0.003783423395589128\n",
      "epoch: 541, loss: 0.003849760860187525\n",
      "epoch: 542, loss: 0.0039270113287424545\n",
      "epoch: 543, loss: 0.003990586735555673\n",
      "epoch: 544, loss: 0.004009745363354435\n",
      "epoch: 545, loss: 0.003993009160078948\n",
      "epoch: 546, loss: 0.003955800173105666\n",
      "epoch: 547, loss: 0.003937042168906\n",
      "epoch: 548, loss: 0.003924034719401983\n",
      "epoch: 549, loss: 0.003915269796296225\n",
      "epoch: 550, loss: 0.0038666677406626027\n",
      "epoch: 551, loss: 0.003805838011464394\n",
      "epoch: 552, loss: 0.0037336998712977926\n",
      "epoch: 553, loss: 0.003690071384358737\n",
      "epoch: 554, loss: 0.0036819411090365534\n",
      "epoch: 555, loss: 0.0037097438272358845\n",
      "epoch: 556, loss: 0.00375747417923623\n",
      "epoch: 557, loss: 0.0038135960282582853\n",
      "epoch: 558, loss: 0.003867919377579945\n",
      "epoch: 559, loss: 0.003926338711100209\n",
      "epoch: 560, loss: 0.003997504327931625\n",
      "epoch: 561, loss: 0.004149799919291254\n",
      "epoch: 562, loss: 0.00439332763066232\n",
      "epoch: 563, loss: 0.004776097083844127\n",
      "epoch: 564, loss: 0.005033888947731239\n",
      "epoch: 565, loss: 0.005114445483748231\n",
      "epoch: 566, loss: 0.004636746103045694\n",
      "epoch: 567, loss: 0.004059039920580743\n",
      "epoch: 568, loss: 0.0036814100096597227\n",
      "epoch: 569, loss: 0.0035729930093337603\n",
      "epoch: 570, loss: 0.0036684868305474844\n",
      "epoch: 571, loss: 0.0038249605870097\n",
      "epoch: 572, loss: 0.004036777943707147\n",
      "epoch: 573, loss: 0.0041188794945482\n",
      "epoch: 574, loss: 0.003975574414148804\n",
      "epoch: 575, loss: 0.0036192222233688317\n",
      "epoch: 576, loss: 0.0033179586372360636\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_dnn, y, epochs=577, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_831_dnn_9_nodes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac33f9e662a49caad5ad3264e3fd61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.005686836047906935\n",
      "epoch: 1, loss: 0.00544768900583937\n",
      "epoch: 2, loss: 0.004768554760900936\n",
      "epoch: 3, loss: 0.00414490410086738\n",
      "epoch: 4, loss: 0.003935131463706737\n",
      "epoch: 5, loss: 0.004158363862356709\n",
      "epoch: 6, loss: 0.004534423388818798\n",
      "epoch: 7, loss: 0.004722950293775359\n",
      "epoch: 8, loss: 0.0046304728482186265\n",
      "epoch: 9, loss: 0.0042921023613879125\n",
      "epoch: 10, loss: 0.003980912217157093\n",
      "epoch: 11, loss: 0.003845118520737389\n",
      "epoch: 12, loss: 0.0039351794605193635\n",
      "epoch: 13, loss: 0.0041427642985717205\n",
      "epoch: 14, loss: 0.004292440951351788\n",
      "epoch: 15, loss: 0.004321785768654827\n",
      "epoch: 16, loss: 0.004205509591222028\n",
      "epoch: 17, loss: 0.004051381370348731\n",
      "epoch: 18, loss: 0.003914472323142629\n",
      "epoch: 19, loss: 0.0038247406130986236\n",
      "epoch: 20, loss: 0.003773647730406114\n",
      "epoch: 21, loss: 0.0037442224645182634\n",
      "epoch: 22, loss: 0.00374711656638082\n",
      "epoch: 23, loss: 0.003783423395589128\n",
      "epoch: 24, loss: 0.003849760860187525\n",
      "epoch: 25, loss: 0.0039270113287424545\n",
      "epoch: 26, loss: 0.003990586735555673\n",
      "epoch: 27, loss: 0.004009745363354435\n",
      "epoch: 28, loss: 0.003993009160078948\n",
      "epoch: 29, loss: 0.003955800173105666\n",
      "epoch: 30, loss: 0.003937042168906\n",
      "epoch: 31, loss: 0.003924034719401983\n",
      "epoch: 32, loss: 0.003915269796296225\n",
      "epoch: 33, loss: 0.0038666677406626027\n",
      "epoch: 34, loss: 0.003805838011464394\n",
      "epoch: 35, loss: 0.0037336998712977926\n",
      "epoch: 36, loss: 0.003690071384358737\n",
      "epoch: 37, loss: 0.0036819411090365534\n",
      "epoch: 38, loss: 0.0037097438272358845\n",
      "epoch: 39, loss: 0.00375747417923623\n",
      "epoch: 40, loss: 0.0038135960282582853\n",
      "epoch: 41, loss: 0.003867919377579945\n",
      "epoch: 42, loss: 0.003926338711100209\n",
      "epoch: 43, loss: 0.003997504327931625\n",
      "epoch: 44, loss: 0.004149799919291254\n",
      "epoch: 45, loss: 0.00439332763066232\n",
      "epoch: 46, loss: 0.004776097083844127\n",
      "epoch: 47, loss: 0.005033888947731239\n",
      "epoch: 48, loss: 0.005114445483748231\n",
      "epoch: 49, loss: 0.004636746103045694\n",
      "epoch: 50, loss: 0.004059039920580743\n",
      "epoch: 51, loss: 0.0036814100096597227\n",
      "epoch: 52, loss: 0.0035729930093337603\n",
      "epoch: 53, loss: 0.0036684868305474844\n",
      "epoch: 54, loss: 0.0038249605870097\n",
      "epoch: 55, loss: 0.004036777943707147\n",
      "epoch: 56, loss: 0.0041188794945482\n",
      "epoch: 57, loss: 0.003975574414148804\n",
      "epoch: 58, loss: 0.0036192222233688317\n",
      "epoch: 59, loss: 0.0033179586372360636\n",
      "epoch: 60, loss: 0.003252600926504404\n",
      "epoch: 61, loss: 0.0034004035452631872\n",
      "epoch: 62, loss: 0.003609618132411694\n",
      "epoch: 63, loss: 0.003715341825367697\n",
      "epoch: 64, loss: 0.0036751322401399325\n",
      "epoch: 65, loss: 0.00352994764634802\n",
      "epoch: 66, loss: 0.0034020453998254894\n",
      "epoch: 67, loss: 0.0033103261101554505\n",
      "epoch: 68, loss: 0.0032510364817827583\n",
      "epoch: 69, loss: 0.003196172143502548\n",
      "epoch: 70, loss: 0.0031829765366482095\n",
      "epoch: 71, loss: 0.0032176619477806\n",
      "epoch: 72, loss: 0.0032844678246197133\n",
      "epoch: 73, loss: 0.0033626046179866162\n",
      "epoch: 74, loss: 0.003407758176135657\n",
      "epoch: 75, loss: 0.003437409881606483\n",
      "epoch: 76, loss: 0.0034219214076850624\n",
      "epoch: 77, loss: 0.0034025619896874064\n",
      "epoch: 78, loss: 0.003375269780975405\n",
      "epoch: 79, loss: 0.0033739414470627216\n",
      "epoch: 80, loss: 0.0033888446633052838\n",
      "epoch: 81, loss: 0.0034122487200889426\n",
      "epoch: 82, loss: 0.0034026859509100576\n",
      "epoch: 83, loss: 0.003354232099758717\n",
      "epoch: 84, loss: 0.0032627822580610704\n",
      "epoch: 85, loss: 0.003180348486167692\n",
      "epoch: 86, loss: 0.003135852973415194\n",
      "epoch: 87, loss: 0.0031447734257797357\n",
      "epoch: 88, loss: 0.0031819572967142257\n",
      "epoch: 89, loss: 0.003249353750768172\n",
      "epoch: 90, loss: 0.003264577542388542\n",
      "epoch: 91, loss: 0.0032789505917696737\n",
      "epoch: 92, loss: 0.003233936029098867\n",
      "epoch: 93, loss: 0.0032245600773955496\n",
      "epoch: 94, loss: 0.0032692366956020035\n",
      "epoch: 95, loss: 0.003417991456907778\n",
      "epoch: 96, loss: 0.003650447369055824\n",
      "epoch: 97, loss: 0.003919809494872013\n",
      "epoch: 98, loss: 0.004036815080979159\n",
      "epoch: 99, loss: 0.0039640077683639795\n",
      "epoch: 100, loss: 0.003742795758056592\n",
      "epoch: 101, loss: 0.0035919113228578903\n",
      "epoch: 102, loss: 0.0034353583616503623\n",
      "epoch: 103, loss: 0.003302124148180734\n",
      "epoch: 104, loss: 0.0029919369341069603\n",
      "epoch: 105, loss: 0.0028470012715574445\n",
      "epoch: 106, loss: 0.0029666507172608062\n",
      "epoch: 107, loss: 0.003232880980417186\n",
      "epoch: 108, loss: 0.0034811367320752296\n",
      "epoch: 109, loss: 0.0034276133995570215\n",
      "epoch: 110, loss: 0.003240128950405951\n",
      "epoch: 111, loss: 0.0029737123865959244\n",
      "epoch: 112, loss: 0.0028748043024393022\n",
      "epoch: 113, loss: 0.0029304456098683427\n",
      "epoch: 114, loss: 0.00294042099293822\n",
      "epoch: 115, loss: 0.0028633430703203944\n",
      "epoch: 116, loss: 0.0027321323381203563\n",
      "epoch: 117, loss: 0.0027090228262576736\n",
      "epoch: 118, loss: 0.0027991461680533375\n",
      "epoch: 119, loss: 0.002897612974384507\n",
      "epoch: 120, loss: 0.002912800382039993\n",
      "epoch: 121, loss: 0.0028211175701467603\n",
      "epoch: 122, loss: 0.0027236892856747235\n",
      "epoch: 123, loss: 0.00269440018172746\n",
      "epoch: 124, loss: 0.0027158580949190473\n",
      "epoch: 125, loss: 0.0027297769796174403\n",
      "epoch: 126, loss: 0.0026827965465220905\n",
      "epoch: 127, loss: 0.0026174847123720205\n",
      "epoch: 128, loss: 0.002567847990729015\n",
      "epoch: 129, loss: 0.00256126586943394\n",
      "epoch: 130, loss: 0.002586628961595301\n",
      "epoch: 131, loss: 0.0026119544730985236\n",
      "epoch: 132, loss: 0.00262208441278044\n",
      "epoch: 133, loss: 0.002605773340821853\n",
      "epoch: 134, loss: 0.0025902237380375846\n",
      "epoch: 135, loss: 0.0025905391617630363\n",
      "epoch: 136, loss: 0.0026307753297120286\n",
      "epoch: 137, loss: 0.0027307948552572725\n",
      "epoch: 138, loss: 0.0029187567937181166\n",
      "epoch: 139, loss: 0.0032003703713066657\n",
      "epoch: 140, loss: 0.003585451704505143\n",
      "epoch: 141, loss: 0.003962690038148747\n",
      "epoch: 142, loss: 0.004319231636100564\n",
      "epoch: 143, loss: 0.004483058383169659\n",
      "epoch: 144, loss: 0.004399100933090035\n",
      "epoch: 145, loss: 0.003619408030311076\n",
      "epoch: 146, loss: 0.00277688773561558\n",
      "epoch: 147, loss: 0.002485047975890371\n",
      "epoch: 148, loss: 0.0028892820158918702\n",
      "epoch: 149, loss: 0.0034013993756252794\n",
      "epoch: 150, loss: 0.003407796673201428\n",
      "epoch: 151, loss: 0.00311250961252458\n",
      "epoch: 152, loss: 0.0028017015352823508\n",
      "epoch: 153, loss: 0.002662174970207113\n",
      "epoch: 154, loss: 0.002630256043310103\n",
      "epoch: 155, loss: 0.0025942329674422454\n",
      "epoch: 156, loss: 0.002658082564399066\n",
      "epoch: 157, loss: 0.002791996425590665\n",
      "epoch: 158, loss: 0.0028083985966336177\n",
      "epoch: 159, loss: 0.002612530107125723\n",
      "epoch: 160, loss: 0.0023934689772958737\n",
      "epoch: 161, loss: 0.0023777042997581246\n",
      "epoch: 162, loss: 0.0025151513317515036\n",
      "epoch: 163, loss: 0.0026005484565188287\n",
      "epoch: 164, loss: 0.0025538285375635034\n",
      "epoch: 165, loss: 0.002457629703147995\n",
      "epoch: 166, loss: 0.002408820827048813\n",
      "epoch: 167, loss: 0.0023990104669270548\n",
      "epoch: 168, loss: 0.002373945675326947\n",
      "epoch: 169, loss: 0.0023559887443450125\n",
      "epoch: 170, loss: 0.0023768680266904344\n",
      "epoch: 171, loss: 0.0024239423421917694\n",
      "epoch: 172, loss: 0.002445294954088638\n",
      "epoch: 173, loss: 0.0024061253470043114\n",
      "epoch: 174, loss: 0.002340692009731393\n",
      "epoch: 175, loss: 0.0022899572193920076\n",
      "epoch: 176, loss: 0.002272219169625048\n",
      "epoch: 177, loss: 0.0022743539428338793\n",
      "epoch: 178, loss: 0.0022730007915359677\n",
      "epoch: 179, loss: 0.0022650620497828293\n",
      "epoch: 180, loss: 0.0022593587376777257\n",
      "epoch: 181, loss: 0.0022657829026473974\n",
      "epoch: 182, loss: 0.0022813962386808713\n",
      "epoch: 183, loss: 0.0022971646359904265\n",
      "epoch: 184, loss: 0.0023026600530684947\n",
      "epoch: 185, loss: 0.002296431097015489\n",
      "epoch: 186, loss: 0.0022847632850754493\n",
      "epoch: 187, loss: 0.0022785593552405656\n",
      "epoch: 188, loss: 0.002280786158505716\n",
      "epoch: 189, loss: 0.0022912709420810943\n",
      "epoch: 190, loss: 0.0023059103009756407\n",
      "epoch: 191, loss: 0.002325875253730477\n",
      "epoch: 192, loss: 0.0023512228456821845\n",
      "epoch: 193, loss: 0.0023934910576950156\n",
      "epoch: 194, loss: 0.0024586580799564953\n",
      "epoch: 195, loss: 0.002561025885557852\n",
      "epoch: 196, loss: 0.002697109674432252\n",
      "epoch: 197, loss: 0.002867056318544578\n",
      "epoch: 198, loss: 0.0030207547325756916\n",
      "epoch: 199, loss: 0.0031348505922959985\n",
      "epoch: 200, loss: 0.003139073368957843\n",
      "epoch: 201, loss: 0.0030436685830858108\n",
      "epoch: 202, loss: 0.002824796637617924\n",
      "epoch: 203, loss: 0.0025558951121045998\n",
      "epoch: 204, loss: 0.0022841077089044283\n",
      "epoch: 205, loss: 0.002123348531942843\n",
      "epoch: 206, loss: 0.002110457177306223\n",
      "epoch: 207, loss: 0.0022209412489472436\n",
      "epoch: 208, loss: 0.0023898617182072637\n",
      "epoch: 209, loss: 0.002541315658902876\n",
      "epoch: 210, loss: 0.0026397478148998225\n",
      "epoch: 211, loss: 0.002631728345701748\n",
      "epoch: 212, loss: 0.0025477829892528213\n",
      "epoch: 213, loss: 0.002415099047257367\n",
      "epoch: 214, loss: 0.0022931489154659527\n",
      "epoch: 215, loss: 0.002199270079093623\n",
      "epoch: 216, loss: 0.0021331340180826244\n",
      "epoch: 217, loss: 0.0020981582595523216\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train_dnn, y, epochs=158, verbose=True)\n",
    "\n",
    "tl = TrajectoryLength(network)\n",
    "tl.fit(x_dnn)\n",
    "saver(tl, data_path(\"tl_expressivity_epochs_929_dnn_9_nodes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
