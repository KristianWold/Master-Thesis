{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from tqdm.notebook import tqdm\n",
    "from costfunction import *\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient, Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(-np.pi/2, np.pi/2, (100, 20))\n",
    "n = 1\n",
    "d = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entangling First and Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_qnn(n_qubits=d*[5],\n",
    "                             dim=d*[5] + [1],\n",
    "                             encoder = Encoder()\n",
    "                             ansatz = Ansatz(block=[\"entangle\", \"ry\"], reps=1),                      \n",
    "                             cost=NoCost(),\n",
    "                             optimizer=Adam(lr=0.1),\n",
    "                             backend = backend,\n",
    "                             shots = 10000)\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])\n",
    "    \n",
    "saver(grad_average, data_path(\"vanishing_grad_entangle_first\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_average = loader(data_path(\"vanishing_grad_entangle_first\"))\n",
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)/np.sqrt(n)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_qnn(n_qubits=d*[5],\n",
    "                             dim=d*[5] + [1],\n",
    "                             ansatz=Ansatz(block=[\"ry\", \"entangle\"]),                      \n",
    "                             cost=NoCost(),\n",
    "                             backend = backend,\n",
    "                             shots = 10000)\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])\n",
    "    \n",
    "saver(grad_average, data_path(\"vanishing_grad_entangle_last\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_average = loader(data_path(\"vanishing_grad_entangle_last\"))\n",
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)/np.sqrt(n)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_qnn(n_qubits=d*[5],\n",
    "                             dim=d*[5] + [1],\n",
    "                             cost=NoCost(),\n",
    "                             backend = backend,\n",
    "                             shots = 10000)\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])\n",
    "    \n",
    "saver(grad_average, data_path(\"vanishing_grad_depth_10_width_5_reps_1_shots_10k\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_average = loader(data_path(\"vanishing_grad_depth_10_width_5_reps_1_shots_10k\"))\n",
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)/np.sqrt(n)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_qnn(n_qubits=d*[5],\n",
    "                             dim=d*[5] + [1],\n",
    "                             ansatz=Ansatz(reps=2),\n",
    "                             cost=NoCost(),\n",
    "                             backend = backend,\n",
    "                             shots = 10000)\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])\n",
    "    \n",
    "saver(grad_average, data_path(\"vanishing_grad_depth_10_width_5_reps_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_average = loader(data_path(\"vanishing_grad_depth_10_width_5_reps_2\"))\n",
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)/np.sqrt(n)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_qnn(n_qubits=d*[5],\n",
    "                             dim=d*[5] + [1],\n",
    "                             ansatz=Ansatz(reps=3),\n",
    "                             cost=NoCost(),\n",
    "                             backend = backend,\n",
    "                             shots = 10000)\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in  network.weight_gradient_list])\n",
    "    \n",
    "saver(grad_average, data_path(\"vanishing_grad_depth_10_width_5_reps_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_average = loader(data_path(\"vanishing_grad_depth_10_width_5_reps_3\"))\n",
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)/np.sqrt(n)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_dnn(dim=d*[5] + [1], cost=NoCost(), optimizer=Adam(lr=0.1))\n",
    "    \n",
    "    network.backward(x[:,:5])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi=130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_dnn(dim=d*[10] + [1], cost=NoCost(), optimizer=Adam(lr=0.1))\n",
    "    \n",
    "    network.backward(x[:,:10])\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grad_average = np.zeros((n, d))\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    network = sequential_dnn(dim=d*[15] + [1])\n",
    "    \n",
    "    network.backward(x[:,:15], include_loss=False)\n",
    "    \n",
    "    grad_average[i] = np.array([np.mean(np.abs(grad)) for grad in network.weight_gradient_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = np.mean(grad_average, axis=0)\n",
    "std = np.std(grad_average, axis=0)\n",
    "\n",
    "log_mean = np.log10(mean)\n",
    "log_std = np.log10(mean + std) - np.log10(mean)\n",
    "\n",
    "spacing = list(range(d))\n",
    "alpha = 0.5\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 130, facecolor='w', edgecolor='k')\n",
    "plt.fill_between(spacing, log_mean - log_std, log_mean + log_std, alpha=alpha)\n",
    "plt.plot(spacing, log_mean)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(0, 0.1, (10, 10))\n",
    "B = np.random.normal(0, 0.1, (10, 10))\n",
    "C = np.random.normal(0, 0.1, (10, 10))\n",
    "D = np.random.normal(0, 0.1, (10, 10))\n",
    "E = np.random.normal(0, 0.1, (10, 10))\n",
    "F = np.random.normal(0, 0.1, (10, 10))\n",
    "G = np.random.normal(0, 0.1, (10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A@B@C@D@E@F@G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
