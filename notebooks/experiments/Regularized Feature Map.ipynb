{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import Aer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "from neuralnetwork import *\n",
    "from analysis import *\n",
    "from utils import *\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Feature Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 200\n",
    "n_features = 4\n",
    "epochs = 100\n",
    "x = np.random.uniform(0, np.pi, (n, n_features))\n",
    "\n",
    "std = 0.2\n",
    "y = np.sin(2*x[:,2])\n",
    "y = scaler(y, a=0.1, b=0.9).reshape(-1, 1)\n",
    "\n",
    "x_train, y_train = x[:100,:], y[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_train[:,2], y_train,\"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model = RegularizedModel(n_features=n_features, \n",
    "                             n_targets=1, \n",
    "                             reps=2,\n",
    "                             alpha=0.00,\n",
    "                             backend=backend, \n",
    "                             shots=10000, \n",
    "                             optimizer=optimizer)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs, verbose=True) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_regularisation_model_no_penalty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51afd92780e4a8cbe09ba8d239f960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bae53b017d49a896d89029c2c06993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09175838411805637\n",
      "epoch: 1, loss: 0.07669595281098665\n",
      "epoch: 2, loss: 0.078531535230976\n",
      "epoch: 3, loss: 0.08026899982365775\n",
      "epoch: 4, loss: 0.0773016005952245\n",
      "epoch: 5, loss: 0.07209445362370574\n",
      "epoch: 6, loss: 0.0661735921570149\n",
      "epoch: 7, loss: 0.06049803487812892\n",
      "epoch: 8, loss: 0.05456455022008882\n",
      "epoch: 9, loss: 0.048009008568964784\n",
      "epoch: 10, loss: 0.04070023598692346\n",
      "epoch: 11, loss: 0.03417031399145585\n",
      "epoch: 12, loss: 0.029550109116788962\n",
      "epoch: 13, loss: 0.02709130565056888\n",
      "epoch: 14, loss: 0.026029941060663125\n",
      "epoch: 15, loss: 0.026117636297436145\n",
      "epoch: 16, loss: 0.026653022483655614\n",
      "epoch: 17, loss: 0.02687908178472526\n",
      "epoch: 18, loss: 0.02615012085067542\n",
      "epoch: 19, loss: 0.024923861784554323\n",
      "epoch: 20, loss: 0.022962255300449784\n",
      "epoch: 21, loss: 0.020766743865657952\n",
      "epoch: 22, loss: 0.018443861120012346\n",
      "epoch: 23, loss: 0.015298693354712787\n",
      "epoch: 24, loss: 0.012152515056488494\n",
      "epoch: 25, loss: 0.009036991391639968\n",
      "epoch: 26, loss: 0.0061042320073922\n",
      "epoch: 27, loss: 0.0038173523822572484\n",
      "epoch: 28, loss: 0.0022092828211255073\n",
      "epoch: 29, loss: 0.00126212731579195\n",
      "epoch: 30, loss: 0.0007001129345746418\n",
      "epoch: 31, loss: 0.000368915472544121\n",
      "epoch: 32, loss: 0.00019066696554977992\n",
      "epoch: 33, loss: 8.462863873287962e-05\n",
      "epoch: 34, loss: 0.00010833382161195925\n",
      "epoch: 35, loss: 0.00015686876719098089\n",
      "epoch: 36, loss: 0.0003001141960296438\n",
      "epoch: 37, loss: 0.0003842270525516299\n",
      "epoch: 38, loss: 0.0004321235621322693\n",
      "epoch: 39, loss: 0.00038453171450035235\n",
      "epoch: 40, loss: 0.00031836331712785463\n",
      "epoch: 41, loss: 0.0002217842464714825\n",
      "epoch: 42, loss: 0.00015948779048059392\n",
      "epoch: 43, loss: 0.00011559307996643372\n",
      "epoch: 44, loss: 0.00011175516005781039\n",
      "epoch: 45, loss: 0.00013350567021685043\n",
      "epoch: 46, loss: 0.00018068702780942969\n",
      "epoch: 47, loss: 0.00020371400353481507\n",
      "epoch: 48, loss: 0.0002673891878634029\n",
      "epoch: 49, loss: 0.0002655005884329342\n",
      "epoch: 50, loss: 0.00025919287616808843\n",
      "epoch: 51, loss: 0.00021028665350954673\n",
      "epoch: 52, loss: 0.00020254612621319488\n",
      "epoch: 53, loss: 0.00018382450355228973\n",
      "epoch: 54, loss: 0.00021377385288752856\n",
      "epoch: 55, loss: 0.0002105577258214514\n",
      "epoch: 56, loss: 0.00021193352645011663\n",
      "epoch: 57, loss: 0.00017918387005432982\n",
      "epoch: 58, loss: 0.00013647987715863572\n",
      "epoch: 59, loss: 0.00010648297568864304\n",
      "epoch: 60, loss: 9.580044717690229e-05\n",
      "epoch: 61, loss: 8.655972773620812e-05\n",
      "epoch: 62, loss: 7.72301545003623e-05\n",
      "epoch: 63, loss: 7.403885151949638e-05\n",
      "epoch: 64, loss: 6.874778949894945e-05\n",
      "epoch: 65, loss: 4.05164923586404e-05\n",
      "epoch: 66, loss: 4.177137397000027e-05\n",
      "epoch: 67, loss: 3.854862486393866e-05\n",
      "epoch: 68, loss: 3.536498111911997e-05\n",
      "epoch: 69, loss: 3.553627021549078e-05\n",
      "epoch: 70, loss: 3.364716441374092e-05\n",
      "epoch: 71, loss: 3.5681667612673334e-05\n",
      "epoch: 72, loss: 3.431496262014972e-05\n",
      "epoch: 73, loss: 3.82544055324702e-05\n",
      "epoch: 74, loss: 3.089157092252163e-05\n",
      "epoch: 75, loss: 2.9885746658754277e-05\n",
      "epoch: 76, loss: 3.078098850840008e-05\n",
      "epoch: 77, loss: 2.4899213522251753e-05\n",
      "epoch: 78, loss: 2.8755226109879736e-05\n",
      "epoch: 79, loss: 2.6928713279222982e-05\n",
      "epoch: 80, loss: 2.9460332404495475e-05\n",
      "epoch: 81, loss: 3.37035384574562e-05\n",
      "epoch: 82, loss: 2.9573606309713762e-05\n",
      "epoch: 83, loss: 2.2440460676107282e-05\n",
      "epoch: 84, loss: 2.6183995034082365e-05\n",
      "epoch: 85, loss: 3.3236580907442834e-05\n",
      "epoch: 86, loss: 2.779941519038322e-05\n",
      "epoch: 87, loss: 2.9836508645592026e-05\n",
      "epoch: 88, loss: 2.5345935850856603e-05\n",
      "epoch: 89, loss: 3.5548656536838e-05\n",
      "epoch: 90, loss: 2.7218701548219815e-05\n",
      "epoch: 91, loss: 2.5534889558262768e-05\n",
      "epoch: 92, loss: 2.8884335107696946e-05\n",
      "epoch: 93, loss: 1.9915577374982116e-05\n",
      "epoch: 94, loss: 2.6972164751834806e-05\n",
      "epoch: 95, loss: 1.7658583906898953e-05\n",
      "epoch: 96, loss: 2.4615778493241335e-05\n",
      "epoch: 97, loss: 1.8222473608976614e-05\n",
      "epoch: 98, loss: 1.802850925435861e-05\n",
      "epoch: 99, loss: 1.8943027518534822e-05\n",
      "2.342803301819917e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7ee43be55d416b8d4e07db05d87a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0780308127110017\n",
      "epoch: 1, loss: 0.078011929927942\n",
      "epoch: 2, loss: 0.07777388901395557\n",
      "epoch: 3, loss: 0.07770753003546266\n",
      "epoch: 4, loss: 0.07658549513375933\n",
      "epoch: 5, loss: 0.07409029961503522\n",
      "epoch: 6, loss: 0.07053211073077024\n",
      "epoch: 7, loss: 0.06448063774814367\n",
      "epoch: 8, loss: 0.056666994568639426\n",
      "epoch: 9, loss: 0.04766831557641647\n",
      "epoch: 10, loss: 0.0396002216154465\n",
      "epoch: 11, loss: 0.033881856166828414\n",
      "epoch: 12, loss: 0.03146123269449683\n",
      "epoch: 13, loss: 0.029370350789312546\n",
      "epoch: 14, loss: 0.02756129376988611\n",
      "epoch: 15, loss: 0.025150878310087874\n",
      "epoch: 16, loss: 0.022774646496514298\n",
      "epoch: 17, loss: 0.020575439604655688\n",
      "epoch: 18, loss: 0.01854911818916449\n",
      "epoch: 19, loss: 0.016663569240900036\n",
      "epoch: 20, loss: 0.015435197720092607\n",
      "epoch: 21, loss: 0.015043570794100689\n",
      "epoch: 22, loss: 0.014751757080910673\n",
      "epoch: 23, loss: 0.01466543668850713\n",
      "epoch: 24, loss: 0.013556949975564883\n",
      "epoch: 25, loss: 0.011756429431587261\n",
      "epoch: 26, loss: 0.010250998042209181\n",
      "epoch: 27, loss: 0.009073234226560041\n",
      "epoch: 28, loss: 0.008243506195424318\n",
      "epoch: 29, loss: 0.007585262177241437\n",
      "epoch: 30, loss: 0.006917852991510976\n",
      "epoch: 31, loss: 0.006457660308361538\n",
      "epoch: 32, loss: 0.005763872502609555\n",
      "epoch: 33, loss: 0.005104218492892571\n",
      "epoch: 34, loss: 0.004572046688306981\n",
      "epoch: 35, loss: 0.0037623405696341305\n",
      "epoch: 36, loss: 0.002813772666122346\n",
      "epoch: 37, loss: 0.0020811003640816327\n",
      "epoch: 38, loss: 0.0011605781977760672\n",
      "epoch: 39, loss: 0.0005629771648243024\n",
      "epoch: 40, loss: 0.0002717374384292457\n",
      "epoch: 41, loss: 0.00039314695500203755\n",
      "epoch: 42, loss: 0.0007000858054044089\n",
      "epoch: 43, loss: 0.00104826280603228\n",
      "epoch: 44, loss: 0.0011560511190938402\n",
      "epoch: 45, loss: 0.0010091081318431376\n",
      "epoch: 46, loss: 0.0007083122542208537\n",
      "epoch: 47, loss: 0.0003212462997819966\n",
      "epoch: 48, loss: 8.951872169357142e-05\n",
      "epoch: 49, loss: 0.00017323118829245033\n",
      "epoch: 50, loss: 0.00046703231992548973\n",
      "epoch: 51, loss: 0.0006759956277563462\n",
      "epoch: 52, loss: 0.0005265401430586087\n",
      "epoch: 53, loss: 0.00021919471475458072\n",
      "epoch: 54, loss: 6.872807463192244e-05\n",
      "epoch: 55, loss: 7.222050705528621e-05\n",
      "epoch: 56, loss: 0.00017453908329626892\n",
      "epoch: 57, loss: 0.00025169899569618144\n",
      "epoch: 58, loss: 0.00027511029804080115\n",
      "epoch: 59, loss: 0.0002493375860547142\n",
      "epoch: 60, loss: 0.00014774760301230534\n",
      "epoch: 61, loss: 6.912482243562377e-05\n",
      "epoch: 62, loss: 6.20805663844839e-05\n",
      "epoch: 63, loss: 9.615004314409891e-05\n",
      "epoch: 64, loss: 0.00012676899234912253\n",
      "epoch: 65, loss: 0.00016492832836373876\n",
      "epoch: 66, loss: 0.00013056303860844813\n",
      "epoch: 67, loss: 8.385824233594883e-05\n",
      "epoch: 68, loss: 5.669937371381225e-05\n",
      "epoch: 69, loss: 5.3717412850107244e-05\n",
      "epoch: 70, loss: 6.300869217738e-05\n",
      "epoch: 71, loss: 6.487197276372836e-05\n",
      "epoch: 72, loss: 6.525325051831802e-05\n",
      "epoch: 73, loss: 7.083821352453375e-05\n",
      "epoch: 74, loss: 7.038436426911854e-05\n",
      "epoch: 75, loss: 5.327356035990359e-05\n",
      "epoch: 76, loss: 2.5859569952115672e-05\n",
      "epoch: 77, loss: 3.762832919975892e-05\n",
      "epoch: 78, loss: 3.672166944851279e-05\n",
      "epoch: 79, loss: 5.495087610889269e-05\n",
      "epoch: 80, loss: 4.904962868159634e-05\n",
      "epoch: 81, loss: 3.7168029278157874e-05\n",
      "epoch: 82, loss: 3.528832427018141e-05\n",
      "epoch: 83, loss: 2.2495220204523358e-05\n",
      "epoch: 84, loss: 2.4992225788539735e-05\n",
      "epoch: 85, loss: 2.300524801145145e-05\n",
      "epoch: 86, loss: 3.3482286677774546e-05\n",
      "epoch: 87, loss: 4.3286651586324046e-05\n",
      "epoch: 88, loss: 3.225208061412848e-05\n",
      "epoch: 89, loss: 2.304087222474369e-05\n",
      "epoch: 90, loss: 1.973500197387956e-05\n",
      "epoch: 91, loss: 1.5988594307977732e-05\n",
      "epoch: 92, loss: 1.666170677559054e-05\n",
      "epoch: 93, loss: 2.4634882426571543e-05\n",
      "epoch: 94, loss: 3.0379489125630168e-05\n",
      "epoch: 95, loss: 2.3652038937138375e-05\n",
      "epoch: 96, loss: 1.9816306014685513e-05\n",
      "epoch: 97, loss: 1.8955041504949926e-05\n",
      "epoch: 98, loss: 2.2131348125342828e-05\n",
      "epoch: 99, loss: 2.4336798283259935e-05\n",
      "2.122612929879396e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf1e3cfe63a4fcf99b2b230d9d2481e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0779188927681771\n",
      "epoch: 1, loss: 0.07793664141668641\n",
      "epoch: 2, loss: 0.07706481619439766\n",
      "epoch: 3, loss: 0.0758273317558325\n",
      "epoch: 4, loss: 0.07443936161265466\n",
      "epoch: 5, loss: 0.0717027714865923\n",
      "epoch: 6, loss: 0.06714739521462235\n",
      "epoch: 7, loss: 0.06087452305431999\n",
      "epoch: 8, loss: 0.053527301166134036\n",
      "epoch: 9, loss: 0.04678244213405998\n",
      "epoch: 10, loss: 0.04036303943187094\n",
      "epoch: 11, loss: 0.033335757466534006\n",
      "epoch: 12, loss: 0.028223096176569575\n",
      "epoch: 13, loss: 0.025046459782114693\n",
      "epoch: 14, loss: 0.023440756063151852\n",
      "epoch: 15, loss: 0.02302396025848741\n",
      "epoch: 16, loss: 0.02349067310913155\n",
      "epoch: 17, loss: 0.023477006211713003\n",
      "epoch: 18, loss: 0.023349556358420853\n",
      "epoch: 19, loss: 0.023534338173776707\n",
      "epoch: 20, loss: 0.023817786884406455\n",
      "epoch: 21, loss: 0.023529259633385462\n",
      "epoch: 22, loss: 0.022870764442909777\n",
      "epoch: 23, loss: 0.02271145232348751\n",
      "epoch: 24, loss: 0.02247218688661974\n",
      "epoch: 25, loss: 0.022446141474941526\n",
      "epoch: 26, loss: 0.02188861760540084\n",
      "epoch: 27, loss: 0.021360995098555043\n",
      "epoch: 28, loss: 0.021149389872833133\n",
      "epoch: 29, loss: 0.02071744559865028\n",
      "epoch: 30, loss: 0.019921234676476524\n",
      "epoch: 31, loss: 0.018741112093089943\n",
      "epoch: 32, loss: 0.017018608920732793\n",
      "epoch: 33, loss: 0.015150744534272002\n",
      "epoch: 34, loss: 0.013070275480418907\n",
      "epoch: 35, loss: 0.01079817026368905\n",
      "epoch: 36, loss: 0.008292177426050488\n",
      "epoch: 37, loss: 0.006084348559523945\n",
      "epoch: 38, loss: 0.00414699248491492\n",
      "epoch: 39, loss: 0.0026290934192175737\n",
      "epoch: 40, loss: 0.0014385789278942125\n",
      "epoch: 41, loss: 0.000752534350587727\n",
      "epoch: 42, loss: 0.00044201048116445664\n",
      "epoch: 43, loss: 0.00019933850499515723\n",
      "epoch: 44, loss: 8.462544568739322e-05\n",
      "epoch: 45, loss: 4.426377967754421e-05\n",
      "epoch: 46, loss: 7.095166068654952e-05\n",
      "epoch: 47, loss: 0.00010497373246808163\n",
      "epoch: 48, loss: 0.00013708955495300807\n",
      "epoch: 49, loss: 0.000101820442128674\n",
      "epoch: 50, loss: 7.487505528844989e-05\n",
      "epoch: 51, loss: 7.169115941728085e-05\n",
      "epoch: 52, loss: 0.00012407373121372628\n",
      "epoch: 53, loss: 0.00022059340402205024\n",
      "epoch: 54, loss: 0.0002612601724783439\n",
      "epoch: 55, loss: 0.00025049197285592346\n",
      "epoch: 56, loss: 0.0002324001038323282\n",
      "epoch: 57, loss: 0.0001984901609630758\n",
      "epoch: 58, loss: 0.00019620837292202994\n",
      "epoch: 59, loss: 0.00017956449400358766\n",
      "epoch: 60, loss: 0.00016953805148422232\n",
      "epoch: 61, loss: 0.00016353891807917995\n",
      "epoch: 62, loss: 0.0001916831992975559\n",
      "epoch: 63, loss: 0.0001527870623371166\n",
      "epoch: 64, loss: 0.00016245089554947298\n",
      "epoch: 65, loss: 0.0001452144548251502\n",
      "epoch: 66, loss: 0.00014756827186629615\n",
      "epoch: 67, loss: 0.00012853233267143177\n",
      "epoch: 68, loss: 0.00010877857183953331\n",
      "epoch: 69, loss: 9.101659905259735e-05\n",
      "epoch: 70, loss: 8.084998772593057e-05\n",
      "epoch: 71, loss: 9.116112934661765e-05\n",
      "epoch: 72, loss: 0.00010418379489008687\n",
      "epoch: 73, loss: 8.060771917545912e-05\n",
      "epoch: 74, loss: 7.622306689243166e-05\n",
      "epoch: 75, loss: 6.342376685119742e-05\n",
      "epoch: 76, loss: 5.518154885018171e-05\n",
      "epoch: 77, loss: 6.736644083692032e-05\n",
      "epoch: 78, loss: 4.676825023959088e-05\n",
      "epoch: 79, loss: 3.334611247600335e-05\n",
      "epoch: 80, loss: 2.487107900815309e-05\n",
      "epoch: 81, loss: 3.632922721915166e-05\n",
      "epoch: 82, loss: 4.705420871347618e-05\n",
      "epoch: 83, loss: 3.0479377885591817e-05\n",
      "epoch: 84, loss: 2.6038481333224465e-05\n",
      "epoch: 85, loss: 2.947174641603616e-05\n",
      "epoch: 86, loss: 2.1533889757776495e-05\n",
      "epoch: 87, loss: 2.23116689717283e-05\n",
      "epoch: 88, loss: 2.0894294230338478e-05\n",
      "epoch: 89, loss: 1.9809706386190473e-05\n",
      "epoch: 90, loss: 2.4929986090864767e-05\n",
      "epoch: 91, loss: 1.9308253072147892e-05\n",
      "epoch: 92, loss: 2.1522249538921483e-05\n",
      "epoch: 93, loss: 2.0683763263395458e-05\n",
      "epoch: 94, loss: 2.0448890796329978e-05\n",
      "epoch: 95, loss: 1.633685955547959e-05\n",
      "epoch: 96, loss: 1.7617794681685925e-05\n",
      "epoch: 97, loss: 1.6943597280325e-05\n",
      "epoch: 98, loss: 1.9963066046076104e-05\n",
      "epoch: 99, loss: 1.6478345284276896e-05\n",
      "2.081854325034494e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51eefc3b931f4264be9b0d20d4b4b857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08402602574406762\n",
      "epoch: 1, loss: 0.07909943436855121\n",
      "epoch: 2, loss: 0.07922072893902893\n",
      "epoch: 3, loss: 0.07825052189096011\n",
      "epoch: 4, loss: 0.07838790085953447\n",
      "epoch: 5, loss: 0.07822923154748336\n",
      "epoch: 6, loss: 0.07791606477975514\n",
      "epoch: 7, loss: 0.07719741605058157\n",
      "epoch: 8, loss: 0.07555660873115252\n",
      "epoch: 9, loss: 0.07202115730204417\n",
      "epoch: 10, loss: 0.06845725054510661\n",
      "epoch: 11, loss: 0.06307194074315614\n",
      "epoch: 12, loss: 0.057207196643706194\n",
      "epoch: 13, loss: 0.049723527566213246\n",
      "epoch: 14, loss: 0.04218445202235793\n",
      "epoch: 15, loss: 0.03422412541056196\n",
      "epoch: 16, loss: 0.028178686525837528\n",
      "epoch: 17, loss: 0.02366992497886076\n",
      "epoch: 18, loss: 0.020733073593541474\n",
      "epoch: 19, loss: 0.018357184486286475\n",
      "epoch: 20, loss: 0.015774483037165207\n",
      "epoch: 21, loss: 0.01277737173914651\n",
      "epoch: 22, loss: 0.010805757573380172\n",
      "epoch: 23, loss: 0.009722813149218756\n",
      "epoch: 24, loss: 0.00872640858611462\n",
      "epoch: 25, loss: 0.007961991808786555\n",
      "epoch: 26, loss: 0.006959229937161311\n",
      "epoch: 27, loss: 0.006355282594069387\n",
      "epoch: 28, loss: 0.006034500508389043\n",
      "epoch: 29, loss: 0.005626058944023491\n",
      "epoch: 30, loss: 0.005077006929179589\n",
      "epoch: 31, loss: 0.004670281718159483\n",
      "epoch: 32, loss: 0.004171263078951182\n",
      "epoch: 33, loss: 0.003674815129961176\n",
      "epoch: 34, loss: 0.0032496635652270265\n",
      "epoch: 35, loss: 0.002787276448881722\n",
      "epoch: 36, loss: 0.002336372788056509\n",
      "epoch: 37, loss: 0.0019103810167816167\n",
      "epoch: 38, loss: 0.0016343728888921982\n",
      "epoch: 39, loss: 0.0014879730590993182\n",
      "epoch: 40, loss: 0.0013075885555332194\n",
      "epoch: 41, loss: 0.0012541336352667487\n",
      "epoch: 42, loss: 0.0011567029423953693\n",
      "epoch: 43, loss: 0.0010143444587378202\n",
      "epoch: 44, loss: 0.0009936678573410022\n",
      "epoch: 45, loss: 0.0010242288258453635\n",
      "epoch: 46, loss: 0.001047443220768408\n",
      "epoch: 47, loss: 0.0010303850281393992\n",
      "epoch: 48, loss: 0.0009535700325201482\n",
      "epoch: 49, loss: 0.0008057526828968351\n",
      "epoch: 50, loss: 0.0005694265716108147\n",
      "epoch: 51, loss: 0.00040929281857559274\n",
      "epoch: 52, loss: 0.00037176926093206164\n",
      "epoch: 53, loss: 0.000488284893718858\n",
      "epoch: 54, loss: 0.0006446274214837153\n",
      "epoch: 55, loss: 0.0005375685752307299\n",
      "epoch: 56, loss: 0.0003214587335114543\n",
      "epoch: 57, loss: 0.00022883006168400812\n",
      "epoch: 58, loss: 0.00020782785813218014\n",
      "epoch: 59, loss: 0.0002607912711286416\n",
      "epoch: 60, loss: 0.0003172077494679908\n",
      "epoch: 61, loss: 0.00027364389151124906\n",
      "epoch: 62, loss: 0.00019159878027364945\n",
      "epoch: 63, loss: 0.00012605803912058236\n",
      "epoch: 64, loss: 0.00012407338112474777\n",
      "epoch: 65, loss: 0.00016327766427373113\n",
      "epoch: 66, loss: 0.00020003893118544197\n",
      "epoch: 67, loss: 0.0001590917298191907\n",
      "epoch: 68, loss: 0.00012890814312887296\n",
      "epoch: 69, loss: 9.784843809687532e-05\n",
      "epoch: 70, loss: 9.469548322319795e-05\n",
      "epoch: 71, loss: 8.180832560732723e-05\n",
      "epoch: 72, loss: 7.162869668586835e-05\n",
      "epoch: 73, loss: 6.756972959099557e-05\n",
      "epoch: 74, loss: 9.487581017867587e-05\n",
      "epoch: 75, loss: 8.261489341287072e-05\n",
      "epoch: 76, loss: 6.884441463673641e-05\n",
      "epoch: 77, loss: 5.344767026407145e-05\n",
      "epoch: 78, loss: 4.8498018677863e-05\n",
      "epoch: 79, loss: 5.8953089333804564e-05\n",
      "epoch: 80, loss: 5.5862339596778766e-05\n",
      "epoch: 81, loss: 5.618121050637066e-05\n",
      "epoch: 82, loss: 4.655497863685488e-05\n",
      "epoch: 83, loss: 4.336935330853027e-05\n",
      "epoch: 84, loss: 5.396493770799698e-05\n",
      "epoch: 85, loss: 5.0304164702501446e-05\n",
      "epoch: 86, loss: 4.606452040061285e-05\n",
      "epoch: 87, loss: 5.975487916152736e-05\n",
      "epoch: 88, loss: 6.12638977869676e-05\n",
      "epoch: 89, loss: 6.143570933169592e-05\n",
      "epoch: 90, loss: 4.080681312559039e-05\n",
      "epoch: 91, loss: 3.769886509798254e-05\n",
      "epoch: 92, loss: 3.415488982710572e-05\n",
      "epoch: 93, loss: 3.8040001695601506e-05\n",
      "epoch: 94, loss: 4.5812632785178234e-05\n",
      "epoch: 95, loss: 4.476300330778236e-05\n",
      "epoch: 96, loss: 4.686890573942719e-05\n",
      "epoch: 97, loss: 5.0409428328612885e-05\n",
      "epoch: 98, loss: 3.310876718291227e-05\n",
      "epoch: 99, loss: 3.539131792435066e-05\n",
      "3.067747450585406e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500617b4d8b74e25ae89b80b3b3a52d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07965057765882654\n",
      "epoch: 1, loss: 0.07916211511176828\n",
      "epoch: 2, loss: 0.07922648795665528\n",
      "epoch: 3, loss: 0.07814329359777611\n",
      "epoch: 4, loss: 0.07826080216347577\n",
      "epoch: 5, loss: 0.07839439765432571\n",
      "epoch: 6, loss: 0.07861309637916088\n",
      "epoch: 7, loss: 0.0776349012864245\n",
      "epoch: 8, loss: 0.07693348677261636\n",
      "epoch: 9, loss: 0.07505907892290692\n",
      "epoch: 10, loss: 0.07059307366445129\n",
      "epoch: 11, loss: 0.0638374855095767\n",
      "epoch: 12, loss: 0.05503284919004714\n",
      "epoch: 13, loss: 0.04499506586713345\n",
      "epoch: 14, loss: 0.036827639290982817\n",
      "epoch: 15, loss: 0.032005120135952254\n",
      "epoch: 16, loss: 0.029005737166121343\n",
      "epoch: 17, loss: 0.02689788386000041\n",
      "epoch: 18, loss: 0.024409583593384614\n",
      "epoch: 19, loss: 0.022696979677947707\n",
      "epoch: 20, loss: 0.021605132674600534\n",
      "epoch: 21, loss: 0.020862727001921478\n",
      "epoch: 22, loss: 0.020010677779601464\n",
      "epoch: 23, loss: 0.01993524832854506\n",
      "epoch: 24, loss: 0.020188094611935825\n",
      "epoch: 25, loss: 0.020646112095619222\n",
      "epoch: 26, loss: 0.020777505586047487\n",
      "epoch: 27, loss: 0.02082746361702029\n",
      "epoch: 28, loss: 0.020459713243457533\n",
      "epoch: 29, loss: 0.01965651593156027\n",
      "epoch: 30, loss: 0.01808418896111528\n",
      "epoch: 31, loss: 0.01622290931252883\n",
      "epoch: 32, loss: 0.014998821586563729\n",
      "epoch: 33, loss: 0.013749935883326634\n",
      "epoch: 34, loss: 0.012037044762660997\n",
      "epoch: 35, loss: 0.010707251297954417\n",
      "epoch: 36, loss: 0.00921045730797091\n",
      "epoch: 37, loss: 0.007988733706111372\n",
      "epoch: 38, loss: 0.00693962618294108\n",
      "epoch: 39, loss: 0.0057329125250094445\n",
      "epoch: 40, loss: 0.004307518940715707\n",
      "epoch: 41, loss: 0.003005234948454675\n",
      "epoch: 42, loss: 0.0016775242972273876\n",
      "epoch: 43, loss: 0.0008636256712008803\n",
      "epoch: 44, loss: 0.00035451588073657716\n",
      "epoch: 45, loss: 0.00019608210713453652\n",
      "epoch: 46, loss: 0.0002109383585925111\n",
      "epoch: 47, loss: 0.0003398967251942109\n",
      "epoch: 48, loss: 0.00039768466023027153\n",
      "epoch: 49, loss: 0.0003777457533099096\n",
      "epoch: 50, loss: 0.0003494285603651302\n",
      "epoch: 51, loss: 0.0002995194475805702\n",
      "epoch: 52, loss: 0.00032293053538591944\n",
      "epoch: 53, loss: 0.00035993239037563035\n",
      "epoch: 54, loss: 0.00036275010033403406\n",
      "epoch: 55, loss: 0.00041762503709068586\n",
      "epoch: 56, loss: 0.0003356503985521709\n",
      "epoch: 57, loss: 0.00023432364328009197\n",
      "epoch: 58, loss: 0.0001686901836712451\n",
      "epoch: 59, loss: 0.0001397069916857074\n",
      "epoch: 60, loss: 0.00017311208982756908\n",
      "epoch: 61, loss: 0.0001680904063896007\n",
      "epoch: 62, loss: 0.00014781535703876193\n",
      "epoch: 63, loss: 0.00012151450085526057\n",
      "epoch: 64, loss: 8.365841763560481e-05\n",
      "epoch: 65, loss: 6.663859666022643e-05\n",
      "epoch: 66, loss: 8.832363477969726e-05\n",
      "epoch: 67, loss: 0.0001056138537264112\n",
      "epoch: 68, loss: 7.417500881532603e-05\n",
      "epoch: 69, loss: 5.630688575266848e-05\n",
      "epoch: 70, loss: 5.103725696344022e-05\n",
      "epoch: 71, loss: 4.7913758430646126e-05\n",
      "epoch: 72, loss: 5.174664584236108e-05\n",
      "epoch: 73, loss: 6.683491034000874e-05\n",
      "epoch: 74, loss: 5.140727925260746e-05\n",
      "epoch: 75, loss: 3.9647061711741086e-05\n",
      "epoch: 76, loss: 3.4560323444000784e-05\n",
      "epoch: 77, loss: 4.396108299403087e-05\n",
      "epoch: 78, loss: 4.5311795978270296e-05\n",
      "epoch: 79, loss: 4.6537891914647104e-05\n",
      "epoch: 80, loss: 4.8961220528750434e-05\n",
      "epoch: 81, loss: 3.200969324958603e-05\n",
      "epoch: 82, loss: 2.8893851924322004e-05\n",
      "epoch: 83, loss: 2.2689125084969664e-05\n",
      "epoch: 84, loss: 2.71088572117949e-05\n",
      "epoch: 85, loss: 3.623847692394629e-05\n",
      "epoch: 86, loss: 2.5849917028920362e-05\n",
      "epoch: 87, loss: 2.8228719816517015e-05\n",
      "epoch: 88, loss: 2.6714104981886887e-05\n",
      "epoch: 89, loss: 2.6573655308469304e-05\n",
      "epoch: 90, loss: 2.5912653301597887e-05\n",
      "epoch: 91, loss: 2.3329082547351946e-05\n",
      "epoch: 92, loss: 2.649206643403597e-05\n",
      "epoch: 93, loss: 1.759918207383889e-05\n",
      "epoch: 94, loss: 1.8184045604936437e-05\n",
      "epoch: 95, loss: 1.9500135256464198e-05\n",
      "epoch: 96, loss: 2.3360922713401078e-05\n",
      "epoch: 97, loss: 2.2993256674288196e-05\n",
      "epoch: 98, loss: 2.8513044904062073e-05\n",
      "epoch: 99, loss: 2.2603899152571304e-05\n",
      "1.2608095150635165e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094e68aa64fd480ab3ed119549c408f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.10164099323983367\n",
      "epoch: 1, loss: 0.08098502895922605\n",
      "epoch: 2, loss: 0.07823058953701983\n",
      "epoch: 3, loss: 0.07862894629369227\n",
      "epoch: 4, loss: 0.07792821249826341\n",
      "epoch: 5, loss: 0.07592224749432916\n",
      "epoch: 6, loss: 0.07417238949153097\n",
      "epoch: 7, loss: 0.07149269116470293\n",
      "epoch: 8, loss: 0.06752974845544912\n",
      "epoch: 9, loss: 0.06304071940977198\n",
      "epoch: 10, loss: 0.056635694047593256\n",
      "epoch: 11, loss: 0.05028814454980576\n",
      "epoch: 12, loss: 0.04454593691166382\n",
      "epoch: 13, loss: 0.03855722671753347\n",
      "epoch: 14, loss: 0.03308519835109327\n",
      "epoch: 15, loss: 0.02805374726387284\n",
      "epoch: 16, loss: 0.02446591579251939\n",
      "epoch: 17, loss: 0.02244320625787473\n",
      "epoch: 18, loss: 0.02202082383416956\n",
      "epoch: 19, loss: 0.022142108140587046\n",
      "epoch: 20, loss: 0.022663174017245775\n",
      "epoch: 21, loss: 0.0224247659526626\n",
      "epoch: 22, loss: 0.02141350031812659\n",
      "epoch: 23, loss: 0.01979940209625359\n",
      "epoch: 24, loss: 0.01737718446898847\n",
      "epoch: 25, loss: 0.015290160580541676\n",
      "epoch: 26, loss: 0.013323693060980836\n",
      "epoch: 27, loss: 0.01156555080815801\n",
      "epoch: 28, loss: 0.00988446678288238\n",
      "epoch: 29, loss: 0.008408971616294567\n",
      "epoch: 30, loss: 0.006742787966890467\n",
      "epoch: 31, loss: 0.005086209522612436\n",
      "epoch: 32, loss: 0.0035334462786810513\n",
      "epoch: 33, loss: 0.002349640722332981\n",
      "epoch: 34, loss: 0.0014596899773486507\n",
      "epoch: 35, loss: 0.0009416563936007448\n",
      "epoch: 36, loss: 0.0006599495382014678\n",
      "epoch: 37, loss: 0.0004918836787285057\n",
      "epoch: 38, loss: 0.0004578541652013941\n",
      "epoch: 39, loss: 0.0004908171670959547\n",
      "epoch: 40, loss: 0.0004872258380590742\n",
      "epoch: 41, loss: 0.0004440615729058774\n",
      "epoch: 42, loss: 0.0003768131349264695\n",
      "epoch: 43, loss: 0.00027930374086864975\n",
      "epoch: 44, loss: 0.00016805302436570906\n",
      "epoch: 45, loss: 0.00016354424310138993\n",
      "epoch: 46, loss: 0.0002248818252365486\n",
      "epoch: 47, loss: 0.0003369050661902673\n",
      "epoch: 48, loss: 0.0004249927389441294\n",
      "epoch: 49, loss: 0.0003634001551330287\n",
      "epoch: 50, loss: 0.00027638453099862394\n",
      "epoch: 51, loss: 0.00016344432074529513\n",
      "epoch: 52, loss: 9.774680325924823e-05\n",
      "epoch: 53, loss: 8.411771612516429e-05\n",
      "epoch: 54, loss: 0.00013869523473830322\n",
      "epoch: 55, loss: 0.00015488372810669386\n",
      "epoch: 56, loss: 0.00018679190801608954\n",
      "epoch: 57, loss: 0.00018046591212769714\n",
      "epoch: 58, loss: 0.0001323515304499783\n",
      "epoch: 59, loss: 8.727395642195483e-05\n",
      "epoch: 60, loss: 4.240296773695259e-05\n",
      "epoch: 61, loss: 4.7411926159072974e-05\n",
      "epoch: 62, loss: 7.575242463854448e-05\n",
      "epoch: 63, loss: 0.00012676331142168917\n",
      "epoch: 64, loss: 0.00012290546829209965\n",
      "epoch: 65, loss: 8.709061833212593e-05\n",
      "epoch: 66, loss: 5.697716931475297e-05\n",
      "epoch: 67, loss: 3.3521836641179445e-05\n",
      "epoch: 68, loss: 4.429924926322759e-05\n",
      "epoch: 69, loss: 5.025323528890827e-05\n",
      "epoch: 70, loss: 6.823872465532058e-05\n",
      "epoch: 71, loss: 6.82514066271338e-05\n",
      "epoch: 72, loss: 5.357817277149137e-05\n",
      "epoch: 73, loss: 3.631056571039661e-05\n",
      "epoch: 74, loss: 2.748598611307097e-05\n",
      "epoch: 75, loss: 4.097808762598631e-05\n",
      "epoch: 76, loss: 4.743120244623668e-05\n",
      "epoch: 77, loss: 5.172486176036277e-05\n",
      "epoch: 78, loss: 4.913667239871915e-05\n",
      "epoch: 79, loss: 2.6931919739787233e-05\n",
      "epoch: 80, loss: 3.5657665835429504e-05\n",
      "epoch: 81, loss: 3.2773227674036185e-05\n",
      "epoch: 82, loss: 2.6374897512982322e-05\n",
      "epoch: 83, loss: 3.9018052917759663e-05\n",
      "epoch: 84, loss: 2.990256577942708e-05\n",
      "epoch: 85, loss: 3.388263223551425e-05\n",
      "epoch: 86, loss: 2.896123767392617e-05\n",
      "epoch: 87, loss: 2.6178748742238233e-05\n",
      "epoch: 88, loss: 3.326236752851523e-05\n",
      "epoch: 89, loss: 3.167666033792174e-05\n",
      "epoch: 90, loss: 3.655221077965801e-05\n",
      "epoch: 91, loss: 3.282907715059254e-05\n",
      "epoch: 92, loss: 2.900067635069178e-05\n",
      "epoch: 93, loss: 3.3436328189177647e-05\n",
      "epoch: 94, loss: 2.614178096543284e-05\n",
      "epoch: 95, loss: 3.100934798222181e-05\n",
      "epoch: 96, loss: 3.230593942718935e-05\n",
      "epoch: 97, loss: 3.357589775982815e-05\n",
      "epoch: 98, loss: 2.4275968391597265e-05\n",
      "epoch: 99, loss: 3.65997029229822e-05\n",
      "2.5159289470673862e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11551d023f694db485b8ba7f4539fba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07755790645966283\n",
      "epoch: 1, loss: 0.07802661325984422\n",
      "epoch: 2, loss: 0.0779639091873206\n",
      "epoch: 3, loss: 0.07730930839261828\n",
      "epoch: 4, loss: 0.07600666613252285\n",
      "epoch: 5, loss: 0.07370317233188015\n",
      "epoch: 6, loss: 0.07002075364560262\n",
      "epoch: 7, loss: 0.06526878273657824\n",
      "epoch: 8, loss: 0.05882318936897657\n",
      "epoch: 9, loss: 0.05061748035678662\n",
      "epoch: 10, loss: 0.04259687342594848\n",
      "epoch: 11, loss: 0.03429835257381004\n",
      "epoch: 12, loss: 0.028251010822739443\n",
      "epoch: 13, loss: 0.024203299441081336\n",
      "epoch: 14, loss: 0.022704360048088693\n",
      "epoch: 15, loss: 0.022611283675558754\n",
      "epoch: 16, loss: 0.022941238031170923\n",
      "epoch: 17, loss: 0.02287272952441047\n",
      "epoch: 18, loss: 0.023264404843491734\n",
      "epoch: 19, loss: 0.023326933521638775\n",
      "epoch: 20, loss: 0.02327694545396054\n",
      "epoch: 21, loss: 0.023131826334515878\n",
      "epoch: 22, loss: 0.022496265141626552\n",
      "epoch: 23, loss: 0.02275036052469158\n",
      "epoch: 24, loss: 0.0220664769387215\n",
      "epoch: 25, loss: 0.02162225393052182\n",
      "epoch: 26, loss: 0.020701279632502095\n",
      "epoch: 27, loss: 0.019515868645825\n",
      "epoch: 28, loss: 0.017860385182907966\n",
      "epoch: 29, loss: 0.01585714390805255\n",
      "epoch: 30, loss: 0.013453860263652804\n",
      "epoch: 31, loss: 0.010794250427818173\n",
      "epoch: 32, loss: 0.007887526408826763\n",
      "epoch: 33, loss: 0.005437416642918085\n",
      "epoch: 34, loss: 0.0033839770542195544\n",
      "epoch: 35, loss: 0.0020525186582032156\n",
      "epoch: 36, loss: 0.0010712294540306353\n",
      "epoch: 37, loss: 0.00044612252262127934\n",
      "epoch: 38, loss: 0.0001680396988911529\n",
      "epoch: 39, loss: 0.00011510646311616371\n",
      "epoch: 40, loss: 0.00017491130606890043\n",
      "epoch: 41, loss: 0.00022766929083356836\n",
      "epoch: 42, loss: 0.00018988170859357813\n",
      "epoch: 43, loss: 0.00014541887249879814\n",
      "epoch: 44, loss: 0.00010898450066113329\n",
      "epoch: 45, loss: 9.191098780787055e-05\n",
      "epoch: 46, loss: 0.00010857164117834969\n",
      "epoch: 47, loss: 0.00022384339536694677\n",
      "epoch: 48, loss: 0.00031228840591249245\n",
      "epoch: 49, loss: 0.00034310434663213684\n",
      "epoch: 50, loss: 0.00022545983393545607\n",
      "epoch: 51, loss: 0.00010501128108226264\n",
      "epoch: 52, loss: 7.399758923542195e-05\n",
      "epoch: 53, loss: 0.00011784967742455444\n",
      "epoch: 54, loss: 0.00016699119729516695\n",
      "epoch: 55, loss: 0.0001732840713922229\n",
      "epoch: 56, loss: 0.00017971890197408284\n",
      "epoch: 57, loss: 0.0001452584065378096\n",
      "epoch: 58, loss: 0.00010796931097284903\n",
      "epoch: 59, loss: 8.822516560673697e-05\n",
      "epoch: 60, loss: 7.610375611416409e-05\n",
      "epoch: 61, loss: 9.643175262889997e-05\n",
      "epoch: 62, loss: 0.00012041855489826849\n",
      "epoch: 63, loss: 0.000135251549644734\n",
      "epoch: 64, loss: 0.00013031331678782943\n",
      "epoch: 65, loss: 0.00011339909018235323\n",
      "epoch: 66, loss: 8.934567053595953e-05\n",
      "epoch: 67, loss: 7.855915118212162e-05\n",
      "epoch: 68, loss: 7.507718229032029e-05\n",
      "epoch: 69, loss: 7.632270260395094e-05\n",
      "epoch: 70, loss: 8.704940668936609e-05\n",
      "epoch: 71, loss: 8.462757678403044e-05\n",
      "epoch: 72, loss: 9.681915835834935e-05\n",
      "epoch: 73, loss: 7.413767898409247e-05\n",
      "epoch: 74, loss: 5.5996941980715154e-05\n",
      "epoch: 75, loss: 4.9271464723981705e-05\n",
      "epoch: 76, loss: 5.9665914138731046e-05\n",
      "epoch: 77, loss: 6.0928934115940045e-05\n",
      "epoch: 78, loss: 6.648395298698865e-05\n",
      "epoch: 79, loss: 5.223049687022139e-05\n",
      "epoch: 80, loss: 5.903515242623516e-05\n",
      "epoch: 81, loss: 4.732162593837404e-05\n",
      "epoch: 82, loss: 4.8748603840963005e-05\n",
      "epoch: 83, loss: 3.0513658408903127e-05\n",
      "epoch: 84, loss: 4.5703786978242236e-05\n",
      "epoch: 85, loss: 4.072867396825186e-05\n",
      "epoch: 86, loss: 3.3154674543462375e-05\n",
      "epoch: 87, loss: 3.1442349433943384e-05\n",
      "epoch: 88, loss: 2.935015370059304e-05\n",
      "epoch: 89, loss: 3.3000536941868384e-05\n",
      "epoch: 90, loss: 2.9535816824749513e-05\n",
      "epoch: 91, loss: 3.0909064117571256e-05\n",
      "epoch: 92, loss: 3.89958220330097e-05\n",
      "epoch: 93, loss: 3.344497425956292e-05\n",
      "epoch: 94, loss: 2.576596693655257e-05\n",
      "epoch: 95, loss: 2.4638623812204228e-05\n",
      "epoch: 96, loss: 2.2944658117340493e-05\n",
      "epoch: 97, loss: 2.4276025226341946e-05\n",
      "epoch: 98, loss: 2.4126106965502718e-05\n",
      "epoch: 99, loss: 2.2343796591837228e-05\n",
      "2.6172077934037214e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376d359713204321988760bfd6c0147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.11344701796159253\n",
      "epoch: 1, loss: 0.0863923978159875\n",
      "epoch: 2, loss: 0.07952905881911874\n",
      "epoch: 3, loss: 0.08330395195324275\n",
      "epoch: 4, loss: 0.08394665143664824\n",
      "epoch: 5, loss: 0.08123893846354786\n",
      "epoch: 6, loss: 0.07794589399808954\n",
      "epoch: 7, loss: 0.07618652998728631\n",
      "epoch: 8, loss: 0.07555860338884635\n",
      "epoch: 9, loss: 0.07502128457128006\n",
      "epoch: 10, loss: 0.07449216493549374\n",
      "epoch: 11, loss: 0.07318347141727753\n",
      "epoch: 12, loss: 0.07055398929857526\n",
      "epoch: 13, loss: 0.06697347086920302\n",
      "epoch: 14, loss: 0.0628996388139163\n",
      "epoch: 15, loss: 0.05747285422390517\n",
      "epoch: 16, loss: 0.052505706557399764\n",
      "epoch: 17, loss: 0.04775430350920959\n",
      "epoch: 18, loss: 0.043935455333319974\n",
      "epoch: 19, loss: 0.04018559838807347\n",
      "epoch: 20, loss: 0.03794710587560213\n",
      "epoch: 21, loss: 0.03639712422733928\n",
      "epoch: 22, loss: 0.03519494448439254\n",
      "epoch: 23, loss: 0.03431843731098601\n",
      "epoch: 24, loss: 0.03259211067529615\n",
      "epoch: 25, loss: 0.03127294788897938\n",
      "epoch: 26, loss: 0.029453237200929863\n",
      "epoch: 27, loss: 0.027896021455394077\n",
      "epoch: 28, loss: 0.026119269973420196\n",
      "epoch: 29, loss: 0.025083668551990713\n",
      "epoch: 30, loss: 0.02360154042323145\n",
      "epoch: 31, loss: 0.022502866898523665\n",
      "epoch: 32, loss: 0.020449122411656067\n",
      "epoch: 33, loss: 0.018652207894245307\n",
      "epoch: 34, loss: 0.016287789909723946\n",
      "epoch: 35, loss: 0.013764016184743927\n",
      "epoch: 36, loss: 0.011526980713764613\n",
      "epoch: 37, loss: 0.009331327826546816\n",
      "epoch: 38, loss: 0.007183162236448538\n",
      "epoch: 39, loss: 0.005069942483084974\n",
      "epoch: 40, loss: 0.0035354186120770846\n",
      "epoch: 41, loss: 0.0022665440599501387\n",
      "epoch: 42, loss: 0.0016762566468051026\n",
      "epoch: 43, loss: 0.0012967671926519106\n",
      "epoch: 44, loss: 0.0010585948343105446\n",
      "epoch: 45, loss: 0.0008949980456370997\n",
      "epoch: 46, loss: 0.0006394267320502285\n",
      "epoch: 47, loss: 0.0004412720333105913\n",
      "epoch: 48, loss: 0.0004066477508273096\n",
      "epoch: 49, loss: 0.00034776357620237695\n",
      "epoch: 50, loss: 0.00032487103329618955\n",
      "epoch: 51, loss: 0.0002580206830105438\n",
      "epoch: 52, loss: 0.00020270964307789608\n",
      "epoch: 53, loss: 0.00019497003224831438\n",
      "epoch: 54, loss: 0.00019853721578221687\n",
      "epoch: 55, loss: 0.00017318541030561716\n",
      "epoch: 56, loss: 0.00013063408923209545\n",
      "epoch: 57, loss: 9.378233596103151e-05\n",
      "epoch: 58, loss: 9.749447194518051e-05\n",
      "epoch: 59, loss: 9.996025140640474e-05\n",
      "epoch: 60, loss: 9.505462578061865e-05\n",
      "epoch: 61, loss: 9.115887510341432e-05\n",
      "epoch: 62, loss: 9.822314282308323e-05\n",
      "epoch: 63, loss: 0.00010131671663651836\n",
      "epoch: 64, loss: 9.503420094767113e-05\n",
      "epoch: 65, loss: 0.00011480782016213643\n",
      "epoch: 66, loss: 0.000102862846483557\n",
      "epoch: 67, loss: 9.2597035585352e-05\n",
      "epoch: 68, loss: 0.00010403246124087692\n",
      "epoch: 69, loss: 0.00011075794962629689\n",
      "epoch: 70, loss: 0.00011307331979136612\n",
      "epoch: 71, loss: 0.0001045203241963502\n",
      "epoch: 72, loss: 0.00010028123992468881\n",
      "epoch: 73, loss: 9.882229627718467e-05\n",
      "epoch: 74, loss: 0.00010066566883754485\n",
      "epoch: 75, loss: 0.00010284832244874192\n",
      "epoch: 76, loss: 9.196971206431117e-05\n",
      "epoch: 77, loss: 0.00010194753239313174\n",
      "epoch: 78, loss: 9.94842502345719e-05\n",
      "epoch: 79, loss: 0.00010228603720948472\n",
      "epoch: 80, loss: 8.313699118853279e-05\n",
      "epoch: 81, loss: 8.185797383652616e-05\n",
      "epoch: 82, loss: 8.948855443757588e-05\n",
      "epoch: 83, loss: 8.34118373869268e-05\n",
      "epoch: 84, loss: 7.764967601330762e-05\n",
      "epoch: 85, loss: 6.697937388665835e-05\n",
      "epoch: 86, loss: 6.56669053509783e-05\n",
      "epoch: 87, loss: 6.54890357040956e-05\n",
      "epoch: 88, loss: 6.578290894514494e-05\n",
      "epoch: 89, loss: 6.018156884742746e-05\n",
      "epoch: 90, loss: 6.493534421385982e-05\n",
      "epoch: 91, loss: 6.143998897390804e-05\n",
      "epoch: 92, loss: 5.9322120833113764e-05\n",
      "epoch: 93, loss: 5.5452781395906954e-05\n",
      "epoch: 94, loss: 5.862224843443076e-05\n",
      "epoch: 95, loss: 5.168482987082115e-05\n",
      "epoch: 96, loss: 5.2881881179651515e-05\n",
      "epoch: 97, loss: 4.984965495719506e-05\n",
      "epoch: 98, loss: 4.912494720217021e-05\n",
      "epoch: 99, loss: 3.9536553028882436e-05\n",
      "4.6063054890275663e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f607d9fa94ef4d44994b40bb8746994b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.08104521434374792\n",
      "epoch: 1, loss: 0.07732858085605832\n",
      "epoch: 2, loss: 0.07709590248801812\n",
      "epoch: 3, loss: 0.07713595729831133\n",
      "epoch: 4, loss: 0.07476076051181696\n",
      "epoch: 5, loss: 0.07129898948740584\n",
      "epoch: 6, loss: 0.06614366890564606\n",
      "epoch: 7, loss: 0.05927787591422654\n",
      "epoch: 8, loss: 0.05233968008272103\n",
      "epoch: 9, loss: 0.04578590670518395\n",
      "epoch: 10, loss: 0.03942171695108212\n",
      "epoch: 11, loss: 0.03598147452727891\n",
      "epoch: 12, loss: 0.03304046582012215\n",
      "epoch: 13, loss: 0.031058284588702655\n",
      "epoch: 14, loss: 0.028589367036996792\n",
      "epoch: 15, loss: 0.025543777761971878\n",
      "epoch: 16, loss: 0.022404205109852412\n",
      "epoch: 17, loss: 0.01896829938283499\n",
      "epoch: 18, loss: 0.016321556745351944\n",
      "epoch: 19, loss: 0.013813359302242936\n",
      "epoch: 20, loss: 0.011150267967654934\n",
      "epoch: 21, loss: 0.008229337274623834\n",
      "epoch: 22, loss: 0.00534671880328618\n",
      "epoch: 23, loss: 0.0031531708272423097\n",
      "epoch: 24, loss: 0.001622586279690551\n",
      "epoch: 25, loss: 0.0008054199031727401\n",
      "epoch: 26, loss: 0.0004897634549517145\n",
      "epoch: 27, loss: 0.0003258658543855651\n",
      "epoch: 28, loss: 0.00028491557518886616\n",
      "epoch: 29, loss: 0.000280162913840096\n",
      "epoch: 30, loss: 0.00025430672916080995\n",
      "epoch: 31, loss: 0.00016984387338158874\n",
      "epoch: 32, loss: 0.0001596539210072892\n",
      "epoch: 33, loss: 0.0002856768315921469\n",
      "epoch: 34, loss: 0.000390310994474198\n",
      "epoch: 35, loss: 0.0005292938306417233\n",
      "epoch: 36, loss: 0.0006275969998991745\n",
      "epoch: 37, loss: 0.0007482266172434941\n",
      "epoch: 38, loss: 0.0007492015364198719\n",
      "epoch: 39, loss: 0.0007171749922201811\n",
      "epoch: 40, loss: 0.0007394206800254957\n",
      "epoch: 41, loss: 0.0007489977972789966\n",
      "epoch: 42, loss: 0.0007673738371210745\n",
      "epoch: 43, loss: 0.000738804396874569\n",
      "epoch: 44, loss: 0.0006637033262745606\n",
      "epoch: 45, loss: 0.0005861990560222823\n",
      "epoch: 46, loss: 0.0005437151459999931\n",
      "epoch: 47, loss: 0.00048005245784178696\n",
      "epoch: 48, loss: 0.0004307649993050495\n",
      "epoch: 49, loss: 0.0003700135952344947\n",
      "epoch: 50, loss: 0.0002877786301015886\n",
      "epoch: 51, loss: 0.00022370384730516782\n",
      "epoch: 52, loss: 0.00018659288873712598\n",
      "epoch: 53, loss: 0.00017825574763308876\n",
      "epoch: 54, loss: 0.00012502379491157703\n",
      "epoch: 55, loss: 8.086520073255468e-05\n",
      "epoch: 56, loss: 6.347408646347104e-05\n",
      "epoch: 57, loss: 4.784606970630264e-05\n",
      "epoch: 58, loss: 5.956887038178265e-05\n",
      "epoch: 59, loss: 6.683064274960298e-05\n",
      "epoch: 60, loss: 5.884338314279514e-05\n",
      "epoch: 61, loss: 5.398894255376174e-05\n",
      "epoch: 62, loss: 5.376618259230823e-05\n",
      "epoch: 63, loss: 7.858315471082499e-05\n",
      "epoch: 64, loss: 5.674906931355678e-05\n",
      "epoch: 65, loss: 7.1914717513258e-05\n",
      "epoch: 66, loss: 4.460659677250546e-05\n",
      "epoch: 67, loss: 6.353142754786862e-05\n",
      "epoch: 68, loss: 5.4094251029127444e-05\n",
      "epoch: 69, loss: 5.57193924069708e-05\n",
      "epoch: 70, loss: 4.887322328816108e-05\n",
      "epoch: 71, loss: 4.857773969007134e-05\n",
      "epoch: 72, loss: 4.618485813120602e-05\n",
      "epoch: 73, loss: 4.3294410199481766e-05\n",
      "epoch: 74, loss: 3.5659321915364007e-05\n",
      "epoch: 75, loss: 3.302517569289254e-05\n",
      "epoch: 76, loss: 3.9486278407890926e-05\n",
      "epoch: 77, loss: 3.231121980118605e-05\n",
      "epoch: 78, loss: 2.656944648193394e-05\n",
      "epoch: 79, loss: 1.9572327313595886e-05\n",
      "epoch: 80, loss: 2.803673062651987e-05\n",
      "epoch: 81, loss: 2.533115610341296e-05\n",
      "epoch: 82, loss: 2.88454114664958e-05\n",
      "epoch: 83, loss: 2.3368839839368064e-05\n",
      "epoch: 84, loss: 1.8745557130704134e-05\n",
      "epoch: 85, loss: 2.1297395470749972e-05\n",
      "epoch: 86, loss: 2.8085939544885468e-05\n",
      "epoch: 87, loss: 1.7774125943502307e-05\n",
      "epoch: 88, loss: 2.054955362950592e-05\n",
      "epoch: 89, loss: 2.9581406696390558e-05\n",
      "epoch: 90, loss: 2.3271714105391818e-05\n",
      "epoch: 91, loss: 2.8179066294300235e-05\n",
      "epoch: 92, loss: 2.0781180726912053e-05\n",
      "epoch: 93, loss: 2.3921590936673344e-05\n",
      "epoch: 94, loss: 2.008449716980385e-05\n",
      "epoch: 95, loss: 1.4902339095892801e-05\n",
      "epoch: 96, loss: 2.4235883816114483e-05\n",
      "epoch: 97, loss: 2.5588054121990048e-05\n",
      "epoch: 98, loss: 2.868030136217651e-05\n",
      "epoch: 99, loss: 2.2497820402506856e-05\n",
      "1.8882440664893647e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2857aa01ce347f59249a9afb9ac0bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07789288173795753\n",
      "epoch: 1, loss: 0.07673277115450076\n",
      "epoch: 2, loss: 0.07498976180059654\n",
      "epoch: 3, loss: 0.0728283947635234\n",
      "epoch: 4, loss: 0.06864075559937718\n",
      "epoch: 5, loss: 0.06302646233489945\n",
      "epoch: 6, loss: 0.05633065736478299\n",
      "epoch: 7, loss: 0.048273757561093686\n",
      "epoch: 8, loss: 0.04051103485465152\n",
      "epoch: 9, loss: 0.03356496760105314\n",
      "epoch: 10, loss: 0.027904870944886313\n",
      "epoch: 11, loss: 0.024701295867954255\n",
      "epoch: 12, loss: 0.02384120154825521\n",
      "epoch: 13, loss: 0.023473062264353035\n",
      "epoch: 14, loss: 0.02236218030694634\n",
      "epoch: 15, loss: 0.019965812742043693\n",
      "epoch: 16, loss: 0.016875579266324395\n",
      "epoch: 17, loss: 0.013226895692255774\n",
      "epoch: 18, loss: 0.009686880278608928\n",
      "epoch: 19, loss: 0.006448906212703608\n",
      "epoch: 20, loss: 0.003800091793250405\n",
      "epoch: 21, loss: 0.0022661676283666694\n",
      "epoch: 22, loss: 0.0019600883451077107\n",
      "epoch: 23, loss: 0.0018843434969950026\n",
      "epoch: 24, loss: 0.0016376793179338532\n",
      "epoch: 25, loss: 0.0013470035309464837\n",
      "epoch: 26, loss: 0.0010012688342574433\n",
      "epoch: 27, loss: 0.0007639800521031224\n",
      "epoch: 28, loss: 0.0005187371924321656\n",
      "epoch: 29, loss: 0.0003718175165657422\n",
      "epoch: 30, loss: 0.00042413455179936727\n",
      "epoch: 31, loss: 0.00045197266171882354\n",
      "epoch: 32, loss: 0.00048086540262275206\n",
      "epoch: 33, loss: 0.0004132746074439463\n",
      "epoch: 34, loss: 0.00034303397987555295\n",
      "epoch: 35, loss: 0.00030697476232907444\n",
      "epoch: 36, loss: 0.0003485771818258438\n",
      "epoch: 37, loss: 0.0003832126277639958\n",
      "epoch: 38, loss: 0.0004247388050250527\n",
      "epoch: 39, loss: 0.0004850605278028631\n",
      "epoch: 40, loss: 0.0004423072204963476\n",
      "epoch: 41, loss: 0.0003875753637633011\n",
      "epoch: 42, loss: 0.00032845892959588687\n",
      "epoch: 43, loss: 0.00028349319634572186\n",
      "epoch: 44, loss: 0.00022278957046837223\n",
      "epoch: 45, loss: 0.0001929177213586107\n",
      "epoch: 46, loss: 0.0001992278253848919\n",
      "epoch: 47, loss: 0.00019968797729540092\n",
      "epoch: 48, loss: 0.00020801762494915168\n",
      "epoch: 49, loss: 0.00017626181922812459\n",
      "epoch: 50, loss: 0.00012821530363817913\n",
      "epoch: 51, loss: 8.511555035119096e-05\n",
      "epoch: 52, loss: 5.860302803735974e-05\n",
      "epoch: 53, loss: 5.1627530448440156e-05\n",
      "epoch: 54, loss: 6.387257538179101e-05\n",
      "epoch: 55, loss: 8.181842632299765e-05\n",
      "epoch: 56, loss: 7.98961789119859e-05\n",
      "epoch: 57, loss: 5.6067142034388574e-05\n",
      "epoch: 58, loss: 4.31837873499162e-05\n",
      "epoch: 59, loss: 3.0983397296137935e-05\n",
      "epoch: 60, loss: 2.862778326366094e-05\n",
      "epoch: 61, loss: 3.599596129670085e-05\n",
      "epoch: 62, loss: 3.944852869876907e-05\n",
      "epoch: 63, loss: 4.190652164765343e-05\n",
      "epoch: 64, loss: 3.9587249343125146e-05\n",
      "epoch: 65, loss: 4.0944251679551745e-05\n",
      "epoch: 66, loss: 3.409113598535321e-05\n",
      "epoch: 67, loss: 3.055949022551745e-05\n",
      "epoch: 68, loss: 2.5740254702070203e-05\n",
      "epoch: 69, loss: 2.411101561557487e-05\n",
      "epoch: 70, loss: 2.3393823410292218e-05\n",
      "epoch: 71, loss: 2.6933193866055558e-05\n",
      "epoch: 72, loss: 2.897418077165726e-05\n",
      "epoch: 73, loss: 2.5141959986838915e-05\n",
      "epoch: 74, loss: 1.8710328762961596e-05\n",
      "epoch: 75, loss: 2.592304274997581e-05\n",
      "epoch: 76, loss: 1.8581289368316354e-05\n",
      "epoch: 77, loss: 1.729208143941886e-05\n",
      "epoch: 78, loss: 2.274737607181764e-05\n",
      "epoch: 79, loss: 2.4659507083649937e-05\n",
      "epoch: 80, loss: 2.6040976245530258e-05\n",
      "epoch: 81, loss: 2.0065661003125223e-05\n",
      "epoch: 82, loss: 1.966362093876946e-05\n",
      "epoch: 83, loss: 1.982235908400199e-05\n",
      "epoch: 84, loss: 2.8985948138380773e-05\n",
      "epoch: 85, loss: 1.9543357053439565e-05\n",
      "epoch: 86, loss: 2.4468313570421385e-05\n",
      "epoch: 87, loss: 2.038597413012649e-05\n",
      "epoch: 88, loss: 2.3237907874347296e-05\n",
      "epoch: 89, loss: 2.5113503128413943e-05\n",
      "epoch: 90, loss: 2.2102042361061442e-05\n",
      "epoch: 91, loss: 2.1448885401577266e-05\n",
      "epoch: 92, loss: 1.4945056641349258e-05\n",
      "epoch: 93, loss: 2.363098966278519e-05\n",
      "epoch: 94, loss: 1.1791131095575038e-05\n",
      "epoch: 95, loss: 2.17498786321161e-05\n",
      "epoch: 96, loss: 1.988642957444154e-05\n",
      "epoch: 97, loss: 2.1512107456558693e-05\n",
      "epoch: 98, loss: 2.0834977925476848e-05\n",
      "epoch: 99, loss: 1.9288335724245864e-05\n",
      "2.0675985567252665e-05\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model = RegularizedModel(n_features=n_features, \n",
    "                             n_targets=1, \n",
    "                             reps=2,\n",
    "                             alpha=0.001,\n",
    "                             backend=backend, \n",
    "                             shots=10000, \n",
    "                             optimizer=optimizer)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs, verbose=True) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_regularisation_model_low_penalty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(10)):\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model = RegularizedModel(n_features=n_features, \n",
    "                             n_targets=1, \n",
    "                             reps=2,\n",
    "                             alpha=0.01,\n",
    "                             backend=backend, \n",
    "                             shots=10000, \n",
    "                             optimizer=optimizer)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_regularisation_model_high_penalty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad3e93d82254aa0957b0989bbecf60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf1e3f1a5445c9a27a8b18a9f85f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.09559016166388344\n",
      "epoch: 1, loss: 0.07763653804795972\n",
      "epoch: 2, loss: 0.06629940140997256\n",
      "epoch: 3, loss: 0.06203183274140439\n",
      "epoch: 4, loss: 0.05990754040036295\n",
      "epoch: 5, loss: 0.05719841942748674\n",
      "epoch: 6, loss: 0.05407732645957505\n",
      "epoch: 7, loss: 0.05062937609841777\n",
      "epoch: 8, loss: 0.048127324652284205\n",
      "epoch: 9, loss: 0.046095605139456985\n",
      "epoch: 10, loss: 0.0445095401012463\n",
      "epoch: 11, loss: 0.042749125092228724\n",
      "epoch: 12, loss: 0.04112803683534045\n",
      "epoch: 13, loss: 0.03852382583364874\n",
      "epoch: 14, loss: 0.03667331181233971\n",
      "epoch: 15, loss: 0.03580967507223566\n",
      "epoch: 16, loss: 0.03525546279810853\n",
      "epoch: 17, loss: 0.034896913366402\n",
      "epoch: 18, loss: 0.034577043636941106\n",
      "epoch: 19, loss: 0.034754696712709955\n",
      "epoch: 20, loss: 0.03469440292913434\n",
      "epoch: 21, loss: 0.03428808717270867\n",
      "epoch: 22, loss: 0.03338297982681935\n",
      "epoch: 23, loss: 0.032508781918724756\n",
      "epoch: 24, loss: 0.03171774864643913\n",
      "epoch: 25, loss: 0.031123518023918374\n",
      "epoch: 26, loss: 0.030497675689803688\n",
      "epoch: 27, loss: 0.029924409767834507\n",
      "epoch: 28, loss: 0.02892505034680657\n",
      "epoch: 29, loss: 0.028754514589277705\n",
      "epoch: 30, loss: 0.027810233637303266\n",
      "epoch: 31, loss: 0.02715397479620117\n",
      "epoch: 32, loss: 0.026576546729326352\n",
      "epoch: 33, loss: 0.025488800646499152\n",
      "epoch: 34, loss: 0.024595733047276733\n",
      "epoch: 35, loss: 0.02339129427643341\n",
      "epoch: 36, loss: 0.02280297469512338\n",
      "epoch: 37, loss: 0.02228435986523766\n",
      "epoch: 38, loss: 0.02184168072484341\n",
      "epoch: 39, loss: 0.0216941840545636\n",
      "epoch: 40, loss: 0.021353733803649112\n",
      "epoch: 41, loss: 0.021160419209913562\n",
      "epoch: 42, loss: 0.020897091392849863\n",
      "epoch: 43, loss: 0.020311635366359968\n",
      "epoch: 44, loss: 0.020033595879354903\n",
      "epoch: 45, loss: 0.019816898762272654\n",
      "epoch: 46, loss: 0.01904258212722862\n",
      "epoch: 47, loss: 0.019087401681697865\n",
      "epoch: 48, loss: 0.018925637769211794\n",
      "epoch: 49, loss: 0.018925246811315525\n",
      "epoch: 50, loss: 0.018953388095083002\n",
      "epoch: 51, loss: 0.018599109685453263\n",
      "epoch: 52, loss: 0.018669898481030286\n",
      "epoch: 53, loss: 0.01887437870821302\n",
      "epoch: 54, loss: 0.018771787565643083\n",
      "epoch: 55, loss: 0.01901990496823512\n",
      "epoch: 56, loss: 0.01859805955188637\n",
      "epoch: 57, loss: 0.018708607261792097\n",
      "epoch: 58, loss: 0.018776934286986794\n",
      "epoch: 59, loss: 0.01889397223691539\n",
      "epoch: 60, loss: 0.018600646005309803\n",
      "epoch: 61, loss: 0.01882033552857558\n",
      "epoch: 62, loss: 0.01857636191464297\n",
      "epoch: 63, loss: 0.018692909098271402\n",
      "epoch: 64, loss: 0.018395080327110815\n",
      "epoch: 65, loss: 0.018532621415564013\n",
      "epoch: 66, loss: 0.0183286141352655\n",
      "epoch: 67, loss: 0.018548922957004454\n",
      "epoch: 68, loss: 0.01836714760960866\n",
      "epoch: 69, loss: 0.018399497488864273\n",
      "epoch: 70, loss: 0.018427011522356512\n",
      "epoch: 71, loss: 0.018452798329973524\n",
      "epoch: 72, loss: 0.01835617334049949\n",
      "epoch: 73, loss: 0.018406624423233105\n",
      "epoch: 74, loss: 0.018434850504281248\n",
      "epoch: 75, loss: 0.01853546695618873\n",
      "epoch: 76, loss: 0.018151620735412297\n",
      "epoch: 77, loss: 0.018588080973299274\n",
      "epoch: 78, loss: 0.018114515767304672\n",
      "epoch: 79, loss: 0.018573955192677735\n",
      "epoch: 80, loss: 0.018378423955061677\n",
      "epoch: 81, loss: 0.0183264320785306\n",
      "epoch: 82, loss: 0.018284529084983753\n",
      "epoch: 83, loss: 0.018243518520526\n",
      "epoch: 84, loss: 0.018241453810646808\n",
      "epoch: 85, loss: 0.018183845072971782\n",
      "epoch: 86, loss: 0.01817256528953233\n",
      "epoch: 87, loss: 0.018332558887008715\n",
      "epoch: 88, loss: 0.018368132492322484\n",
      "epoch: 89, loss: 0.018505598520540224\n",
      "epoch: 90, loss: 0.018222487649998467\n",
      "epoch: 91, loss: 0.018278173038264768\n",
      "epoch: 92, loss: 0.01832435364577337\n",
      "epoch: 93, loss: 0.018196872355359998\n",
      "epoch: 94, loss: 0.018469872659750145\n",
      "epoch: 95, loss: 0.018182152928315133\n",
      "epoch: 96, loss: 0.01842627662426255\n",
      "epoch: 97, loss: 0.018373519369342926\n",
      "epoch: 98, loss: 0.018132178830666618\n",
      "epoch: 99, loss: 0.018348682878884467\n",
      "0.01849067201651418\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model = RegularizedModel(n_features=n_features, \n",
    "                             n_targets=1, \n",
    "                             reps=3,\n",
    "                             alpha=0,\n",
    "                             train_map=False,\n",
    "                             backend=backend, \n",
    "                             shots=10000, \n",
    "                             optimizer=optimizer)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs, verbose=True) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_no_train_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3629ba9b167243f3ba8632db78b4d392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8683686f7e480a9398084c4caf5c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.07328912082325022\n",
      "epoch: 1, loss: 0.05286396906682561\n",
      "epoch: 2, loss: 0.03411853532170011\n",
      "epoch: 3, loss: 0.02280312044248059\n",
      "epoch: 4, loss: 0.017530109288133054\n",
      "epoch: 5, loss: 0.015368532687874713\n",
      "epoch: 6, loss: 0.015404735474853735\n",
      "epoch: 7, loss: 0.016538601820326862\n",
      "epoch: 8, loss: 0.016922415853440915\n",
      "epoch: 9, loss: 0.0163536833966227\n",
      "epoch: 10, loss: 0.014544288212475246\n",
      "epoch: 11, loss: 0.013030017421520967\n",
      "epoch: 12, loss: 0.011730829593906745\n",
      "epoch: 13, loss: 0.011196526561296742\n",
      "epoch: 14, loss: 0.010938221404598296\n",
      "epoch: 15, loss: 0.010495193927367086\n",
      "epoch: 16, loss: 0.01011845079242857\n",
      "epoch: 17, loss: 0.009867746021174273\n",
      "epoch: 18, loss: 0.00935788424658885\n",
      "epoch: 19, loss: 0.00896251510219976\n",
      "epoch: 20, loss: 0.008746769033286665\n",
      "epoch: 21, loss: 0.008289528519211437\n",
      "epoch: 22, loss: 0.007728695223402655\n",
      "epoch: 23, loss: 0.007089212633676623\n",
      "epoch: 24, loss: 0.006670371564314399\n",
      "epoch: 25, loss: 0.005992601817855877\n",
      "epoch: 26, loss: 0.005481413071249277\n",
      "epoch: 27, loss: 0.005357270399127383\n",
      "epoch: 28, loss: 0.004958400065802826\n",
      "epoch: 29, loss: 0.0049713235308395266\n",
      "epoch: 30, loss: 0.004779967290571524\n",
      "epoch: 31, loss: 0.004804923241288117\n",
      "epoch: 32, loss: 0.004740563288421098\n",
      "epoch: 33, loss: 0.004709152145497361\n",
      "epoch: 34, loss: 0.004678772750754294\n",
      "epoch: 35, loss: 0.004417456687331037\n",
      "epoch: 36, loss: 0.004367022306829498\n",
      "epoch: 37, loss: 0.004364553059091743\n",
      "epoch: 38, loss: 0.0042564906988754245\n",
      "epoch: 39, loss: 0.004119289352115904\n",
      "epoch: 40, loss: 0.0040512097058864995\n",
      "epoch: 41, loss: 0.0039053795932365142\n",
      "epoch: 42, loss: 0.003911205559109514\n",
      "epoch: 43, loss: 0.003957307125294679\n",
      "epoch: 44, loss: 0.003820172785502478\n",
      "epoch: 45, loss: 0.0038278142220767924\n",
      "epoch: 46, loss: 0.0038205784335223186\n",
      "epoch: 47, loss: 0.00391323571993966\n",
      "epoch: 48, loss: 0.003723210847674967\n",
      "epoch: 49, loss: 0.0037896821362927153\n",
      "epoch: 50, loss: 0.003942021223912723\n",
      "epoch: 51, loss: 0.0039034157746728965\n",
      "epoch: 52, loss: 0.003843147289423709\n",
      "epoch: 53, loss: 0.0038698455458323956\n",
      "epoch: 54, loss: 0.0037927292965600754\n",
      "epoch: 55, loss: 0.00369259788221379\n",
      "epoch: 56, loss: 0.0037480741567401093\n",
      "epoch: 57, loss: 0.0037733051653917136\n",
      "epoch: 58, loss: 0.003870434682107232\n",
      "epoch: 59, loss: 0.0037431794790008144\n",
      "epoch: 60, loss: 0.0038459125111793423\n",
      "epoch: 61, loss: 0.0038415873248959483\n",
      "epoch: 62, loss: 0.0037410184892569293\n",
      "epoch: 63, loss: 0.0037617016180123935\n",
      "epoch: 64, loss: 0.00381087034116087\n",
      "epoch: 65, loss: 0.003674634940930869\n",
      "epoch: 66, loss: 0.0037802728705826068\n",
      "epoch: 67, loss: 0.0037326697022454377\n",
      "epoch: 68, loss: 0.0038074016241314945\n",
      "epoch: 69, loss: 0.003801207744195167\n",
      "epoch: 70, loss: 0.003743516586916184\n",
      "epoch: 71, loss: 0.0038055084053055526\n",
      "epoch: 72, loss: 0.003715662125319838\n",
      "epoch: 73, loss: 0.0037376760194503507\n",
      "epoch: 74, loss: 0.0038181263513766602\n",
      "epoch: 75, loss: 0.0038062532774185776\n",
      "epoch: 76, loss: 0.0037776995710919687\n",
      "epoch: 77, loss: 0.0037149166784673105\n",
      "epoch: 78, loss: 0.00378556965419319\n",
      "epoch: 79, loss: 0.0037460384956586566\n",
      "epoch: 80, loss: 0.0037879931938624685\n",
      "epoch: 81, loss: 0.003693101162318247\n",
      "epoch: 82, loss: 0.0037623655809150146\n",
      "epoch: 83, loss: 0.0037560686840535664\n",
      "epoch: 84, loss: 0.0037482300896612393\n",
      "epoch: 85, loss: 0.003762645521179511\n",
      "epoch: 86, loss: 0.003715786512522933\n",
      "epoch: 87, loss: 0.003733734568198794\n",
      "epoch: 88, loss: 0.0037356747495454184\n",
      "epoch: 89, loss: 0.003726783067811129\n",
      "epoch: 90, loss: 0.003668373622690812\n",
      "epoch: 91, loss: 0.0037562082454267853\n",
      "epoch: 92, loss: 0.0037304936868107668\n",
      "epoch: 93, loss: 0.0036543785115001324\n",
      "epoch: 94, loss: 0.0035840834799090882\n",
      "epoch: 95, loss: 0.0036690150788789553\n",
      "epoch: 96, loss: 0.0037131896062523395\n",
      "epoch: 97, loss: 0.00357023209563773\n",
      "epoch: 98, loss: 0.003658274765485572\n",
      "epoch: 99, loss: 0.0037887530795541304\n",
      "0.0036846426414865875\n"
     ]
    }
   ],
   "source": [
    "x_train[:,0] = np.pi/2\n",
    "x_train[:,1] = np.pi/2\n",
    "x_train[:,3] = np.pi/2\n",
    "\n",
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model = RegularizedModel(n_features=n_features, \n",
    "                             n_targets=1, \n",
    "                             reps=3,\n",
    "                             alpha=0,\n",
    "                             train_map=False,\n",
    "                             backend=backend, \n",
    "                             shots=10000, \n",
    "                             optimizer=optimizer)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs, verbose=True) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_no_train_model_dense\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Single Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    model = sequential_qnn(q_bits = [n_features+1],\n",
    "                     dim = [n_features, 1],\n",
    "                     reps = 3,\n",
    "                     backend=backend,\n",
    "                     shots=10000,\n",
    "                     lr = 0.1)\n",
    "    \n",
    "    model.train(x_train, y_train, epochs=epochs, verbose=True) \n",
    "    model_list.append(model)\n",
    "    print(model.loss[-1])\n",
    "\n",
    "saver(model_list, data_path(\"sparse_standard_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
