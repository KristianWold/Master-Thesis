%================================================================
%------------------------- Abstract -----------------------------
%================================================================
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\thispagestyle{plain}

In this thesis we have made a python-based framework for machine learning, capable of implementing dense neural networks (DNNs), quantum neural networks (QNNs), and hybrids of the two. The framework can also implement a sequential architecture consisting of several quantum neural networks, which we call a quantum circuit network (QCNs). To optimize the models, we have implemented gradient-based methods using their analytical gradient. For quantum neural networks, we utilize the parameter shift rule to calculate the gradient analytically on quantum hardware. For quantum circuit networks, we have developed a backpropagation algorithm based on the parameters shift rule. We perform a numerical study where we seek to characterize how the DNNs, QNNs and QCNs  behave as a function of architecture, focusing on quantifying expressiveness of the models and the magnitude of its gradient. We also perform a thorough benchmarking of the models, training them on mixed Gaussian data in one, two and three dimensions, as well the real-world data sets Boston Housing data and Breast Cancer Wisconsin data.
We found that the magnitude of the gradient of our implementation of QNNs vanish exponentially in the number of qubits, potentially causing slow training due to bad signal-to-noise ratio when estimating the gradient. For QCNs, we found that the magnitude of partial gradients estimated on quantum hardware stayed constant when increasing the number of layers of the model. 