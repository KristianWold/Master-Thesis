%================================================================
\chapter{Conclusion \& Future Research}\label{chap:Conclusion}
%================================================================

%----------------------------------------------------------------
\section{Conclusion}\label{sec:conclusion}
%----------------------------------------------------------------
In this thesis, we have made a python-based framework capable of implementing and training dense neural networks (DNNs), quantum neural networks (QNNs), and quantum circuit networks (QCNs) on data. The models are optimized using gradient-based methods, using Adam optimizer. For the QCN, we have developed a backprogagation algorithm based on the parameter shift rule for calculating its gradient analytically.

Quantum neural networks are parameterized, single-circuit quantum algorithms used for learning from data. The ones implemented in this thesis was inspired by the ones proposed by \citet{abbas2020power}. We showed in \cref{sec:Vanishing Gradient for QNNs} that increasing the circuit depth and number of qubits of QNNs caused their gradients to vanish due to the barren plateau phenomenon \cite{McClean_2018}. As their gradients are estimated on quantum hardware, this causes an exponential overhead on the quantum computer in order to estimate the gradient accurately. This suggests that training on large data sets is intractable, especially on noisy quantum hardware. Unlike the QNN, the QCN is able to achieve a larger model size by adding several smaller parameterized circuits, rather than using larger ones. In \autoref{sec:Vanishing Local Gradients in QCNs} we showed that the magnitude of the local gradients (the partial gradient of each circuit) of the QCN were unaffected by increasing the number of layers, but tended to vanish for increased number of qubits in each circuit. This enables the possibility of avoiding significant overhead in the estimation on the quantum hardware for big models by using QCNs with several smaller circuits. However, in \autoref{sec:Vanishing Total Gradients in QCNs}, we showed that backpropagating the local gradients still produced a gradient that vanished exponentially fast in the number of layers, similar to the behaviour of DNNs. This effect also strengthened when increasing the number of qubits. Still, for fewer than eight qubits, QCNs were shown to have a larger gradient than similarly sized DNNs, suggesting that they should be faster to train.

In \autoref{sec:Untrained Models}, we showed that untrained QCNs of sufficiently many qubits and similarly sized DNNs exhibited an exponentially decaying trajectory length. For the QCNs, it was concluded that this was a result of the fact that untrained parameterized circuits approximate random circuits, which tend to produce outputs that concentrate around some mean \cite{McClean_2018}. However, we showed in \autoref{sec:Trained Models} that the expressivity of QCNs and DNNs could be increased into the exponential regime through training, producing exponentially more expressive models for each layer. In addition, the trajectory length increased faster for the QCNs compared to the DNNs, suggesting that they are more expressive and can fit more complicated functions.

In \autoref{sec:Ideal Simulation}, we show that QCNs using four and five qubits and shallow circuits minimize their MSE faster than similarly DNNs when fit to Gaussian data, as was suggested by the analysis of the gradient in \autoref{sec:Vanishing Gradient Phenomenon}. Also, when the DNNs where trained for 10000 epochs (until saturation), they obtained a MSE lower than the QCNs, but still within the same order of magnitude. As the QCNs were trained for only 100 epochs, this might suggest that they could outperform the DNNs given enough training. While speculative, this indicates that QCNs are more expressive then DNNs and thus can fit more complicated data, as predicted by the analysis of its trajectory length. On the other hand, the QNNs implemented in this thesis was unable to fit the Gaussian data, suggesting that the RZZ encoding of the data or the repetitions of the simple anstaz (or both) are unfit for producing a QNN that can successfully train on mixed Gaussian data. 

In \autoref{sec:Noisy Simulation}, we showed that QCNs still outperformed DNNs when trained using a noisy simulation of the Santiago quantum computer. This was likely due to the low circuit depth of each circuit in the QCNs. Not surprisingly, the QNN performed even worse due to their high circuit depth.   



%----------------------------------------------------------------
\section{Future Research}\label{sec:future}
%----------------------------------------------------------------
For future research, we suggest experimenting with different ansatzes beyond the simple ansatz \autoref{eq:simple ansatz} for constructing QCNs. Especially, \citet{Cerezo_2021} recently showed that there exists parameterized circuits with circuit depth logarithmic in the number of qubits that does not suffer from an exponential vanishing gradient. While such shallow circuits likely are unfit as QNNs due to their low complexity, they could be useful as nodes in an QCN as one could utilize multiple circuits to build a more expressive model. This can potentially alleviate the vanishing gradient problem to a large degree.

To establish a better comparison between QCNs and QNNs, we suggest exploring different QNN architectures that are able to fit nonlinear data. Recently, \citet{Schuld_2021} showed that QNNs can be universal function approximators if multiple repetitions of alternating feature encoding and ansatzes are applied. This means that a sufficiently deep QNN could approximate any function to sufficient accuracy. It would be interesting to see whether QCNs could outperform such a QNN when trained on noisy quantum hardware, since the QNN would likely have higher circuit depth.

Due to limited limited computational resources, the training of QNN and QCN models were limited to a small number of qubits. This hindered 


%----------------------------------------------------------------
%\section{Todo}\label{sec:todo}
%----------------------------------------------------------------

%\begin{itemize}
%    \item Noise simulation only an approximation
%    \item Classical activation saturation(tanh, xavier paper). Plateau, %akin to concentration of PQC output around mean.  
%    \item More details on vanishing PQC gradient
%    \item Calculate complexity of QCN
%    \item batch norm for QCN
%    \item fewer parameters, relieving bottleneck for extremly large %models. 
%    \item is QCN scalable for many qubits? With log n depth and local %observable? QCN perfect usecase!
%    \item is QCN with few qubits smart? Scales no better than linearly. %We already have very powerful classical computers. 
%    \item more details FIM
%    \item Elaborate on other ways of overcoming vanishing %gradient(layerwise learning)
%    \item Santiago quantum computer
%    \item Write about transpiler
%    \item Source of non-linearity, measurement (Shuld vanilla QNN). %Also, imprivitive 2-qubit gate, entanglement, important for ansatz. %Need to motive that QML can be used on NISQ! No amplification of defect in a single quantum circuit. Property of PQC: learn to compensate for hardware specific faults. 
%\end{itemize}