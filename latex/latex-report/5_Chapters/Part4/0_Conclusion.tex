%================================================================
\chapter{Summary \& Conclusions}\label{chap:Conclusion}
%================================================================

%----------------------------------------------------------------
\section{Summary}\label{sec:Summary}
%----------------------------------------------------------------
In this thesis, we have made a python-based framework capable of implementing and training dense neural networks (DNNs), quantum neural networks (QNNs), and quantum circuit networks (QCNs) on data. The models are optimized using gradient-based methods, using Adam optimizer. For the QCN, we have developed a backprogagation algorithm based on the parameter shift rule for calculating its gradient analytically.

Quantum neural networks are parameterized, single-circuit quantum algorithms used for learning from data. The ones implemented in this thesis was inspired by the ones proposed by \citet{abbas2020power}. We showed in \cref{sec:Vanishing Gradient for QNNs} that increasing the circuit depth and number of qubits of QNNs caused their gradients to vanish due to the barren plateau phenomenon \cite{McClean_2018}. As their gradients are estimated on quantum hardware, this causes an exponential overhead on the quantum computer in order to estimate the gradient accurately. This suggests that training on large data sets is intractable, especially on noisy quantum hardware. Unlike the QNN, the QCN is able to achieve a larger model size by adding several smaller parameterized circuits, rather than using larger ones. In \autoref{sec:Vanishing Local Gradients in QCNs} we showed that the magnitude of the local gradients (the partial gradient of each circuit) of the QCN were unaffected by increasing the number of layers, but tended to vanish for increased number of qubits in each circuit. This enables the possibility of avoiding significant overhead in the estimation on the quantum hardware for big models by using QCNs with several smaller circuits. However, in \autoref{sec:Vanishing Total Gradients in QCNs}, we showed that backpropagating the local gradients still produced a gradient that vanished exponentially fast in the number of layers, similar to the behaviour of DNNs. This effect also strengthened when increasing the number of qubits. Still, for fewer than eight qubits, QCNs were shown to have a larger gradient than similarly sized DNNs, suggesting that they should be faster to train.

In \autoref{sec:Investigating the Loss Landscape}, we showed that the loss landscape of QCNs with few qubits are similar to that of QNNs. We showed this by calculating the empirical fisher information matrix (EFIM) spectrum, and showed that this spectrum was highly uniform for QCNs and QNNs. This feature is known to speed up training when using gradient-based methods \citet{karakida2019universal}. However, when increasing the number of qubits and layers of the QCNs, we observed that the EFIM spectra became skewed, resulting in loss landscapes that are flat in most directions, but highly distorted in one direction. This is similar behavior as DNNs, which cause slow optimization.

In \autoref{sec:Untrained Models}, we showed that untrained QCNs of sufficiently many qubits and similarly sized DNNs exhibited an exponentially decaying trajectory length. For the QCNs, it was concluded that this was a result of the fact that untrained parameterized circuits approximate random circuits, which tend to produce outputs that concentrate around some mean \cite{McClean_2018}. However, we showed in \autoref{sec:Trained Models} that the expressivity of QCNs and DNNs could be increased into the exponential regime through training, producing exponentially more expressive models for each layer. In addition, the trajectory length increased faster for the QCNs compared to the DNNs, suggesting that they are more expressive and can fit more complicated functions.

In \autoref{sec:Ideal Simulation}, we showed that QCNs using four and five qubits, two and three layers, and shallow circuits minimize their MSE faster than similarly sized DNNs when fit to Gaussian data, as was suggested by the analysis of the gradient in \autoref{sec:Vanishing Gradient Phenomenon}. Also, when the DNNs where trained for 10000 epochs (until saturation), they obtained a MSE lower than the QCNs, but still within the same order of magnitude. As the QCNs were trained for only 100 epochs, this might suggest that they could outperform the DNNs given enough training. While speculative, this indicates that QCNs are more expressive then DNNs and thus can fit more complicated data, as predicted by the analysis of its trajectory length. On the other hand, the QNNs implemented in this thesis was unable to fit the Gaussian data, suggesting that the RZZ encoding of the data or the repetitions of the simple anstaz (or both) are unfit for producing a QNN that can successfully train on mixed Gaussian data. In \autoref{sec:Noisy Simulation}, we showed that QCNs still outperformed DNNs when trained using a noisy simulation of the Santiago quantum computer. This was likely due to the low circuit depth of each circuit in the QCNs. Not surprisingly, the QNN performed even worse due to their high circuit depth.   

In \autoref{sec:Real Data}, we trained QCNs and DNNs on the real-world datasets Boston Housing data \cite{boston} and Breast Cancer Wisconsin data \cite{cancer}. We showed that QCNs generalize better on the former data set than DNNs. However, this was not the case for the latter.  


%----------------------------------------------------------------
\section{Conclusions}\label{sec:conclusion}
%----------------------------------------------------------------
QCNs of few qubits and layers have a relatively larger gradient and a less distorted loss landscape compared to similarly sized DNNs. This suggests that QCNs should train comparatively faster. We showed this in practice by training QCNs and DNNs on mixed Gaussian data in one, two and three dimensions, observing faster minimization of the loss for QCNs. During training, QNCs and DNNs increase their expressivity as measured by the trajectory length. This expressivity increased exponentially in the number of layers, for both type of models. However, QNCs of six and seven qubits, they required several order of magnitudes less training to obtain a higher expressivity than DNNs, suggesting that they are more flexible and can fit more complicated data than similarly sized DNNs. It is however unknown if this scales to larger models. For the Boston Housing data, QCNs were shown to generalize better than DNNs to unseen data. However, this was not the case for the Breast Cancer Wisconsin data. This shows that the use of QCNs may have some merit for training on real-world data, but not always.

The multi-circuit architecture of QCNs allows for creating large models by combining several smaller parameterized circuits. This architecture can be trained with a backpropagation algorithm based on the parameters shift rule. The use of smaller circuits has two advantages: First, the relatively shallow circuit depth of each circuit allow for easier evaluation on noisy quantum hardware, as deep circuits are prone to noisy outputs. Second, since the gradient of parameterized circuits tends to vanish exponentially in the number of qubits, smaller circuits with few qubits tends to produce gradients with larger magnitude. Because of their larger magnitude, they are easier to estimate on quantum hardware without resulting in a bad signal-to-noise ratio.
As QNNs consist of a single parameterized circuit, one must increase either its circuit depth or number of qubits to increase the model size, potentially leading to models that are intractable to train on noisy quantum hardware because of the aforementioned reasons. Using a noisy simulation of quantum hardware, we showed QCNs with four qubits, and two to three layers, were able to outperform DNNs on mixed Gaussian data. However, we did not succeed in determining if the properties of QCNs give them any advantage over QNNs when trained on noisy quantum hardware. This is because the QNN architecture with RZZ encoding used in this thesis failed to fit the Gaussian data, using both ideal and noisy simulation. 

%----------------------------------------------------------------
\section{Future Research}\label{sec:future}
%----------------------------------------------------------------
For future research, we suggest experimenting with different ansatzes beyond the simple ansatz \autoref{eq:simple ansatz} for constructing QCNs. Especially, \citet{Cerezo_2021} recently showed that there exists parameterized circuits with circuit depth logarithmic in the number of qubits that does not suffer from an exponential vanishing gradient. While such shallow circuits likely are unfit as QNNs on their own due to low complexity, they could be useful as nodes in an QCN as one could utilize multiple circuits to build a more expressive model. This can potentially alleviate the vanishing gradient problem to some degree.

To establish a better comparison between QCNs and QNNs, we suggest exploring different QNN architectures that are able to fit nonlinear data. Recently, \citet{Schuld_2021} showed that QNNs can be universal function approximators if multiple repetitions of alternating feature encoding and ansatzes are applied. This means that a sufficiently deep QNN could approximate any function to sufficient accuracy. It would be interesting to see whether QCNs could outperform such a QNN when trained on noisy quantum hardware, since the QNN would likely have higher circuit depth and be more prone to noise.

Due to limited computational resources, the training of QNN and QCN models were limited to a small number of qubits and layers. This hindered the exploration of larger model to see how they perform and if they become intractable to train, as the earlier analysis suggests. This can be explored by adapting the Python framework to run on supercomputers, or obtain access to real quantum computers with many qubits.


%----------------------------------------------------------------
%\section{Todo}\label{sec:todo}
%----------------------------------------------------------------

%\begin{itemize}
%    \item Noise simulation only an approximation
%    \item Classical activation saturation(tanh, xavier paper). Plateau, %akin to concentration of PQC output around mean.  
%    \item More details on vanishing PQC gradient
%    \item Calculate complexity of QCN
%    \item batch norm for QCN
%    \item fewer parameters, relieving bottleneck for extremly large %models. 
%    \item is QCN scalable for many qubits? With log n depth and local %observable? QCN perfect usecase!
%    \item is QCN with few qubits smart? Scales no better than linearly. %We already have very powerful classical computers. 
%    \item more details FIM
%    \item Elaborate on other ways of overcoming vanishing %gradient(layerwise learning)
%    \item Santiago quantum computer
%    \item Write about transpiler
%    \item Source of non-linearity, measurement (Shuld vanilla QNN). %Also, imprivitive 2-qubit gate, entanglement, important for ansatz. %Need to motive that QML can be used on NISQ! No amplification of defect in a single quantum circuit. Property of PQC: learn to compensate for hardware specific faults. 
%\end{itemize}