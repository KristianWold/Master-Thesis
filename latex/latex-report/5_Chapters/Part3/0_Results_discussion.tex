%================================================================
\chapter{Results and Discussion}\label{chap:results_discussion}
%================================================================
In this chapter, we will investigate and characterize models discussed in earlier chapters. We will also detail how the various models are configured, and how numerical methods are used for characterization. The results will be briefly commented as they are presented, with a more in-depth discussion at the end of each section.

In \autoref{sec:Vanishing Gradient Phenomenon}, we investigate and compare the vanishing gradient phenomenon for QNNs, QCNs and DNNs by studying how the magnitude of their gradients vary as a function of architecture. There will be put emphasis on the behaviour with respect to the number of layers.

In \autoref{sec:Investigating the Loss Landscape}, we will characterize the geometry of the loss landscape of QNNs, QCNs and DNNs by studying their EFIM spectrum, as presented in \autoref{sec:EFIM}. The result will be used to asses the trainability of different models and predict how architecture affects training.

In \autoref{sec:Expressivity}, we assess the expressivity of QCNs and compare them to DNNs, we will use trajectory length as presented in \autoref{sec:TrajectoryLength}. This will be done for both trained and untrained models.

In \autoref{sec:Training Models}, we test models in a practical setting, and give support to previous results and analyses in this thesis, by fitting them to gaussian data in multiple dimensions. This will be done both using idealised simulation(see \autoref{sec:Exact Expectation Value}), and simulated, noisy hardware(see).


%================================================================
\section{Vanishing Gradient Phenomenon}\label{sec:Vanishing Gradient Phenomenon}
%================================================================
In this section, we investigate and compare the vanishing gradient phenomenon for QNNs, QCNs and DNNs by studying how the magnitude of their gradients vary as a function of architecture. First, we will study in how the gradient of QNNs behave as the number of qubits and repetition of the ansatz increase. Then, the local gradients \autoref{eq:localGradients} of QCNs will be studies for different number of qubits, nodes and layers. Lastly, the vanishing of the total gradient \autoref{eq:derivweightsQCN} resulting from the local gradients is studies.  


%================================================================
\subsection{Vanishing Gradient in QNNs}\label{sec:Vanishing Gradient for QNNs}
%================================================================
We start by investigating the magnitude of the gradient of QNNs for different number of qubits and repetitions of the ansatz. To construct the QNNs, we use qubit encoding with $R_y$ rotations together with the simple ansatz. To derive an output, we calculate the expected parity of the state exactly using the methods in \autoref{sec:Exact Expectation Value} and \autoref{sec:Inference}. We calculate the average magnitude of the gradient using \autoref{eq:magnitude QNN}, with $T=10$ different realisations of the parameters to increase the statistical significance of the result. To get a representative result for a large feature space, we will sample features $\mathcal{X} = \{\boldsymbol{x}^{(1)}, \cdots, \boldsymbol{x}^{(N)}\}$ uniformly as $\mathcal{X} \sim U(-\frac{\pi}{2}, \frac{\pi}{2})^{[N,p]}$. Here, we will use $N=100$ samples, and the number of features $p$ will be set equal to the number of qubits for each QNN. The QNNs will also be initialized in the standard way, i.e. sampling parameters as $\theta_j \sim U(-\pi, \pi)$. The resulting magnitude of the gradients for different number of qubits and ansatz repetitions can be seen in figure \autoref{fig:QNN_vanishing}.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/vanishing_gradient_QNN.pdf}
    \caption{Average magnitude of gradients for QNNs with different ansatz repetitons and number of qubits. The QNNs utilize qubit encoding with $R_y$ rotations, the simple ansatz for processing, and parity sampling to derive an output. The QNNs are fed $N=100$ points of uniformly sampled samples, where the number of features is set equal the number of qubits. }
    \label{fig:QNN_vanishing}
\end{figure}

From the above figure, we see that our implementation of QNNs results in a gradient that vanishes in the exponential regime with respect to the number of qubits. Also, the vanishing is worse the more repetitions of the ansatz is used (with the exception of going from one to two repetitions, for which the gradient actually increased.) This behaviour can be explained by the fact that QNNs are a special case of PQCs. By encoding random inputs and randomly initializing the parameters, the QNN approaches essentially a random circuit as they grow deeper. As shown by \citet{McClean_2018}, randomly initialized PQCs tend to produce gradients closely centered around zero as the number of qubits are increased, which we see also applies for our implementation of QNNs. The only exception to this is the case where repetitions are increased from one to two, resulting in a surprising increase of the gradient. This indicates very shallow QNNs are biased in a way that results in a relatively small gradient.    

%================================================================
\subsection{Vanishing Local Gradient in QCNs}\label{sec:Vanishing Local Gradients in QCNs}
%================================================================

An interesting feature of QCNs is their ability to scale up by introducing more circuits, rather than wider and deeper ones. As the gradient of QNNs tend to vanish for a high number of qubits, we want to investigate how the gradients of smaller QNNs behave when they enter as nodes in a QCN architecture. 

The QNNs used to contruct QCNs here are set up and initialized in the same manner as in \autoref{sec:Vanishing Gradient for QNNs}, but with always two repetitions of the simple ansatz(skrive om?). Also, we sample input data in the same manner, i.e. uniformly as $\mathcal{X} \sim U(-\frac{\pi}{2}, \frac{\pi}{2})^{[N,p]}$, with $N=100$ and the number of features $p$ set to the number of qubits used in the QNNs. In this section, all QCNs have 8 hidden layers and each hidden layer will utilize $d$ number of nodes. The number of qubits in each node will also be set to $d$, which will range from $4$ to $8$. Further, the outputs of each hidden layer is scaled to the interval $[-\pi, \pi]$ to make full use of the qubit encoding in the subsequent layer, as explained in \autoref{sec:Configuring QCNs and DNNs}.

In order to investigate the average behaviour of the local gradients \autoref{eq:localGradients} for each layer, we calculate their magnitude averaged over each layer, the samples and $T = 10$ different realizations. This quantity is given by \autoref{eq:magnitude local}, and is plotted in \autoref{fig:QCN_local_vanishing} for various layers of different QCNs. In addition, the standard deviation of this quantity is estimated over the different realizations, yielding a confidence interval as seen in the figure. 

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/vanishing_gradient_partial_input.pdf}
    \caption{Average magnitude of local gradients \autoref{eq:localGradients} calculated for each layer for various 8 layer QCNs. This quantity was calculated using \autoref{eq:magnitude local}. The number of qubits per node are constant for each QCN, and the number of nodes per layer is set equal the number of qubits. The QNNs are fed $N=100$ points of uniformly sampled samples, where the number of features is set equal the number of qubits. The standard deviation is calculated over 10 different realizations of the model parameters.}
    \label{fig:QCN_local_vanishing}
\end{figure}

In the above figure we see the average magnitude of the local gradients for different layers and number of qubits. The local gradients of the QNNs entering the QCN model tend to vanish exponentially in the number of qubits, as with the single-circuit QNNs seen in \autoref{fig:QNN_vanishing}. However, the relative position of the QNNs along the depth of the QCN does not seem to affect the magnitude. This can be seen from the overlapping confidence intervals of the magnitudes for each number of qubits. Any variation of the average magnitude between the layers is thus likely just noise induced by the low number of $10$ parameter realizations, and does not indicate significant differences.  (No systematic structure that kills the gradient)

%================================================================
\subsection{Vanishing Total Gradient in QCNs}\label{sec:Vanishing Total Gradients in QCNs}
%================================================================

%In the previous section, it was shown that scaling up QCNs by adding more layers did not effect the number of shots needed to sufficiently estimate the local gradients of each QNN. This is in opposition to single-QNN models, whose gradient vanishes as both the number of qubits and repetitions are increase, as seen in \autoref{fig:QNN_vanishing}.

In the previous subsection, it was shown that the magnitude of local gradients of any layer were independent of the relative position of that layer in the QCN model. However, the parameters of are not updated using the local gradients directly. rather, they are updated using the total gradient \autoref{eq:derivweightsQCN}, which is calculated by combining the local gradients using back propagation \autoref{eq:errorQCN}. We need to investigate how the magnitude of the total gradient \autoref{eq:derivweightsQCN} behaves as a function of layers and qubits. 

In this section, the total gradient is calculated using the local gradients from the same numerical experiment as in \autoref{sec:Vanishing Local Gradients in QCNs}. As previously, the magnitude of the total gradient is averaged over each layer, the samples and 10 realisations of the parameters using \autoref{eq:magnitude QCN DNN}. This quantity is plotted in \autoref{fig:QNC_vanishing_total} for different layers and number of qubits. For comparison, it also shows the magnitude of the total gradient of a DNN with the same number of layers and similar number of parameters as the biggest QCN.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/vanishing_gradient_total.pdf}
    \caption{Average magnitude of the total gradient \autoref{eq:derivweightsQCN} calculated for each layer for various 8 layer QCNs. This quantity is calculated using \autoref{eq:magnitude QCN DNN}. The number of qubits per node are constant for each QCN, and the number of nodes per layer is set equal the number of qubits.  For comparison, the same quantity is also calculated for an 8 layer DNN.(more info about DNN)} 
    \label{fig:QNC_vanishing_total}
\end{figure}

From \autoref{fig:QNC_vanishing_total}, we see that the total gradient for a given layer of the DNN tends to vanish exponentially in the number of layers after it. This is a well-known phenomenon for classical neural networks, often explained by the saturation of the activation function during feed forward\cite{shalevshwartz2017failures}. As the activations tend to be very flat for when saturated, as for the tanh function used here, the gradient tends vanish during back propagation.

As with the DNN, also the QCNs exhibits vanishing total gradient with increasing number of layers, with a strong dependence on the number of qubits in each node. Seen in \autoref{fig:QCN_local_vanishing}, the total gradient vanishes faster for higher number of qubits in each node. This phenomenon can be related to the magnitude of the local gradients. As the error of the QCN is propagated backwards using \autoref{eq:errorQCN}, it accumulates the local gradients $\frac{\partial \boldsymbol{a}^{l+1}_k}{\partial \boldsymbol{a}^{l}_j}$ as factors. In the case that these factors are large, the error will tend to decrease slowly, and hence also the total gradient. This is the case for architectures with few qubits per node, as discussed in \autoref{sec:Vanishing Local Gradients in QCNs}. However, as the number of qubits increase, the local gradients will tend to decrease. Accumulating small factors will causes the error to decrease faster, exponentially so for each layer. In a sense, this vanishing of local gradients with increasing number of qubits is analogous to the saturation of the activations for classical networks.

%================================================================
\subsection{Discussion}\label{sec:Vanishing Gradient Phenomenon Discussion}
%================================================================
The results of \autoref{sec:Vanishing Gradient for QNNs} show that up-scaling of QNNs by increasing the number of qubits and repetitions of the ansatz results in an exponential decay of their gradients. As explained in \autoref{sec:BarrenPlateus}, this means that exponentially many shots are required in order to obtain a good signal-to-noise when estimating the gradient. If the gradient is too noisy, optimization using gradient descent and similar methods may result in essentially a random walk in parameter space that fail to converge (kilde). This becomes even more problematic in the presence of addition noise introduced by noisy hardware, as discussed in \autoref{sec:Noisy Simulation}. Ultimately, this indicate that the training of QNNs can become intractable as they are up-scaled to solve harder learning problems.    

In \autoref{sec:Vanishing Local Gradients in QCNs}, we see that the local gradients of QCNs also vanish exponentially in the number of qubits, but are independent of the overall number of layers of the QCN. This suggests that QCNs can be up-scaled by making them deeper, without affecting the magnitude of the local gradients. Consequently, a constant number of shots can be used for each node during estimation to obtain a certain single-to-noise ratio, making their estimation tractable on a quantum computer. 

Even though the magnitude of local gradients of QCNs tends to stay constant in the number of layers, we see in \autoref{sec:Vanishing Total Gradients in QCNs} that back propagation still induce an exponentially vanishing total gradient for sufficiently many qubits. This is due to the accumulation of small factors when the local gradients are combined using back propogation. This behaviour is similar to that of DNNs, with the gradient vanishing faster for initial layers. However, the vanishing was not as severe for a conservative number of qubits. For 4 qubits, the gradient actually tended to increase. For 8 qubits, and presumably above, the gradient vanished faster for QCNs than for similarly sized DNN.  

An interesting observation is that the vanishment caused by back propagation happens in a purely classical part of the optimization, with the local gradients stored as floating-point numbers. This means that even though the total gradient tends to decrease exponentially with the number of layers, it does not introduce an exponential overhead on the quantum computer by requiring more shots. This is true, however, for single-QNN models as discussed in \autoref{sec:BarrenPlateus}. Put another way, QCNs' use of several smaller circuits, rather than one big, moves the estimation of vanishing quantities (the gradient) from quantum expectation values to classical computation. 



%================================================================
\section{Investigating the Loss Landscape}\label{sec:Investigating the Loss Landscape}
%================================================================
We explore the geometry of the loss landscape of various models and quantify its degree of distortion and flatness by studying the eigenvalue spectrum of the EFIM. Looking at \autoref{eq:EmpiricalFisher}, we see that the EFIM, unlike the Hessian, is independent of targets $y^{(i)}$. This makes the analysis data agnostic, and serves to characterise the architectures themselves. The input data $\mathcal{X} = \{\boldsymbol{x}^{(1)}, \cdots, \boldsymbol{x}^{(N)}\}$ used for calculating the EFIM is sampled randomly from a standard normal distribution as $\mathcal{X} \sim N(0,1)^{[N,p]}$. This ensures that the input is evenly sampled from feature space and is consistent with the analysis of \citet{abbas2020power}. Here, we use $N=200$ samples and either $p=4$ or $p=6$ inputs, depending on the model. For each model, the EFIM is calculated 10 times for different random initializations of the parameters. The resulting spectrum is then averaged over the 10 initializations to produce more a significant results. For a complete description of the models analysed in this section, see \autoref{tab:FIM models}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Model &Type & Qubits& Reps & Layers & Width & Encoder        & $n_{\theta}$ \\ \hline
A    & QNN & 4& 18   & 1      & 4     & RZZ Encoding   & 72  \\ \hline
B    & QCN & 4& 3    & 2      & 4     & Qubit Encoding & 60 \\ \hline
C    & QCN & 4& 2    & 3      & 4     & Qubit Encoding & 72  \\ \hline
D    & QCN & 4& 1    & 5      & 4     & Qubit Encoding & 68  \\ \hline
E    & DNN & NA& NA   & 3      & 6     & NA             & 79 \\ \hline
F    & QNN & 6& 26   & 1      & 6     & RZZ Encoding   & 156  \\ \hline
G    & QCN & 6& 4    & 2      & 6     & Qubit Encoding & 168 \\ \hline
H    & QCN & 6& 2    & 3      & 6     & Qubit Encoding & 156  \\ \hline
I    & QCN & 6& 1    & 5      & 6     & Qubit Encoding & 150  \\ \hline
J    & DNN & NA& NA   & 3      & 9     & NA             & 163 \\ \hline
\end{tabular}
\caption{Description of the architecture of the models analysed in this section. The QNN and QCN models use exact evaluation of parity to derive outputs (see \autoref{sec:Exact Expectation Value} and \autoref{sec:Inference}). The DNN models uses tanh activation in all layers. The parameters of the models are appropriately initialized as presented in \autoref{sec:Initialization}.} 
\label{tab:FIM models}
\end{table}

\autoref{fig:FIM Comparison} compares the EFIM spectrum of QNNs, QCNs and DNNs. Their architectures are chosen so that the models have approximately equal number of parameters. This is to ensure a fair comparison. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/FIM_qubits_4.pdf}
        \caption{Lorem ipsum}
        
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/FIM_qubits_6.pdf}
        \caption{Lorem ipsum}
    \end{subfigure}
    \caption{Comparison of EFIM eigenvalue spectrum between QNNs, QCNs and DNNs. For details about the architectures, see \autoref{tab:FIM models}. The EFIM is calculated using $N=200$ points of uniformly sampled points, where the number of features are set equal the number of qubits. The EFIM spectrum is finally averaged over 10 different model realizations.}
    \label{fig:FIM Comparison}
\end{figure}

Looking at the spectra of the DNNs in \autoref{fig:FIM Comparison}, we see the characteristic result of a singular large eigenvalue, with the rest sitting close to zero. This indicates that DNN models exhibit a loss landscape that is very flat in all but one direction, where it is extremely distorted. We also see that the spectra of the our implementation of QNNs are much more uniformly distributed compared to the DNN models. This results in a loss landscape that is significantly distorted in most directions, rather than just one. 

Moving over to the QCNs, we see from \autoref{fig:FIM Comparison} that the spectra of the three layer QCNs exhibit much the same uniformity as the QNNs. A more thorough comparison between different QCNs can be seen in \autoref{fig:FIM QCN}. In this figure, we vary the number of layers of the QNCs and the number of repetitions (i.e. the number of times the ansatz is repeated for each node), while keeping the total number of parameters roughly constant. In doing this, we get to precisely shift how much of the complexity of a given QCN results from the complexity of each node or the overall structure of the network.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/FIM_qubits_4_comparison.pdf}
        \caption{Comparison of the EFIM eigenvalue spectrum for different 4-qubit QNNs and QCNs.}
        \label{fig:FIM QCN a}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/FIM_qubits_6_comparison.pdf}
        \caption{Comparison of the EFIM eigenvalue spectrum for different 6-qubit QNNs and QCNs.}
        \label{fig:FIM QCN b}
    \end{subfigure}
    \caption{Comparison of the EFIM eigenvalue spectrum between different QNNs and QCNs. For details about the architectures, see \autoref{tab:FIM models}. The EFIM is calculated using $N=200$ points of uniformly sampled points, where the number of features is set equal the number of qubits for each model. The EFIM spectrum is finally averaged over 10 different model realizations to produce a more significant result.}
    \label{fig:FIM QCN}
\end{figure}

\autoref{fig:FIM QCN a} shows that, for 4 qubits, the spectra of the different QCNs exhibit roughly the same uniformity, with the eigenvalues staying within roughly an order of magnitude of each-other. Going up to 6 qubits, \autoref{fig:FIM QCN b} shows that the spectrum tends to concentrate more around zero the more layers the QCN have. This is likely related to the vanishing of the gradient induced by back propagation. For 4 qubits, this is not as big of a problem since the local gradients are relatively big and hence also the total gradient. However, for 6 qubits, the local gradients tend to vanish. This results in the gradient vanishing faster when increasing the number of layers, which in turn results in a flatter landscape. 


%================================================================
\subsection{Discussion}\label{sec:Loss Landscape Discussion}
%================================================================
The highly uneven EFIM spectrum of DNNs indicates a loss landscape that is strongly distorted in one direction and mostly flat otherwise. This result is consistent with the findings of \citet{karakida2019universal} and \citet{abbas2020power}. The former authors point out that strong distortions in some directions indicate that the model outputs are very sensitive to changes in parameter space in exactly these directions, and likewise not sensitive to changes in the others. This tend to slow training when using gradient descent and similar methods, as too high learning rate leads to overstepping in the distorted directions, while a low learning rate changes the model insignificantly in the flat directions. 

For the QNNs, we found the EFIM spectrum to be much more uniform than that of a comparable DNN. \citet{abbas2020power} came to the same conclusion for their QNN models, and argued that this uniformity of the spectrum meant that landscape was more well-condition for optimization, and thus should train faster. They strengthened this hypothesis by showing experimentally that QNNs reduced error faster than DNNs for equal number of training iterations. 

We found that few layered exhibited similar uniformity of the EFIM spectrum as QNNs. The spectrum became increasingly more skewed for increasing number of layers and qubits. However, they still showed several order of magnitude larger eigenvalues that DNNs, suggesting that small-scale QCNs should train comparably faster than DNNs, like QNNs.

%================================================================
\section{Expressivity}\label{sec:Expressivity}
%================================================================
We will investigate the expressivity of QCNs and DNNs using the trajectory length method of \citet{raghu2017expressive}, as described in \autoref{sec:TrajectoryLength}. The trajectory length will be first studied for randomly initialized QCNs for varying number of qubits in each node. Then, for some selected QCNs, the trajectory length will be investigated as the models are gradually fitted on 2D mixed Gaussian data. The results in both cases will be compared to similar DNNs, with approximately the same number of parameters for fair comparison. We will use a input trajectory  used will be circle $\boldsymbol{x}(t_i) \in \mathbb{R}^2$ with radius $\frac{\pi}{2}$ and centered around $0$, divided up into $1000$ equally spaced point. The input trajectory and 2D mixed Gaussian data will be pre-processed appropriately, depending on the model, as described in \autoref{sec:Pre-processing Input}.

%================================================================
\subsection{Untrained Models}\label{sec:Untrained Models}
%================================================================

In this subsection, we investigate the same QCN and DNN architectures as formulated in \autoref{sec:Vanishing Local Gradients in QCNs}, initialized randomly in the same manner. \autoref{fig:TL_untrained} shows how the trajectory length varies as a function of layer, when the different models are fed the circle trajectory $\boldsymbol{x}(t_i)$ defined earlier.

%All layers have $d$ number of nodes, and all nodes have $d$ number of qubits. $d$ ranges from $4$ to $8$. For comparison, the trajectory length is also calculated for an 8-layer DNN with approximately the same number of parameters as the biggest QCN. The QCNs and the DNN are randomly initialized as in \autoref{sec:Vanishing Gradient Phenomenon}.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/TL_untrained.pdf}
    \caption{}
    \label{fig:TL_untrained}
\end{figure}

\autoref{fig:TL_untrained_projection}, accompanying \autoref{fig:TL_untrained}, shows the trajectories of selected models and layers, projected onto 2D. The rows correspond to the 4 qubit QCN, 8 qubit QCN and DNN, from top to bottom. The columns correspond to the first layer, second layer, third layer and last layer, left to right.   

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/TL_untrained_projection.pdf}
    \caption{}
    \label{fig:TL_untrained_projection}
\end{figure}

%In \autoref{fig:TL_untrained}, we see that the untrained DNN exhibits an exponential decrease in trajectory length as it is being transformed by each layer. From \autoref{fig:TL_untrained_projection}, we see this manifesting itself as the trajectory concentrating around some mean, progressively more for each layer. This shows that randomly initialized DNNs can tend to compute functions which are not very sensitive to the input, which was shown experimentally by \citet{raghu2017expressive} to be possible in practical settings. 

From \autoref{fig:TL_untrained}, we see that the trajectory length of QCNs with 4 and 5 qubits tend to grow exponentially with the number of layers. This growth is however diminishing as the number of qubits increase, and switches over to an exponential decay for 6 qubits and above. For 8 qubits, the decay is similar to that of the DNN with similarly many parameters. Comparing the results to \autoref{fig:TL_untrained_projection}, we see how the increasing and decreasing trajectory length manifests themselves. Seen from the top row, the trajectory produced by the 4 qubit QCN tends to become increasingly distorted and complex. This is similar to the behaviour of classical networks seen in \autoref{fig:trajectoryLengthExample}, produced by \citet{raghu2017expressive}, and shows that also QCNs can compute functions exponentially complex in the number of layers. In contrast, the trajectory of the 8 qubit QCN and DNN (seen in the next two rows) can be seen to gradually concentrate for each layer, resulting in a function that is very little sensitive to the input.  


%================================================================
\subsection{Trained Models}\label{sec:Trained Models}
%================================================================
\citet{raghu2017expressive} showed that networks that initially don't lie in the exponential growth regime can be pushed there via training. In this subsection, we will train different models by incrementally fitting them to the 2D mixed Gaussian data. The trajectory length will be recalculated for each layer after each increment. The models being investigated here are two four layer QCNs with 6 and 7 qubits, respectively, with each node having the same architecture as in \autoref{sec:Untrained Models}. These will be compared with DNNs with approximately same number of parameters. For more information about the models, see \autoref{tab:TL models}. All the models are trained using Adam optimizer with the standard hyperparameters and a learning rate of $0.1$. The QCNs are trained for a total of $20$ epochs in increments of $5$. In order to produce a fair comparison, the DNNs will not be trained for the same increments of epochs. Rather, they will be trained until they achieve approximately the same MSE on the training set as the QCNs, for each increment. In this way, we get to compare the expressivity of QCNs and DNNs that fit the data to an equal degree. \autoref{fig:TL_trained} shows how the trajectory length changes as the different models presented here are incrementally trained, for each layer. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/TL_trained_QCN_qubit_6.pdf}
        \caption{QCN, 6 qubits. 228 parameters.}
        \label{fig:TL_trained_A}
        
    \end{subfigure}%
    \hfill 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/TL_trained_QCN_qubit_7.pdf}
        \caption{QCN, 7 qubits. 308 parameters.}
        \label{fig:TL_trained_B}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/TL_trained_DNN_nodes_9.pdf}
        \caption{DNN, 9 nodes. 217 parameters.}
        \label{fig:TL_trained_C}
        
    \end{subfigure}%
    \hfill 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/TL_trained_DNN_nodes_11}
        \caption{DNN, 11 nodes. 309 parameters.}
        \label{fig:TL_trained_D}
    \end{subfigure}
    \caption{Caption place holder}
    \label{fig:TL_trained}
\end{figure}

From \autoref{fig:TL_trained}, we see that training the QCNs and DNNs progressively increases the trajectory length of the models. After only $5$ epochs, the trajectory length of the 6 qubit QNC enters the exponential growth regime. This demonstrates that randomly initialized QCNs can be made to produce complex functions through training, even though their outputs initially tend to concentrate around a mean, as shown in \autoref{fig:TL_untrained}. The 7-qubit QCN is brought into the exponential growth regime after $10$ epochs, twice the amount required for 6 qubits. 

Moving over to the corresponding DNNs, we see that they fail to enter the same exponential growth regime as the QCNs, even when trained until they fit the data to the same degree. This indicates that QCNs, in this context, may be more expressive than DNNs for the same number of parameters.  

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Model &Type & Qubits& Layers & Nodes &$n_{\theta}$ \\ \hline
A    & QCN & 6 &  4 & 6& 228   \\ \hline
B    & QCN & 7 &  4 & 7& 308 \\ \hline
C    & DNN & NA&  4 & 9& 217  \\ \hline
D    & DNN & NA&  4 & 11& 309  \\ \hline
\end{tabular}
\caption{Stuff.} 
\label{tab:TL models}
\end{table}

%================================================================
\subsection{Discussion}\label{sec:Discussion Expressivity}
%================================================================

For higher number of qubits, we saw from \autoref{fig:TL_untrained_projection} that the trajectory of untrained QNCs tend to concentrate progressively for each layer they pass through. This is a manifestation of the phenomenon where PQC outputs tends to concentrate around their average value when the number of qubits is increased. Since each node is a QNN, which is a type of PQC, QCN are also subject to this. As the inputs are sequentially transformed by each layer in the QCN, this concentration of the outputs are in fact applied multiple times, causing an exponential decrease of the trajectory length, as seen in \autoref{fig:TL_untrained}.

From \autoref{fig:TL_trained} we see that training brought the QCNs into the exponential growth regime, increasing the sensitivity of the node outputs with respect to the inputs. In other words, optimizing the QCNs updates the parameters in a way that brings structure to each of the circuits, moving them away from being random circuits. This causes the outputs to no longer concentrate around the mean, which in turn lets the model compute more complex functions. 

Comparing \autoref{fig:TL_trained_A} and \autoref{fig:TL_trained_B}, we see that the 7-qubit QCN required twice the number of epochs to enter the exponential growth regime compared to the 6-qubit QCN. As discussed in \autoref{sec:Vanishing Gradient Phenomenon}, increasing the number of qubits makes the magnitude of the gradient exponentially smaller. Thus, a larger number of epochs, potentially exponentially so, are required to significantly change the parameters such that the nodes no longer resemble random circuits. This might produce a significant overhead for QCNs with a high number of qubits.

Looking at \autoref{fig:TL_trained_C} and \autoref{fig:TL_trained_D}, we see that the corresponding DNNs fail to enter the exponential growth regime, even after training the models for a number of epochs that result in the same MSE as the QCNs. The DNNs achieve exponential growth first after more than an order magnitude more epochs. This suggests that QCNs can be trained to be more expressive than DNNs of similar number of parameters. How can we explained this increased expressivity? A possible explanation for this is that the mathematical transformation applied by the QCN is in a way more powerful compared to DNNs. Typically, sufficiently deep PQCs are conjectured to be intractable for classical computers to simulate efficiently \cite{abbas2020power}\cite{lloyd2018quantum}. Stated in \autoref{sec:Multiple Qubits}, as the numbers of qubits $n$ increase, the dimensionality of the resulting Hilbert space grows exponentially as $d = 2^n$. It follows that gates applied on this space is mathematically represented as a $d\times d$ matrices, which causes an exponential overhead on classical computers. In this sense, the encoding and unitary transformations of each QCN node constitutes a harder computation than the affine transformation and non-linearity of DNN nodes. Still, even though a computation is harder, it does necessarily result in the computation of a richer, more complicated function. For example, it was shown from \autoref{fig:TL_untrained} that the opposite is true for randomly initialized QCNs. However, the increasing trajectory length seen in \autoref{fig:TL_trained} suggest that(elaborate). 

%================================================================
\section{Training Models on Gaussian Data}\label{sec:Training Models}
%================================================================
In this section, we will study various models in a more practical setting, fitting them to mixed Gaussian data in one, two and three dimensions. For more details on the data, see (see). We will train QNNs, QCN and DNNs with varying complexity and use MSE on the training data to evaluate how good the fit is. This will be done first in the ideal case, with exact calculation of outputs. Then, we will repeat the training for all models by simulating the noise model of the Santiago quantum computer(kilde). For a complete description of the models trained in this section, see \autoref{tab:training models}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Model& Type& Data& Qubits& Layers & Width &$n_{\theta}$ \\ \Xhline{3\arrayrulewidth}
A    & QNN & 1D  & 4     & NA     & NA& 168   \\ \hline
B    & QCN & 1D  & 4     & 2      & 4& 228 \\ \hline
C    & QCN & 1D  & 4     & 2      & 4& 177  \\ \hline
D    & DNN & 1D  & NA    & 2      & 4& 217  \\ \Xhline{3\arrayrulewidth}
E    & QNN & 2D  & 4     & NA     & NA& 217  \\ \hline
F    & QCN & 2D  & 4     & 3      & 4& 217  \\ \hline
G    & QCN & 2D  & 4     & 3      & 4& 217  \\ \hline
H    & DNN & 2D  & NA    & 3      & 4& 217  \\ \Xhline{3\arrayrulewidth}
I    & QNN & 3D  & 5     & NA     & NA& 217  \\ \hline
J    & QCN & 3D  & 5     & 3      & 5& 217  \\ \hline
K    & QCN & 3D  & 5     & 3      & 5& 217  \\ \hline
K    & DNN & 3D  & NA    & 3      & 5& 217  \\ \hline
\end{tabular}
\caption{Stuff.} 
\label{tab:training models}
\end{table}


%================================================================
\subsection{Ideal Simulation}\label{sec:Ideal Simulation}
%================================================================
\autoref{fig:trained ideal} shows the MSE on the training data for the models defined in \autoref{tab:training models}, trained on the mixed Gaussian data. In order to produce a more significant result, each model is randomly initialized(see \autoref{sec:Initialization}) 10 times and trained separately. The resulting MSE for each model type is then averaged over the 10 runs and plotted with fill corresponding to one standard deviation. In this way, we get to see the average model behaviour during training  and how it varies between different runs.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/1D_gaussian_data_fit.pdf}
        \caption{Lorem ipsum}
        
    \end{subfigure}%
    \hfill 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/2D_gaussian_data_fit.pdf}
        \caption{Lorem ipsum}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/3D_gaussian_data_fit.pdf}
        \caption{Lorem ipsum}
        
    \end{subfigure}%
    \caption{Caption place holder}
    \label{fig:trained ideal}
\end{figure}

From \autoref{fig:trained ideal}, we see that the QCNs minimized the MSE quicker than both the QNN and the DNN on the mixed Gaussian data. The QCNs faster optimization compared to the DNNs is not surprising, since this was suggested by the its relatively larger gradient and uniform EFIM spectrum seen in \autoref{sec:Vanishing Gradient Phenomenon} and \autoref{sec:Investigating the Loss Landscape}, respectively. Even though the QNNs also exhibit a relatively uniform EFIM spectrum, \autoref{fig:trained ideal} shows that they are unable to make a good fit on the mixed Gaussian data. This shows that our choice of QNN architecture is insufficient for approximating this data. As explained in \autoref{sec:FeedForward}, our QNNs perform a mostly unitary (and thus linear) transformation of the input, except at the stage of encoding and measurement. This results in a perhaps too constrained functional form, which explains the QNN's inability to fit the data. QCNs, on the other hand, incorporate multiple non-linearities for each layer. This is likely the key to their greater flexibility, as it enables them to compute a larger family of functions. 

Looking again at \autoref{fig:trained ideal}, we see that the optimization of the QCNs with one ansatz repetition exhibit a lot of variance, indicating that the training is very sensitive to parameter initialization and that it is getting stuck in local minima.   

In \autoref{tab:training models}, the final MSE is 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
Model& Type& Data& MSE, $10^{2}$ Epochs& MSE, $10^{4}$ Epochs \\ \hline
A    & QNN & 1D  &    & NA   \\ \hline
B    & QCN & 1D  & $7.5\times 10^{-3}$  & NA \\ \hline
C    & QCN & 1D  & $2.2\times 10^{-3}$  & NA  \\ \hline
D    & DNN & 1D  & $1.3\times 10^{-2}$ & $\boldsymbol{5.1\times 10^{-5}}$  \\ \Xhline{2\arrayrulewidth}
E    & QNN & 2D  &                    & NA  \\ \hline
F    & QCN & 2D  &  $1.7\times 10^{-2}$ & NA  \\ \hline
G    & QCN & 2D  &  $\boldsymbol{4.2\times 10^{-3}}$ & NA  \\ \hline
H    & DNN & 2D  &  $3.0\times10^{-2}$ & $4.6\times10^{-3}$\\ \Xhline{2\arrayrulewidth}
I    & QNN & 3D  &                    & NA  \\ \hline
J    & QCN & 3D  &  $4.9\times 10^{-3}$ & NA  \\ \hline
K    & QCN & 3D  &  $\boldsymbol{2.8\times10^{-3}}$  & NA  \\ \hline
K    & DNN & 3D  &  $1.3\times10^{-2}$  & $3.1\times10^{-3}$  \\ \hline
\end{tabular}
\caption{Stuff.} 
\label{tab:training models}
\end{table}

%================================================================
\subsection{Noisy Simulation}\label{sec:Noisy Simulation}
%================================================================

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/1D_gaussian_data_fit_noisy.pdf}
        \caption{Lorem ipsum}
        
    \end{subfigure}%
    \hfill 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/2D_gaussian_data_fit.pdf}
        \caption{Lorem ipsum}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/3D_gaussian_data_fit.pdf}
        \caption{Lorem ipsum}
        
    \end{subfigure}%
    \caption{Caption place holder}
    \label{fig:trained ideal}
\end{figure}


%================================================================
\subsection{Discussion}\label{sec:Training Discussion}
%================================================================
