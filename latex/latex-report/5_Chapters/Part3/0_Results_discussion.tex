%================================================================
\chapter{Results and Discussion}\label{chap:results_discussion}
%================================================================
In this chapter, we will detail how various models presented in previous chapters were configured and trained. The models will also be characterized using aforementioned numerical methods, and the results will be compared to earlier research.

We will characterize the geometry of the loss landscape of QCNs(see \autoref{sec:Quantum Circuit Network}) and other models by studying the EFIM presented in \autoref{sec:EFIM}. The result will be used to asses the trainability of different models and predict how architecture affects training.

The vanishing gradient phenomenon will be investigated for QCN and DNN models by studying the magnitude of their gradients (see \autoref{sec:BackwardPropagationQCN} and \autoref{sec:BackpropogationDNN}). This will be done for different architectures, especially different number of layers.

To assess the expressivity of QCN models and compare them to other models, we will use trajectory length as presented in \autoref{sec:TrajectoryLength}. This will be done for both trained and untrained models.

In order to test models in a practical setting, and give support to previous results and analyses in this thesis, the models will be trained to fit gaussian data. This will be done both using idealised simulation(see \autoref{sec:Exact Expectation Value}), and simulated, noisy hardware(see).

%================================================================
\section{Investigating the Loss Landscapes}\label{sec:Investigating the Loss Landscapes}
%================================================================



%----------------------------------------------------------------
\subsection{Project Results 1}\label{sec:project results}
%----------------------------------------------------------------


%================================================================
\section{Vanishing of the Gradient}\label{sec:Vanishing of the Gradient}
%================================================================

%================================================================
\section{Expressivity of Untrained and Trained Models}\label{sec:Vanishing Gradient}
%================================================================


%================================================================
\section{Training Models on Gaussian Data}\label{sec:Training Models on Gaussian Data}
%================================================================

