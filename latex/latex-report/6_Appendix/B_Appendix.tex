%================================================================
\chapter{Data Sets}\label{sec:Appendix B}
%================================================================
In this chapter, we will present details surrounding the data sets used for training and testing models in this thesis. 

%================================================================
\section{Mixed Gaussian Data}\label{sec:Mixed Gaussian Data}
%================================================================
In order to obtain a complex, varying surface suited for regression, we choose to generate such data artificially by summing multiple Gaussian functions with different means and standard deviations. This creates what is known as mixed Gaussian data. 

Given a data point $\boldsymbol{x}$ with $p$ features, the output of a general multivariate Gaussian (without normalization and correlations) can be computed using 
\begin{equation}\label{eq:Gaussian}
    y = e^{(\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x} - \boldsymbol{\mu})},
\end{equation}
where $\boldsymbol{\mu}$ is a $p$-dimensional vector that defines the position of the center of the Gaussian function, and $\Sigma$ is a $p\times p$ diagonal matrix defining the extension of the Gaussian in each direction. In this thesis, we will prepare samples $\boldsymbol{x}^{(i)} \in [0,1]^p$ as a meshgrid that uniformly fills the input space $[0,1]^p$. This ensures a dense data set that captures the details of the mixed Gaussian function. We will generate data sets for $p \in[1,2,3].$ These differed data sets are described in \autoref{tab:MixedGaussianData}. For a visualization, see \autoref{fig:mixed Gaussian 1D}, \autoref{fig:mixed Gaussian 2D} and \autoref{fig:mixed Gaussian 3D}. For a complete description on how the data is generated, see \url{https://github.com/KristianWold/Master-Thesis/blob/main/src/utils.py}.

To import the mixed Gaussian data, the following code can be used:
\begin{lstlisting}[language=python, numbers=left]
from utils import generate_1D_mixed_gaussian
from utils import generate_2D_mixed_gaussian
from utils import generate_3D_mixed_gaussian

x1, y1 = generate_1D_mixed_gaussian()
x2, y2 = generate_2D_mixed_gaussian()
x3, y3 = generate_1D_mixed_gaussian()
\end{lstlisting}

\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|}
\hline
 Name& \#Samples&  \# Features& Feature Type& Target Type \\ \hline
 1D mixed Gaussian&  100&  1& $x_i \in [0,1]$ & $y \in [0,1]$  \\ \hline
 2D mixed Gaussian&  144&  2& $x_i \in [0,1]$ & $y \in [0,1]$ \\ \hline
 3D mixed Gaussian&  216&  3& $x_i \in [0,1]$ & $y \in [0,1]$ \\ \hline
\end{tabular}
\caption{Details on the various mixed Gaussian data sets. For a complete description on how to produce it, see \url{https://github.com/KristianWold/Master-Thesis/blob/main/src/utils.py}}
\label{tab:MixedGaussianData}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/gaussian_1D.pdf}
    \caption{Visualization of the 1D mixed Gaussian dataset. For more details, see \autoref{tab:MixedGaussianData}.} 
    \label{fig:mixed Gaussian 1D}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{latex/figures/gaussian_2D.pdf}
    \caption{Visualization of the 2D mixed Gaussian dataset. For more details, see \autoref{tab:MixedGaussianData}} 
    \label{fig:mixed Gaussian 2D}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/gaussian_3D_1.pdf}
        \caption{Slice of the data set at $x_3 = \frac{1}{6}$.}
        
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.9in]{latex/figures/gaussian_3D_2.pdf}
        \caption{Slice of the data set at $x_3 = \frac{5}{6}$.}
    \end{subfigure}
    \caption{Visualization of the 3D mixed Gaussian dataset. For more details, see \autoref{tab:MixedGaussianData}}
    \label{fig:mixed Gaussian 3D}
\end{figure}





%================================================================
\section{Real Data}\label{sec:Real Data stuff}
%================================================================
For benchmarking the dense neural networks (DNNs) and quantum circuit networks (QCNs) implemented in this thesis against realistic data sets, and for investigating how they generalize to unseen data, we will be using the popular Boston Housing data (kilde) and Breast Cancer Wisconsin data (kilde). In this section, we will presents details about these data sets. 

%================================================================
\subsection{Boston Housing Data}\label{sec:Boston Housing Data}
%================================================================
The Boston Housing data is a popular data set used for regression, readily available though the scikit-learn python package\cite{scikit-learn}. The data set can be loaded using the following code:

\begin{lstlisting}[language=python, numbers=left]
from sklearn.datasets import load_boston
data = load_boston()
x = data.data
y = data.target
\end{lstlisting}

The targets $y$ of the Boston Housing data are \emph{median of owner-occupied homes by town}, in \$1000, which can be predicted using for regression methods. Some of the features include quantities such as \emph{per capita crime rate by town}, \emph{average number of rooms per dwelling} and \emph{pupil-teacher ratio by town}. For a complete description of the features, see \url{https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html}. For some general details of the data set, see \autoref{tab:Boston}.

\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|}
\hline
 Name& \#Samples&  \# Features& Feature Type& Target Type \\ \hline
 Boston Housing Data&  506&  13& $x_i \in \mathbb{R}$ & $y \in \mathbb{R}$  \\ 
 \hline
 
\end{tabular}
\caption{Some details on the Boston Housing data set.}
\label{tab:Boston}
\end{table}

%================================================================
\subsection{Breast Cancer Wisconsin Data}\label{sec:Breast Cancer Data}
%================================================================

The Breast Cancer Wisconsin data set is another popular data accessible through scikit-learn. The data set can be loaded using the following code:

\begin{lstlisting}[language=python, numbers=left]
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
x = data.data
y = data.target
\end{lstlisting}

The targets $y$ of the data set are binary values indicating whether breast tissue is malignant or benign, suitable for classification methods. Some of the features include quantities such as \emph{mean radius}, \emph{mean area} and \emph{mean smoothness} describing the breast tissue. For a complete description of the features, see \url{https://www.kaggle.com/uciml/breast-cancer-wisconsin-datal}. For some general details of the data set, see \autoref{tab:Cancer}.

\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|}
\hline
 Name& \#Samples&  \# Features& Feature Type& Target Type \\ \hline
 Breast Cancer Wisconsin&  569&  30& $x_i \in \mathbb{R}$ & $y \in \{0,1\}$  \\ 
 \hline
 
\end{tabular}
\caption{Some details on the Breast Cancer Wisconsin data set.}
\label{tab:Cancer}
\end{table}


%================================================================
\subsection{Feature Reduction with PCA}\label{sec:Feature Reduction with PCA}
%================================================================
For both the Boston Housing data and the Breast Cancer Wisconsin data, we will perform a principal component analysis (PCA) to reduce the number of features from $13$ and $30$ to four, respectively. This is for making the training of QCNs more feasible, as a high number of features also require a high number of qubits when using qubit encoding. For more details, see \autoref{sec:Principal Component Analysis} and \autoref{sec:Feature Reduction with PCA}.

%================================================================
%\section{Sparse Data Set}\label{sec:Spare Data Set}
%================================================================





